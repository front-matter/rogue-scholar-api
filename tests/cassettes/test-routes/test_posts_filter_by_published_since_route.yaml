interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.31.0
    method: GET
    uri: https://fmxr36stzdcbiw7hp-1.a1.typesense.net/collections/posts/documents/search?q=&query_by=tags%2Ctitle%2Cdoi%2Cauthors.name%2Cauthors.url%2Csummary%2Ccontent_html%2Creference&filter_by=published_at%3A%3E%3D+1696550400&sort_by=published_at%3Adesc&per_page=10&page=1
  response:
    body:
      string: "{\"facet_counts\":[],\"found\":12,\"hits\":[{\"document\":{\"authors\":[{\"name\":\"Stephen
        Royle\",\"url\":\"https://orcid.org/0000-0001-8927-6967\"}],\"blog_id\":\"s1e9w75\",\"blog_name\":\"quantixed\",\"blog_slug\":\"quantixed\",\"content_html\":\"\\n<p>On
        a scientist&#8217;s Google Scholar page, there is a list of co-authors in
        the sidebar. I&#8217;ve often wondered how Google determines in what order
        these co-authors appear.</p>\\n\\n\\n\\n<p>The list of co-authors on a primary
        author&#8217;s page is not exhaustive. It only lists co-authors who also have
        a Google Scholar profile. They also have to be suggested to the primary author
        and they need to accept the co-author to list them on the page. Finally, the
        profile page only displays the first 20 co-authors. Any further co-authors
        can be seen by clicking &#8220;View All&#8221;. As I understand it, there
        is a limit to the number of co-authors a primary author is allowed to have;
        I currently have 40 and haven&#8217;t yet hit a limit. The ranking of co-authors
        is determined <em>somehow</em> and the first 20 are displayed on the primary
        author&#8217;s profile page, in the sidebar on the right.</p>\\n\\n\\n\\n<p><strong>How
        does Google Scholar rank these co-authors?</strong> Let&#8217;s use R to find
        out!</p>\\n\\n\\n\\n<p>We&#8217;ll make use of the <a href=\\\"https://github.com/jkeirstead/scholar\\\">scholar</a>
        package to get the data. The primary author used in the package vignette is
        Albert Einstein, but he doesn&#8217;t have any co-authors on Google Scholar;
        so we&#8217;ll use my data instead.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; title: ; notranslate\\\" title=\\\"\\\">\\nlibrary(scholar)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(zoo)\\n\\n#
        use a Google Scholar ID here\\nid &lt;- &quot;PBcP8-oAAAAJ&quot;\\n# retrieve
        all the info from the page\\nl &lt;- get_profile(id)\\n# retrieve details
        of all of the primary author&#039;s papers\\npapers &lt;- get_publications(id)\\n#
        sadly this only has 6 authors max for each paper, let&#039;s get the missing
        authors\\npapers$authors &lt;-  papers$author\\n\\n# we&#039;ll use a loop
        to use get_complete_authors() if required\\n# we can test for this because
        papers with missing authors have an ellipse as final author\\nfor(i in 1 :
        nrow(papers)) {\\n  if(grepl(&quot;...&quot;, papers$author&#91;i], fixed
        = TRUE)) {\\n    papers$authors&#91;i] &lt;- get_complete_authors(id, papers$pubid&#91;i])\\n
        \ }\\n}\\n</pre></div>\\n\\n\\n<p>At this point we have the primary author&#8217;s
        info, and a nice data frame of all of the primary author&#8217;s papers with
        number of cites and co-authors per paper.</p>\\n\\n\\n\\n<p>We need to match
        up the Scholar co-authors to the authors in data frame. This involves a bit
        of manipulation because the Scholar co-author can enter their name in any
        format!</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code \\\"><pre
        class=\\\"brush: r; first-line: 22; title: ; notranslate\\\" title=\\\"\\\">\\n#
        authors in the data frame have names like &quot;JD Bloggs&quot;\\n# we need
        names like &quot;j bloggs&quot; to match efficiently\\n\\n# get character
        vector of all authors from the data frame\\nall_authors &lt;- unlist(strsplit(papers$authors,&quot;,
        &quot;))\\n# use this function remove middle initials that we don&#039;t need\\nsimplify_authors
        &lt;- function(x) {\\n  s &lt;- unlist(strsplit(x,&quot; &quot;))\\n  t &lt;-
        paste(substr(s&#91;1],1,1),s&#91;length(s)])\\n  return(t)\\n}\\nall_authors
        &lt;- sapply(all_authors, simplify_authors)\\n\\n# Now, let&#039;s count the
        frequency of each co-author\\ncount_coau &lt;- data.frame(au = tolower(all_authors))
        %&gt;% \\n  group_by(au) %&gt;% \\n  count()\\n# get the Scholar co-authors
        from `l`\\nscholar_coau &lt;- l$coauthors\\n# here I manually added in the
        other Scholar co-authors from the View All modal\\n# and again put them into
        the correct format\\nscholar_coau &lt;- sapply(scholar_coau, simplify_authors)\\n#
        make a data frame of the Scholar co-authors and their rank\\nscholar_df &lt;-
        data.frame(coauthors = tolower(scholar_coau),\\n                         rank
        = seq(1,length(scholar_coau)))\\n# now merge with the data frame with the
        paper count by author\\ncompare_df &lt;- merge(scholar_df, count_coau, by.x
        = &quot;coauthors&quot;, by.y = &quot;au&quot;, sort = FALSE)\\n</pre></div>\\n\\n\\n<p>First,
        let&#8217;s look if Scholar co-author rank is determined by the number of
        papers co-authored with the primary author.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; first-line: 49; title: ; notranslate\\\" title=\\\"\\\">\\n#
        plot the number of papers as a function of rank\\nggplot(compare_df, aes(x
        = rank, y = n)) +\\n  geom_point() +\\n  lims(x = c(0,NA), y = c(0,NA)) +\\n
        \ labs(x = &quot;Scholar Rank&quot;, y  = &quot;Co-authored Papers&quot;)
        +\\n  theme_bw()\\n# so rank is not determined by number of papers\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3088\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/nvsrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"nVsRank\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3088\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>Number of co-authored papers correlates with rank,
        but doesn&#8217;t determine it.</p>\\n\\n\\n\\n<p>If rank is not determined
        (only) by number of co-authored papers, let&#8217;s look at the total citations
        that each co-author shares with the primary author.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; first-line: 56; title: ; notranslate\\\" title=\\\"\\\">\\ncompare_df$cites
        &lt;- 0\\nfor(i in 1 : nrow(compare_df)) {\\n  total_cites &lt;- 0\\n  au
        &lt;- compare_df$coauthors&#91;i]\\n  for(j in 1 : nrow(papers)) {\\n    aus
        &lt;- unlist(strsplit(papers$authors&#91;j],&quot;, &quot;))\\n    aus &lt;-
        sapply(aus,simplify_authors)\\n    aus &lt;- paste(unlist(tolower(aus)), collapse
        = &quot;,&quot;)\\n    if(grepl(au, aus)) {\\n      total_cites &lt;- total_cites
        + papers$cites&#91;j]\\n    }\\n  }\\n  compare_df$cites&#91;i] &lt;- total_cites\\n}\\n\\n#
        plot the total citation as a function of rank\\nggplot(compare_df, aes(x =
        rank, y = cites)) +\\n  geom_point() +\\n  lims(x = c(0,NA), y = c(0,NA))
        +\\n  labs(x = &quot;Scholar Rank&quot;, y  = &quot;Co-cites&quot;) +\\n  theme_bw()\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3089\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/citesvsrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"citesVsRank\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3089\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>Co-cites do not match the rank either.</p>\\n\\n\\n\\n<p>Of
        course, it is possible that the ranking is done by some complex method, e.g.
        the number of co-authors that the co-author has. But if we assume the ranking
        is done using only information on the primary author&#8217;s page, how can
        it be done?</p>\\n\\n\\n\\n<p>Let&#8217;s look at a graph of co-cites and
        number of papers.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; first-line: 77; title: ; notranslate\\\" title=\\\"\\\">\\nggplot(compare_df,
        aes(x = n, y = cites, colour = rank)) +\\n  geom_point() +\\n  scale_colour_gradient(low
        = &quot;red&quot;, high = &quot;blue&quot;) +\\n  lims(x = c(0,NA), y = c(0,NA))
        +\\n  labs(x = &quot;Papers&quot;, y  = &quot;Co-cites&quot;) +\\n  theme_bw()\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3090\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/citesvspapers/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"citesVsPapers\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3090\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>This graph shows that the distance from the origin
        roughly scales inversely with rank.</p>\\n\\n\\n\\n<p>If we take the log2
        transform of the co-cites, we can see this more clearly.</p>\\n\\n\\n<div
        class=\\\"wp-block-syntaxhighlighter-code \\\"><pre class=\\\"brush: r; first-line:
        83; title: ; notranslate\\\" title=\\\"\\\">\\nggplot(compare_df, aes(x =
        n, y = log2(cites), colour = rank)) +\\n  geom_point() +\\n  scale_colour_gradient(low
        = &quot;red&quot;, high = &quot;blue&quot;) +\\n  lims(x = c(0,NA), y = c(0,NA))
        +\\n  labs(x = &quot;Papers&quot;, y  = &quot;Co-cites (log2)&quot;) +\\n
        \ theme_bw()\\n</pre></div>\\n\\n\\n<figure class=\\\"wp-block-image size-large\\\"><img
        data-attachment-id=\\\"3093\\\" data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/citeslogvspapers/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"citesLogVsPapers\\\" data-image-description=\\\"\\\"
        data-image-caption=\\\"\\\" data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3093\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>If we use the manhattan distance of number of papers
        and log2 scaled number of citations, we get something approximating the ranking!</p>\\n\\n\\n<div
        class=\\\"wp-block-syntaxhighlighter-code \\\"><pre class=\\\"brush: r; first-line:
        89; title: ; notranslate\\\" title=\\\"\\\">\\ncompare_df$distance &lt;- compare_df$n
        + log2(compare_df$cites)\\n\\nggplot(compare_df, aes(x = rank, y = distance))
        +\\n  geom_point() +\\n  lims(x = c(0,NA), y = c(0,NA)) +\\n  labs(x = &quot;Rank&quot;,
        y  = &quot;Distance&quot;) +\\n  theme_bw()\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3091\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/distancevsrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"distanceVsRank\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3091\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>This approximation works well. It&#8217;s not perfect.
        There are authors whose distance is not in ranked order with their neighbours.
        On closer inspection it seems that the number of co-authored papers is not
        accurate, or perhaps zero-cited papers are excluded from the paper count.</p>\\n\\n\\n\\n<p>I
        tried this simple strategy on a few other primary authors and could replicate
        their co-authors&#8217; rank order. I&#8217;m not certain this is the algorithm
        used but it certainly seems simple enough to be readily computed on each profile
        page.</p>\\n\\n\\n\\n<p><strong>So the total co-citations and the number of
        co-authored papers is used to compute the rank of co-authors</strong></p>\\n\\n\\n\\n<p>Not
        every co-author is a Scholar co-author. Some don&#8217;t have accounts for
        example. Knowing how the ranking is done, we can ask <strong>which lucky co-author
        could slot into the top co-author spots on my page</strong>, if they made
        an account!</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code \\\"><pre
        class=\\\"brush: r; first-line: 96; title: ; notranslate\\\" title=\\\"\\\">\\n#
        get a list of all co-authors\\nunique_authors &lt;- unique(all_authors)\\n#
        exclude current Scholar co-authors\\nunique_authors &lt;- unique_authors&#91;!(unique_authors
        %in% scholar_coau)]\\n# generate a data frame of these authors in the correct
        format with blank ranking\\ntemp_df &lt;- data.frame(coauthors = tolower(unique_authors),\\n
        \                        rank = 0)\\n# merge to find the cumber of co-authored
        papers\\nother_df &lt;- merge(temp_df, count_coau, by.x = &quot;coauthors&quot;,
        by.y = &quot;au&quot;, sort = FALSE)\\n# remove single paper coauthors for
        ease \\nother_df &lt;- other_df&#91;other_df$n &gt; 1,]\\n\\n# retrieve to
        number of co-citations for each co-author\\nother_df$rank &lt;- 0\\nother_df$cites
        &lt;- 0\\nfor(i in 1 : nrow(other_df)) {\\n  total_cites &lt;- 0\\n  au &lt;-
        other_df$coauthors&#91;i]\\n  for(j in 1 : nrow(papers)) {\\n    aus &lt;-
        unlist(strsplit(papers$authors&#91;j],&quot;, &quot;))\\n    aus &lt;- sapply(aus,simplify_authors)\\n
        \   aus &lt;- paste(unlist(tolower(aus)), collapse = &quot;,&quot;)\\n    if(grepl(au,
        aus)) {\\n      total_cites &lt;- total_cites + papers$cites&#91;j]\\n    }\\n
        \ }\\n  other_df$cites&#91;i] &lt;- total_cites\\n}\\n\\n# get the distance
        used for ranking\\nother_df$distance &lt;- other_df$n + log2(other_df$cites)\\n#
        bind with the original data frame so that we can see where the new co-authors
        slot in\\nall_df &lt;- rbind(compare_df,other_df)\\n# order by distance\\nall_df
        &lt;- all_df&#91;order(all_df$distance, decreasing = TRUE),]\\n# remove primary
        author (should have most cites and papers!)\\nall_df &lt;- all_df&#91;-1,]\\n#
        mark out non-Scholar co-authors\\nall_df$new &lt;- ifelse(all_df$rank == 0,
        1, 0)\\n# make a new column for interpolated rank\\nall_df$interrank &lt;-
        ifelse(all_df$rank == 0, NA, all_df$rank)\\nall_df$interrank &lt;- na.approx(all_df$interrank)\\n#
        plot the result - limit to original top ten\\nggplot(all_df, aes(x = interrank,
        y = distance, colour = as.factor(new))) +\\n  geom_point() +\\n  lims(x =
        c(0,10), y = c(0,NA)) +\\n  labs(x = &quot;Rank&quot;, y  = &quot;Distance&quot;)
        +\\n  theme_bw() +\\n  theme(legend.position = &quot;none&quot;)\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3094\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/distancevsnewrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"distanceVsNewRank\\\" data-image-description=\\\"\\\"
        data-image-caption=\\\"\\\" data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3094\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>Scholar co-authors are shown in salmon while co-authors
        without a Scholar profile are shown in teal. From this plot, we can see that
        the coveted third place is up for grabs, along with the new 5th place. If
        everyone made a Scholar account, the person currently in 6th place would be
        pushed down into 10th.</p>\\n\\n\\n\\n<p>I don&#8217;t imagine for one minute
        that anyone would be motivated to sign up to make it onto the sidebar of my
        page, but this exercise was interesting to highlight to me who my &#8220;closest&#8221;
        co-authors are.</p>\\n\\n\\n\\n<p>&#8212;</p>\\n\\n\\n\\n<p>The post title
        comes from &#8220;All The Right Friends&#8221; by R.E.M. The version I have
        is on a Best Of&#8230; compilation. I have many songs with &#8220;Friends&#8221;
        in the title but this seemed appropriate since the co-author side bar is over
        on the right of the Scholar profile page.</p>\\n\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/gf1j9-s3e70\",\"id\":\"89fe52d6-7c67-4a77-b496-78218aaa0855\",\"image\":\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=640%2C360&ssl=1\",\"language\":\"en\",\"published_at\":1697897693,\"reference\":[],\"relationships\":[],\"summary\":\"On
        a scientist\u2019s Google Scholar page, there is a list of co-authors in the
        sidebar. I\u2019ve often wondered how Google determines in what order these
        co-authors appear. The list of co-authors on a primary author\u2019s page
        is not exhaustive. It only lists co-authors who also have a Google Scholar
        profile.\",\"tags\":[\"Publishing\",\"Google Scholar\",\"Metrics\",\"Rstats\"],\"title\":\"All
        The Right Friends: how does Google Scholar rank co-authors?\",\"updated_at\":1697897697,\"url\":\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors\"},\"highlight\":{\"authors\":[{\"name\":\"Stephen
        Royle\",\"url\":\"https://orcid.org/0000-0001-8927-6967\"}],\"content_html\":\"\\n<p>On
        a scientist&#8217;s Google Scholar page, there is a list of co-authors in
        the sidebar. I&#8217;ve often wondered how Google determines in what order
        these co-authors appear.</p>\\n\\n\\n\\n<p>The list of co-authors on a primary
        author&#8217;s page is not exhaustive. It only lists co-authors who also have
        a Google Scholar profile. They also have to be suggested to the primary author
        and they need to accept the co-author to list them on the page. Finally, the
        profile page only displays the first 20 co-authors. Any further co-authors
        can be seen by clicking &#8220;View All&#8221;. As I understand it, there
        is a limit to the number of co-authors a primary author is allowed to have;
        I currently have 40 and haven&#8217;t yet hit a limit. The ranking of co-authors
        is determined <em>somehow</em> and the first 20 are displayed on the primary
        author&#8217;s profile page, in the sidebar on the right.</p>\\n\\n\\n\\n<p><strong>How
        does Google Scholar rank these co-authors?</strong> Let&#8217;s use R to find
        out!</p>\\n\\n\\n\\n<p>We&#8217;ll make use of the <a href=\\\"https://github.com/jkeirstead/scholar\\\">scholar</a>
        package to get the data. The primary author used in the package vignette is
        Albert Einstein, but he doesn&#8217;t have any co-authors on Google Scholar;
        so we&#8217;ll use my data instead.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; title: ; notranslate\\\" title=\\\"\\\">\\nlibrary(scholar)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(zoo)\\n\\n#
        use a Google Scholar ID here\\nid &lt;- &quot;PBcP8-oAAAAJ&quot;\\n# retrieve
        all the info from the page\\nl &lt;- get_profile(id)\\n# retrieve details
        of all of the primary author&#039;s papers\\npapers &lt;- get_publications(id)\\n#
        sadly this only has 6 authors max for each paper, let&#039;s get the missing
        authors\\npapers$authors &lt;-  papers$author\\n\\n# we&#039;ll use a loop
        to use get_complete_authors() if required\\n# we can test for this because
        papers with missing authors have an ellipse as final author\\nfor(i in 1 :
        nrow(papers)) {\\n  if(grepl(&quot;...&quot;, papers$author&#91;i], fixed
        = TRUE)) {\\n    papers$authors&#91;i] &lt;- get_complete_authors(id, papers$pubid&#91;i])\\n
        \ }\\n}\\n</pre></div>\\n\\n\\n<p>At this point we have the primary author&#8217;s
        info, and a nice data frame of all of the primary author&#8217;s papers with
        number of cites and co-authors per paper.</p>\\n\\n\\n\\n<p>We need to match
        up the Scholar co-authors to the authors in data frame. This involves a bit
        of manipulation because the Scholar co-author can enter their name in any
        format!</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code \\\"><pre
        class=\\\"brush: r; first-line: 22; title: ; notranslate\\\" title=\\\"\\\">\\n#
        authors in the data frame have names like &quot;JD Bloggs&quot;\\n# we need
        names like &quot;j bloggs&quot; to match efficiently\\n\\n# get character
        vector of all authors from the data frame\\nall_authors &lt;- unlist(strsplit(papers$authors,&quot;,
        &quot;))\\n# use this function remove middle initials that we don&#039;t need\\nsimplify_authors
        &lt;- function(x) {\\n  s &lt;- unlist(strsplit(x,&quot; &quot;))\\n  t &lt;-
        paste(substr(s&#91;1],1,1),s&#91;length(s)])\\n  return(t)\\n}\\nall_authors
        &lt;- sapply(all_authors, simplify_authors)\\n\\n# Now, let&#039;s count the
        frequency of each co-author\\ncount_coau &lt;- data.frame(au = tolower(all_authors))
        %&gt;% \\n  group_by(au) %&gt;% \\n  count()\\n# get the Scholar co-authors
        from `l`\\nscholar_coau &lt;- l$coauthors\\n# here I manually added in the
        other Scholar co-authors from the View All modal\\n# and again put them into
        the correct format\\nscholar_coau &lt;- sapply(scholar_coau, simplify_authors)\\n#
        make a data frame of the Scholar co-authors and their rank\\nscholar_df &lt;-
        data.frame(coauthors = tolower(scholar_coau),\\n                         rank
        = seq(1,length(scholar_coau)))\\n# now merge with the data frame with the
        paper count by author\\ncompare_df &lt;- merge(scholar_df, count_coau, by.x
        = &quot;coauthors&quot;, by.y = &quot;au&quot;, sort = FALSE)\\n</pre></div>\\n\\n\\n<p>First,
        let&#8217;s look if Scholar co-author rank is determined by the number of
        papers co-authored with the primary author.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; first-line: 49; title: ; notranslate\\\" title=\\\"\\\">\\n#
        plot the number of papers as a function of rank\\nggplot(compare_df, aes(x
        = rank, y = n)) +\\n  geom_point() +\\n  lims(x = c(0,NA), y = c(0,NA)) +\\n
        \ labs(x = &quot;Scholar Rank&quot;, y  = &quot;Co-authored Papers&quot;)
        +\\n  theme_bw()\\n# so rank is not determined by number of papers\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3088\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/nvsrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"nVsRank\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3088\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/nVsRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>Number of co-authored papers correlates with rank,
        but doesn&#8217;t determine it.</p>\\n\\n\\n\\n<p>If rank is not determined
        (only) by number of co-authored papers, let&#8217;s look at the total citations
        that each co-author shares with the primary author.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; first-line: 56; title: ; notranslate\\\" title=\\\"\\\">\\ncompare_df$cites
        &lt;- 0\\nfor(i in 1 : nrow(compare_df)) {\\n  total_cites &lt;- 0\\n  au
        &lt;- compare_df$coauthors&#91;i]\\n  for(j in 1 : nrow(papers)) {\\n    aus
        &lt;- unlist(strsplit(papers$authors&#91;j],&quot;, &quot;))\\n    aus &lt;-
        sapply(aus,simplify_authors)\\n    aus &lt;- paste(unlist(tolower(aus)), collapse
        = &quot;,&quot;)\\n    if(grepl(au, aus)) {\\n      total_cites &lt;- total_cites
        + papers$cites&#91;j]\\n    }\\n  }\\n  compare_df$cites&#91;i] &lt;- total_cites\\n}\\n\\n#
        plot the total citation as a function of rank\\nggplot(compare_df, aes(x =
        rank, y = cites)) +\\n  geom_point() +\\n  lims(x = c(0,NA), y = c(0,NA))
        +\\n  labs(x = &quot;Scholar Rank&quot;, y  = &quot;Co-cites&quot;) +\\n  theme_bw()\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3089\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/citesvsrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"citesVsRank\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3089\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>Co-cites do not match the rank either.</p>\\n\\n\\n\\n<p>Of
        course, it is possible that the ranking is done by some complex method, e.g.
        the number of co-authors that the co-author has. But if we assume the ranking
        is done using only information on the primary author&#8217;s page, how can
        it be done?</p>\\n\\n\\n\\n<p>Let&#8217;s look at a graph of co-cites and
        number of papers.</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code
        \\\"><pre class=\\\"brush: r; first-line: 77; title: ; notranslate\\\" title=\\\"\\\">\\nggplot(compare_df,
        aes(x = n, y = cites, colour = rank)) +\\n  geom_point() +\\n  scale_colour_gradient(low
        = &quot;red&quot;, high = &quot;blue&quot;) +\\n  lims(x = c(0,NA), y = c(0,NA))
        +\\n  labs(x = &quot;Papers&quot;, y  = &quot;Co-cites&quot;) +\\n  theme_bw()\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3090\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/citesvspapers/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"citesVsPapers\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3090\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesVsPapers.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>This graph shows that the distance from the origin
        roughly scales inversely with rank.</p>\\n\\n\\n\\n<p>If we take the log2
        transform of the co-cites, we can see this more clearly.</p>\\n\\n\\n<div
        class=\\\"wp-block-syntaxhighlighter-code \\\"><pre class=\\\"brush: r; first-line:
        83; title: ; notranslate\\\" title=\\\"\\\">\\nggplot(compare_df, aes(x =
        n, y = log2(cites), colour = rank)) +\\n  geom_point() +\\n  scale_colour_gradient(low
        = &quot;red&quot;, high = &quot;blue&quot;) +\\n  lims(x = c(0,NA), y = c(0,NA))
        +\\n  labs(x = &quot;Papers&quot;, y  = &quot;Co-cites (log2)&quot;) +\\n
        \ theme_bw()\\n</pre></div>\\n\\n\\n<figure class=\\\"wp-block-image size-large\\\"><img
        data-attachment-id=\\\"3093\\\" data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/citeslogvspapers/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"citesLogVsPapers\\\" data-image-description=\\\"\\\"
        data-image-caption=\\\"\\\" data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3093\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/citesLogVsPapers.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>If we use the manhattan distance of number of papers
        and log2 scaled number of citations, we get something approximating the ranking!</p>\\n\\n\\n<div
        class=\\\"wp-block-syntaxhighlighter-code \\\"><pre class=\\\"brush: r; first-line:
        89; title: ; notranslate\\\" title=\\\"\\\">\\ncompare_df$distance &lt;- compare_df$n
        + log2(compare_df$cites)\\n\\nggplot(compare_df, aes(x = rank, y = distance))
        +\\n  geom_point() +\\n  lims(x = c(0,NA), y = c(0,NA)) +\\n  labs(x = &quot;Rank&quot;,
        y  = &quot;Distance&quot;) +\\n  theme_bw()\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3091\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/distancevsrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"distanceVsRank\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3091\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>This approximation works well. It&#8217;s not perfect.
        There are authors whose distance is not in ranked order with their neighbours.
        On closer inspection it seems that the number of co-authored papers is not
        accurate, or perhaps zero-cited papers are excluded from the paper count.</p>\\n\\n\\n\\n<p>I
        tried this simple strategy on a few other primary authors and could replicate
        their co-authors&#8217; rank order. I&#8217;m not certain this is the algorithm
        used but it certainly seems simple enough to be readily computed on each profile
        page.</p>\\n\\n\\n\\n<p><strong>So the total co-citations and the number of
        co-authored papers is used to compute the rank of co-authors</strong></p>\\n\\n\\n\\n<p>Not
        every co-author is a Scholar co-author. Some don&#8217;t have accounts for
        example. Knowing how the ranking is done, we can ask <strong>which lucky co-author
        could slot into the top co-author spots on my page</strong>, if they made
        an account!</p>\\n\\n\\n<div class=\\\"wp-block-syntaxhighlighter-code \\\"><pre
        class=\\\"brush: r; first-line: 96; title: ; notranslate\\\" title=\\\"\\\">\\n#
        get a list of all co-authors\\nunique_authors &lt;- unique(all_authors)\\n#
        exclude current Scholar co-authors\\nunique_authors &lt;- unique_authors&#91;!(unique_authors
        %in% scholar_coau)]\\n# generate a data frame of these authors in the correct
        format with blank ranking\\ntemp_df &lt;- data.frame(coauthors = tolower(unique_authors),\\n
        \                        rank = 0)\\n# merge to find the cumber of co-authored
        papers\\nother_df &lt;- merge(temp_df, count_coau, by.x = &quot;coauthors&quot;,
        by.y = &quot;au&quot;, sort = FALSE)\\n# remove single paper coauthors for
        ease \\nother_df &lt;- other_df&#91;other_df$n &gt; 1,]\\n\\n# retrieve to
        number of co-citations for each co-author\\nother_df$rank &lt;- 0\\nother_df$cites
        &lt;- 0\\nfor(i in 1 : nrow(other_df)) {\\n  total_cites &lt;- 0\\n  au &lt;-
        other_df$coauthors&#91;i]\\n  for(j in 1 : nrow(papers)) {\\n    aus &lt;-
        unlist(strsplit(papers$authors&#91;j],&quot;, &quot;))\\n    aus &lt;- sapply(aus,simplify_authors)\\n
        \   aus &lt;- paste(unlist(tolower(aus)), collapse = &quot;,&quot;)\\n    if(grepl(au,
        aus)) {\\n      total_cites &lt;- total_cites + papers$cites&#91;j]\\n    }\\n
        \ }\\n  other_df$cites&#91;i] &lt;- total_cites\\n}\\n\\n# get the distance
        used for ranking\\nother_df$distance &lt;- other_df$n + log2(other_df$cites)\\n#
        bind with the original data frame so that we can see where the new co-authors
        slot in\\nall_df &lt;- rbind(compare_df,other_df)\\n# order by distance\\nall_df
        &lt;- all_df&#91;order(all_df$distance, decreasing = TRUE),]\\n# remove primary
        author (should have most cites and papers!)\\nall_df &lt;- all_df&#91;-1,]\\n#
        mark out non-Scholar co-authors\\nall_df$new &lt;- ifelse(all_df$rank == 0,
        1, 0)\\n# make a new column for interpolated rank\\nall_df$interrank &lt;-
        ifelse(all_df$rank == 0, NA, all_df$rank)\\nall_df$interrank &lt;- na.approx(all_df$interrank)\\n#
        plot the result - limit to original top ten\\nggplot(all_df, aes(x = interrank,
        y = distance, colour = as.factor(new))) +\\n  geom_point() +\\n  lims(x =
        c(0,10), y = c(0,NA)) +\\n  labs(x = &quot;Rank&quot;, y  = &quot;Distance&quot;)
        +\\n  theme_bw() +\\n  theme(legend.position = &quot;none&quot;)\\n</pre></div>\\n\\n\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"3094\\\"
        data-permalink=\\\"https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/distancevsnewrank/\\\"
        data-orig-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?fit=1600%2C900&amp;ssl=1\\\"
        data-orig-size=\\\"1600,900\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"distanceVsNewRank\\\" data-image-description=\\\"\\\"
        data-image-caption=\\\"\\\" data-medium-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?fit=300%2C169&amp;ssl=1\\\"
        data-large-file=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?fit=640%2C360&amp;ssl=1\\\"
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"640\\\" height=\\\"360\\\"
        src=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=640%2C360&#038;ssl=1\\\"
        alt=\\\"\\\" class=\\\"wp-image-3094\\\" srcset=\\\"https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=1024%2C576&amp;ssl=1
        1024w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=300%2C169&amp;ssl=1
        300w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=768%2C432&amp;ssl=1
        768w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?resize=1536%2C864&amp;ssl=1
        1536w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?w=1600&amp;ssl=1
        1600w, https://i0.wp.com/quantixed.org/wp-content/uploads/2023/10/distanceVsNewRank.png?w=1280&amp;ssl=1
        1280w\\\" sizes=\\\"(max-width: 640px) 100vw, 640px\\\" data-recalc-dims=\\\"1\\\"
        /></figure>\\n\\n\\n\\n<p>Scholar co-authors are shown in salmon while co-authors
        without a Scholar profile are shown in teal. From this plot, we can see that
        the coveted third place is up for grabs, along with the new 5th place. If
        everyone made a Scholar account, the person currently in 6th place would be
        pushed down into 10th.</p>\\n\\n\\n\\n<p>I don&#8217;t imagine for one minute
        that anyone would be motivated to sign up to make it onto the sidebar of my
        page, but this exercise was interesting to highlight to me who my &#8220;closest&#8221;
        co-authors are.</p>\\n\\n\\n\\n<p>&#8212;</p>\\n\\n\\n\\n<p>The post title
        comes from &#8220;All The Right Friends&#8221; by R.E.M. The version I have
        is on a Best Of&#8230; compilation. I have many songs with &#8220;Friends&#8221;
        in the title but this seemed appropriate since the co-author side bar is over
        on the right of the Scholar profile page.</p>\\n\",\"doi\":\"https://doi.org/10.59350/gf1j9-s3e70\",\"reference\":[],\"summary\":\"On
        a scientist\u2019s Google Scholar page, there is a list of co-authors in the
        sidebar. I\u2019ve often wondered how Google determines in what order these
        co-authors appear. The list of co-authors on a primary author\u2019s page
        is not exhaustive. It only lists co-authors who also have a Google Scholar
        profile.\",\"tags\":[\"Publishing\",\"Google Scholar\",\"Metrics\",\"Rstats\"],\"title\":\"All
        The Right Friends: how does Google Scholar rank co-authors?\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"The
        rOpenSci Team\"}],\"blog_id\":\"e22ws68\",\"blog_name\":\"rOpenSci - open
        tools for open science\",\"blog_slug\":\"ropensci\",\"content_html\":\"<!--
        Before sending DELETE THE INDEX_CACHE and re-knit! --><p>Dear rOpenSci friends,
        it&rsquo;s time for our monthly news roundup!</p><!-- blabla --><p>You can
        read this post <a href=\\\"/blog/2023/10/20/news-october-2023\\\">on our blog</a>.Now
        let&rsquo;s dive into the activity at and around rOpenSci!</p><h2 id=\\\"ropensci-hq\\\">rOpenSci
        HQ</h2><h3 id=\\\"help-us-translate-our-dev-guide-to-portuguese\\\">Help us
        translate our dev guide to Portuguese</h3><p>Since last year we started the
        translation and localization of our Spanish version of our <a href=\\\"https://devguide.ropensci.org/\\\">comprehensive
        guide to software development</a>. We have a <a href=\\\"https://devdevguide.netlify.app/es/preface.es.html\\\">first
        version in Spanish</a> and now, thanks to the R Community initiative, we are
        working on the Portuguese version.</p><p>Our process includes the review of
        automatic translations by two people, and we need your help!</p><p>If you
        are interested in collaborating with this community effort, please check the
        &ldquo;TODO - Second Review&rdquo; tab of <a href=\\\"https://github.com/orgs/ropensci/projects/7\\\">this
        project on GitHub</a> and let us know you want to collaborate by leaving a
        comment in the corresponding Pull Request.</p><p>All contributors will be
        added to the rOpenSci Slack, <a href=\\\"/multilingual-publishing/\\\">listed
        on our multilingual publishing project webpage</a> and acknowledged as authors
        of the Portuguese translation.</p><h3 id=\\\"coworking\\\">Coworking</h3><p>Read
        <a href=\\\"/blog/2023/06/21/coworking/\\\">all about coworking</a> in our
        recent <a href=\\\"/blog/2023/06/21/coworking/\\\">post</a>!</p><p>Join us
        for social coworking &amp; office hours monthly on first Tuesdays!Hosted by
        Steffi LaZerte and various community hosts.Everyone welcome.No RSVP needed.Consult
        our <a href=\\\"/events\\\">Events</a> page to find your local time and how
        to join.</p><ul><li><a href=\\\"/events/coworking-2023-11/\\\">Tuesday, Nov
        7th, 9:00 America Pacific (17:00 UTC)</a>, Fixing scary bugs \U0001F631\U0001F41B
        With community host <a href=\\\"/author/salix-dubois/\\\">Salix Dubois</a>
        and <a href=\\\"/author/steffi-lazerte/\\\">Steffi LaZerte</a>.<ul><li>Identify/fix
        some scary bugs;</li><li>Chat with Salix about scary bugs!</li></ul></li></ul><p>And
        remember, you can always cowork independently on work related to R, work on
        packages that tend to be neglected, or work on what ever you need to get done!</p><h3
        id=\\\"reminder-three-upcoming-community-calls-and-our-first-one-in-spanish\\\">Reminder:
        Three upcoming community calls, and our first one in Spanish!</h3><h4 id=\\\"r-in-governmentcommcallsoct2023-government\\\"><a
        href=\\\"/commcalls/oct2023-government/\\\">R in Government</a></h4><p>In
        this community call, our panelists will share their experiences and examples
        of projects with R at different levels of government and in different countries.We
        invite you to learn about the challenges and lessons learned from our panelists
        and attendees in their efforts to make their government data, processes, and
        analyses more open and reproducible.</p><p>With <a href=\\\"https://ropensci.org/author/lu%C3%ADza-andrade/\\\">Lu\xEDza
        Andrade</a>, <a href=\\\"https://ropensci.org/author/karly-harker/\\\">Karly
        Harker</a>, <a href=\\\"https://ropensci.org/author/ahmadou-dicko/\\\">Ahmadou
        Dicko</a>, <a href=\\\"https://ropensci.org/author/pablo-tiscornia/\\\">Pablo
        Tiscornia</a>.</p><p><strong>Tuesday, 31 October 2023 16:00 UTC</strong></p><h4
        id=\\\"multilingual-publishing\\\">Multilingual Publishing</h4><p>As global
        movements, Open Source and Open Science face language-based exclusion as most
        resources are in English. This affects scientists and research software engineers
        working in R, particularly those who don\u2019t have English as their first
        language.</p><p>rOpenSci multilingual efforts aim to lower access barriers,
        democratize quality resources, and increase the possibilities of contributing
        to open software and science. We successfully piloted our Spanish-language
        peer review and the localization to Spanish of our comprehensive guide to
        software development, with Portuguese translation underway.</p><p><a href=\\\"/author/ma%C3%ABlle-salmon/\\\">Ma\xEBlle
        Salmon</a>, <a href=\\\"/author/paola-corrales/\\\">Paola Corrales</a>, and
        <a href=\\\"/author/elio-campitelli/\\\">Elio Campitelli</a>,  will share
        the rOpenSci Multilingual project details on this call. Ma\xEBlle will present
        the R packages that allow us to have our content in several languages. Then
        Elio and Paola will share the translation workflow and show the Translation
        Guide written to document the process.</p><ul><li><p><a href=\\\"/commcalls/nov2023-multilingual/\\\"><strong>Multilingual
        Publishing</strong>:  Tuesday, 21 November 2023 12:00 UTC</a>.</p></li><li><p><a
        href=\\\"/commcalls/nov2023-multilenguaje/\\\"><strong>Proyecto Multiling\xFCe</strong>:
        Thursday, 23 November 2023 12:00 UTC</a>.</p></li></ul><h2 id=\\\"software--new-versions\\\">Software
        \U0001F4E6: new versions</h2><p>The following twenty-three packages have had
        an update since the last newsletter: <a href=\\\"https://docs.ropensci.org/gert\\\"
        title=\\\"Simple Git Client for R\\\">gert</a> (<a href=\\\"https://github.com/r-lib/gert/releases/tag/v2.0.0\\\"><code>v2.0.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/aorsf\\\" title=\\\"Accelerated Oblique
        Random Survival Forests\\\">aorsf</a> (<a href=\\\"https://github.com/ropensci/aorsf/releases/tag/v0.1.1\\\"><code>v0.1.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/beastier\\\" title=\\\"Call BEAST2\\\">beastier</a>
        (<a href=\\\"https://github.com/ropensci/beastier/releases/tag/v2.4.12\\\"><code>v2.4.12</code></a>),
        <a href=\\\"https://docs.ropensci.org/beautier\\\" title=\\\"BEAUti from R\\\">beautier</a>
        (<a href=\\\"https://github.com/ropensci/beautier/releases/tag/v2.6.9\\\"><code>v2.6.9</code></a>),
        <a href=\\\"https://docs.ropensci.org/biomartr\\\" title=\\\"Genomic Data
        Retrieval\\\">biomartr</a> (<a href=\\\"https://github.com/ropensci/biomartr/releases/tag/v1.0.5\\\"><code>v1.0.5</code></a>),
        <a href=\\\"https://docs.ropensci.org/drake\\\" title=\\\"A Pipeline Toolkit
        for Reproducible Computation at Scale\\\">drake</a> (<a href=\\\"https://github.com/ropensci/drake/releases/tag/7.13.6\\\"><code>7.13.6</code></a>),
        <a href=\\\"https://docs.ropensci.org/EDIutils\\\" title=\\\"An API Client
        for the Environmental Data Initiative Repository\\\">EDIutils</a> (<a href=\\\"https://github.com/ropensci/EDIutils/releases/tag/v1.0.3\\\"><code>v1.0.3</code></a>),
        <a href=\\\"https://docs.ropensci.org/epubr\\\" title=\\\"Read EPUB File Metadata
        and Text\\\">epubr</a> (<a href=\\\"https://github.com/ropensci/epubr/releases/tag/v0.6.4\\\"><code>v0.6.4</code></a>),
        <a href=\\\"https://docs.ropensci.org/FedData\\\" title=\\\"Functions to Automate
        Downloading Geospatial Data Available fromSeveral Federated Data Sources\\\">FedData</a>
        (<a href=\\\"https://github.com/ropensci/FedData/releases/tag/v4.0.0\\\"><code>v4.0.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/hunspell\\\" title=\\\"High-Performance
        Stemmer, Tokenizer, and Spell Checker\\\">hunspell</a> (<a href=\\\"https://github.com/ropensci/hunspell/releases/tag/v3.0.3\\\"><code>v3.0.3</code></a>),
        <a href=\\\"https://docs.ropensci.org/MODIStsp\\\" title=\\\"Find, Download
        and Process MODIS Land ProductsData\\\">MODIStsp</a> (<a href=\\\"https://github.com/ropensci/MODIStsp/releases/tag/v2.1.0\\\"><code>v2.1.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/nodbi\\\" title=\\\"NoSQL Database Connector\\\">nodbi</a>
        (<a href=\\\"https://github.com/ropensci/nodbi/releases/tag/v0.9.8\\\"><code>v0.9.8</code></a>),
        <a href=\\\"https://docs.ropensci.org/prism\\\" title=\\\"Access Data from
        the Oregon State Prism Climate Project\\\">prism</a> (<a href=\\\"https://github.com/ropensci/prism/releases/tag/v0.2.1\\\"><code>v0.2.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/rdhs\\\" title=\\\"API Client and Dataset
        Management for the Demographic and Health Survey (DHS) Data\\\">rdhs</a> (<a
        href=\\\"https://github.com/ropensci/rdhs/releases/tag/v0.8.0\\\"><code>v0.8.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/rglobi\\\" title=\\\"Interface to Global
        Biotic Interactions\\\">rglobi</a> (<a href=\\\"https://github.com/ropensci/rglobi/releases/tag/v0.3.4\\\"><code>v0.3.4</code></a>),
        <a href=\\\"https://docs.ropensci.org/rtweet\\\" title=\\\"Collecting Twitter
        Data\\\">rtweet</a> (<a href=\\\"https://github.com/ropensci/rtweet/releases/tag/v1.2.1\\\"><code>v1.2.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/stats19\\\" title=\\\"Work with Open
        Road Traffic Casualty Data from Great Britain\\\">stats19</a> (<a href=\\\"https://github.com/ropensci/stats19/releases/tag/v3.0.0\\\"><code>v3.0.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/tarchetypes\\\" title=\\\"Archetypes
        for Targets\\\">tarchetypes</a> (<a href=\\\"https://github.com/ropensci/tarchetypes/releases/tag/0.7.9\\\"><code>0.7.9</code></a>),
        <a href=\\\"https://docs.ropensci.org/targets\\\" title=\\\"Dynamic Function-Oriented
        Make-Like Declarative Pipelines\\\">targets</a> (<a href=\\\"https://github.com/ropensci/targets/releases/tag/1.3.2\\\"><code>1.3.2</code></a>),
        <a href=\\\"https://docs.ropensci.org/terrainr\\\" title=\\\"Landscape Visualizations
        in R and Unity\\\">terrainr</a> (<a href=\\\"https://github.com/ropensci/terrainr/releases/tag/v0.7.5\\\"><code>v0.7.5</code></a>),
        <a href=\\\"https://docs.ropensci.org/tiler\\\" title=\\\"Create Geographic
        and Non-Geographic Map Tiles\\\">tiler</a> (<a href=\\\"https://github.com/ropensci/tiler/releases/tag/v0.3.1\\\"><code>v0.3.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/tracerer\\\" title=\\\"Tracer from R\\\">tracerer</a>
        (<a href=\\\"https://github.com/ropensci/tracerer/releases/tag/v2.2.3\\\"><code>v2.2.3</code></a>),
        and <a href=\\\"https://docs.ropensci.org/waywiser\\\" title=\\\"Ergonomic
        Methods for Assessing Spatial Models\\\">waywiser</a> (<a href=\\\"https://github.com/ropensci/waywiser/releases/tag/v0.5.0\\\"><code>v0.5.0</code></a>).</p><h2
        id=\\\"software-peer-review\\\">Software Peer Review</h2><p>There are eighteen
        recently closed and active submissions and 3 submissions on hold. Issues are
        at different stages:</p><ul><li><p>Four at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A4/review(s)-in-awaiting-changes\\\">&lsquo;4/review(s)-in-awaiting-changes&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/606\\\">fastMatMR</a>,
        &ldquo;fastMatMR: High-Performance Matrix Market File Operations in R&rdquo;.
        Submitted by <a href=\\\"https://rgoswami.me\\\">Rohit Goswami</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/600\\\">naijR</a>,
        Operations to Ease Data Analyses Specific to Nigeria. Submitted by <a href=\\\"https://victorordu.wordpress.com\\\">Victor
        Ordu </a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/522\\\">wmm</a>,
        World Magnetic Model. Submitted by <a href=\\\"https://github.com/wfrierson\\\">Will
        Frierson</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/502\\\">octolog</a>,
        Better Github Action Logging. Submitted by <a href=\\\"https://github.com/assignUser\\\">Jacob
        Wujciak-Jens</a>.</p></li></ul></li><li><p>Seven at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A3/reviewer(s)-assigned\\\">&lsquo;3/reviewer(s)-assigned&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/603\\\">GLMMcosinor</a>,
        Fit a cosinor model using a generalised mixed modelling framework. Submitted
        by <a href=\\\"https://rwparsons.github.io/\\\">Rex Parsons</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/595\\\">rangr</a>,
        Mechanistic Simulation of Species Range Dynamics. Submitted by <a href=\\\"https://github.com/katarzynam-165\\\">Katarzyna
        Markowska</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/590\\\">mregions2</a>,
        Access Data from Marineregions.org: The Marine Regions Gazetteer and the Marine
        Regions Data Products. Submitted by <a href=\\\"https://salvafern.github.io/\\\">salvafern</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/575\\\">pangoling</a>,
        Access to Large Language Model Predictions. Submitted by <a href=\\\"https://bnicenboim.github.io/\\\">Bruno
        Nicenboim</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/556\\\">dfms</a>,
        Dynamic Factor Models. Submitted by <a href=\\\"https://github.com/SebKrantz\\\">Sebastian
        Krantz</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/546\\\">fwildclusterboot</a>,
        Fast Wild Cluster Bootstrap Inference for Linear Models. Submitted by <a href=\\\"https://s3alfisc.github.io/blog/\\\">Alexander
        Fischer</a>.  (Stats).</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/603\\\">GLMMcosinor</a>,
        Fit a cosinor model using a generalised mixed modelling framework. Submitted
        by <a href=\\\"https://rwparsons.github.io/\\\">Rex Parsons</a>.</p></li></ul></li><li><p>Four
        at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A2/seeking-reviewer(s)\\\">&lsquo;2/seeking-reviewer(s)&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/613\\\">comtradr</a>,
        Interface with the United Nations Comtrade API. Submitted by <a href=\\\"https://github.com/datapumpernickel\\\">paulbochtler</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/609\\\">baRulho</a>,
        Quantifying (Animal) Sound Degradation. Submitted by <a href=\\\"https://marceloarayasalas.weebly.com/\\\">Marcelo
        Araya-Salas</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/598\\\">weatherOz</a>,
        An API Client for Australian Weather and Climate Data Resources. Submitted
        by <a href=\\\"https://github.com/bozaah\\\">Rodrigo Pires</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/489\\\">bssm</a>,
        Bayesian Inference of Non-Linear and Non-Gaussian State Space. Submitted by
        <a href=\\\"https://jounihelske.netlify.app\\\">Jouni Helske</a>.  (Stats).</p></li></ul></li><li><p>Three
        at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A1/editor-checks\\\">&lsquo;1/editor-checks&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/608\\\">rgeeExtra</a>,
        Extensions for rgee. Submitted by <a href=\\\"http://csaybar.github.io\\\">Cesar
        Aybar</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/599\\\">agromet</a>,
        \xCDndices y Estad\xEDsticos Clim\xE1ticos e Hidrol\xF3gicos. Submitted by
        <a href=\\\"http://paocorrales.github.io\\\">Paola Corrales</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/572\\\">qualtdict</a>,
        Generating Variable Dictionaries and Labelled Data Exports of Qualtrics. Submitted
        by <a href=\\\"https://github.com/lyh970817\\\">lyh970817</a>.</p></li></ul></li></ul><p>Find
        out more about <a href=\\\"/software-review\\\">Software Peer Review</a> and
        how to get involved.</p><h2 id=\\\"on-the-blog\\\">On the blog</h2><!-- Do
        not forget to rebase your branch! --><ul><li><a href=\\\"/blog/2023/09/26/how-to-translate-a-hugo-blog-post-with-babeldown\\\">How
        to Translate a Hugo Blog Post with Babeldown</a> by Ma\xEBlle Salmon, and
        Yanina Bellini Saibene. Other languages: <a href='/es/blog/2023/09/26/c\xF3mo_traducir_una_entrada_de_blog_de_hugo_con_babeldown'
        lang='es'>C\xF3mo traducir un art\xEDculo de blog de Hugo con Babeldown (es)</a>,
        <a href='/fr/blog/2023/09/26/comment_traduire_un_billet_de_blog_hugo_avec_babeldown'
        lang='fr'>Comment traduire un billet de blog Hugo avec Babeldown (fr)</a>.</li></ul><h2
        id=\\\"call-for-maintainers\\\">Call for maintainers</h2><p>If you&rsquo;re
        interested in maintaining any of the R packages below, you might enjoy reading
        our blog post <a href=\\\"/blog/2023/02/07/what-does-it-mean-to-maintain-a-package/\\\">What
        Does It Mean to Maintain a Package?</a> (or listening to its discussion on
        the <a href=\\\"https://rweekly.fireside.fm/111\\\">R Weekly highlights podcast</a>
        hosted by Eric Nantz and Mike Thomas)!</p><ul><li><strong><a href=\\\"https://cran.r-project.org/web/packages/rvertnet/index.html\\\">rvertnet</a></strong>,
        Retrieve, map and summarize data from the VertNet.org archives (<a href=\\\"https://vertnet.org/\\\">https://vertnet.org/</a>).
        Functions allow searching by many parameters, including taxonomic names, places,
        and dates. In addition, there is an interface for conducting spatially delimited
        searches, and another for requesting large datasets via email. <a href=\\\"https://github.com/ropensci-archive/rvertnet/issues/71\\\">Issue
        for volunteering</a>.</li></ul><h3 id=\\\"call-for-co-maintainers\\\">Call
        for co-maintainers</h3><p>Refer to our somewhat <a href=\\\"/blog/2022/10/17/maintain-or-co-maintain-an-ropensci-package/#packages-looking-for-co-maintainers\\\">recent
        blog post</a> to identify other packages where help is especially wished for!See
        also our <a href=\\\"/help-wanted/\\\">help wanted page</a> &ndash; before
        opening a PR, we recommend asking in the issue whether help is still needed.</p><h2
        id=\\\"package-development-corner\\\">Package development corner</h2><p>Some
        useful tips for R package developers. \U0001F440</p><h3 id=\\\"system-dependencies-in-r-packages--automatic-testing\\\">System
        Dependencies in R Packages &amp; Automatic Testing</h3><p>Are you curious
        about, or stalled by, the installation of system dependencies in your continuous
        integration workflows?Refer to <a href=\\\"https://blog.r-hub.io/2023/09/26/system-dependency/\\\">Hugo
        Gruson&rsquo;s clear and extensive post</a> on the R-hub blog.</p><h3 id=\\\"translate-your-packages-messages-with-potools\\\">Translate
        your package&rsquo;s messages with {potools}</h3><p>The <a href=\\\"https://michaelchirico.github.io/potools/\\\">potools</a>
        package by Michael Chirico is to translation files what roxygen2 is to Rd
        documentation files: it very much simplifies your writing and maintaining
        them!Refer to <a href=\\\"https://michaelchirico.github.io/potools/\\\">potools
        documentation</a> or a recent <a href=\\\"https://masalmon.eu/2023/10/06/potools-mwe/\\\">tutorial</a>.</p><h3
        id=\\\"testthat-new-release\\\">testthat new release</h3><p>If you use testthat
        for your package tests, don&rsquo;t miss the <a href=\\\"https://www.tidyverse.org/blog/2023/10/testthat-3-2-0/\\\">release
        announcement</a> of testthat 3.2.0!That post describes the major features
        such as the return of mocking support within testthat itself.</p><p>Among
        <a href=\\\"https://testthat.r-lib.org/news/index.html#minor-features-and-bug-fixes-3-2-0\\\">minor
        features</a> you might notice the new <code>desc</code> argument of <code>testthat::test_file()</code>
        to run a single test at a time.You can now for instance run <code>devtools::test_active_file(desc
        = 'blop() runs')</code></p><h2 id=\\\"last-words\\\">Last words</h2><p>Thanks
        for reading! If you want to get involved with rOpenSci, check out our <a href=\\\"https://contributing.ropensci.org\\\">Contributing
        Guide</a> that can help direct you to the right place, whether you want to
        make code contributions, non-code contributions, or contribute in other ways
        like sharing use cases.</p><p>If you haven&rsquo;t subscribed to our newsletter
        yet, you can <a href=\\\"/news/\\\">do so via a form</a>. Until it&rsquo;s
        time for our next newsletter, you can keep in touch with us via our <a href=\\\"/\\\">website</a>
        and <a href=\\\"https://hachyderm.io/@rOpenSci\\\">Mastodon account</a>.</p>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/c6935-n5w84\",\"id\":\"25797\",\"language\":\"en\",\"published_at\":1697760000,\"reference\":[],\"relationships\":[],\"summary\":\"Dear
        rOpenSci friends, it\u2019s time for our monthly news roundup!\",\"tags\":[\"Newsletter\"],\"title\":\"rOpenSci
        News Digest, October 2023\",\"updated_at\":1697788477,\"url\":\"https://ropensci.org/blog/2023/10/20/news-october-2023\",\"uuid\":\"35bea8e0-7c8d-475c-83b0-c9c10b96f468\"},\"highlight\":{\"authors\":[{\"name\":\"The
        rOpenSci Team\"}],\"content_html\":\"<!-- Before sending DELETE THE INDEX_CACHE
        and re-knit! --><p>Dear rOpenSci friends, it&rsquo;s time for our monthly
        news roundup!</p><!-- blabla --><p>You can read this post <a href=\\\"/blog/2023/10/20/news-october-2023\\\">on
        our blog</a>.Now let&rsquo;s dive into the activity at and around rOpenSci!</p><h2
        id=\\\"ropensci-hq\\\">rOpenSci HQ</h2><h3 id=\\\"help-us-translate-our-dev-guide-to-portuguese\\\">Help
        us translate our dev guide to Portuguese</h3><p>Since last year we started
        the translation and localization of our Spanish version of our <a href=\\\"https://devguide.ropensci.org/\\\">comprehensive
        guide to software development</a>. We have a <a href=\\\"https://devdevguide.netlify.app/es/preface.es.html\\\">first
        version in Spanish</a> and now, thanks to the R Community initiative, we are
        working on the Portuguese version.</p><p>Our process includes the review of
        automatic translations by two people, and we need your help!</p><p>If you
        are interested in collaborating with this community effort, please check the
        &ldquo;TODO - Second Review&rdquo; tab of <a href=\\\"https://github.com/orgs/ropensci/projects/7\\\">this
        project on GitHub</a> and let us know you want to collaborate by leaving a
        comment in the corresponding Pull Request.</p><p>All contributors will be
        added to the rOpenSci Slack, <a href=\\\"/multilingual-publishing/\\\">listed
        on our multilingual publishing project webpage</a> and acknowledged as authors
        of the Portuguese translation.</p><h3 id=\\\"coworking\\\">Coworking</h3><p>Read
        <a href=\\\"/blog/2023/06/21/coworking/\\\">all about coworking</a> in our
        recent <a href=\\\"/blog/2023/06/21/coworking/\\\">post</a>!</p><p>Join us
        for social coworking &amp; office hours monthly on first Tuesdays!Hosted by
        Steffi LaZerte and various community hosts.Everyone welcome.No RSVP needed.Consult
        our <a href=\\\"/events\\\">Events</a> page to find your local time and how
        to join.</p><ul><li><a href=\\\"/events/coworking-2023-11/\\\">Tuesday, Nov
        7th, 9:00 America Pacific (17:00 UTC)</a>, Fixing scary bugs \U0001F631\U0001F41B
        With community host <a href=\\\"/author/salix-dubois/\\\">Salix Dubois</a>
        and <a href=\\\"/author/steffi-lazerte/\\\">Steffi LaZerte</a>.<ul><li>Identify/fix
        some scary bugs;</li><li>Chat with Salix about scary bugs!</li></ul></li></ul><p>And
        remember, you can always cowork independently on work related to R, work on
        packages that tend to be neglected, or work on what ever you need to get done!</p><h3
        id=\\\"reminder-three-upcoming-community-calls-and-our-first-one-in-spanish\\\">Reminder:
        Three upcoming community calls, and our first one in Spanish!</h3><h4 id=\\\"r-in-governmentcommcallsoct2023-government\\\"><a
        href=\\\"/commcalls/oct2023-government/\\\">R in Government</a></h4><p>In
        this community call, our panelists will share their experiences and examples
        of projects with R at different levels of government and in different countries.We
        invite you to learn about the challenges and lessons learned from our panelists
        and attendees in their efforts to make their government data, processes, and
        analyses more open and reproducible.</p><p>With <a href=\\\"https://ropensci.org/author/lu%C3%ADza-andrade/\\\">Lu\xEDza
        Andrade</a>, <a href=\\\"https://ropensci.org/author/karly-harker/\\\">Karly
        Harker</a>, <a href=\\\"https://ropensci.org/author/ahmadou-dicko/\\\">Ahmadou
        Dicko</a>, <a href=\\\"https://ropensci.org/author/pablo-tiscornia/\\\">Pablo
        Tiscornia</a>.</p><p><strong>Tuesday, 31 October 2023 16:00 UTC</strong></p><h4
        id=\\\"multilingual-publishing\\\">Multilingual Publishing</h4><p>As global
        movements, Open Source and Open Science face language-based exclusion as most
        resources are in English. This affects scientists and research software engineers
        working in R, particularly those who don\u2019t have English as their first
        language.</p><p>rOpenSci multilingual efforts aim to lower access barriers,
        democratize quality resources, and increase the possibilities of contributing
        to open software and science. We successfully piloted our Spanish-language
        peer review and the localization to Spanish of our comprehensive guide to
        software development, with Portuguese translation underway.</p><p><a href=\\\"/author/ma%C3%ABlle-salmon/\\\">Ma\xEBlle
        Salmon</a>, <a href=\\\"/author/paola-corrales/\\\">Paola Corrales</a>, and
        <a href=\\\"/author/elio-campitelli/\\\">Elio Campitelli</a>,  will share
        the rOpenSci Multilingual project details on this call. Ma\xEBlle will present
        the R packages that allow us to have our content in several languages. Then
        Elio and Paola will share the translation workflow and show the Translation
        Guide written to document the process.</p><ul><li><p><a href=\\\"/commcalls/nov2023-multilingual/\\\"><strong>Multilingual
        Publishing</strong>:  Tuesday, 21 November 2023 12:00 UTC</a>.</p></li><li><p><a
        href=\\\"/commcalls/nov2023-multilenguaje/\\\"><strong>Proyecto Multiling\xFCe</strong>:
        Thursday, 23 November 2023 12:00 UTC</a>.</p></li></ul><h2 id=\\\"software--new-versions\\\">Software
        \U0001F4E6: new versions</h2><p>The following twenty-three packages have had
        an update since the last newsletter: <a href=\\\"https://docs.ropensci.org/gert\\\"
        title=\\\"Simple Git Client for R\\\">gert</a> (<a href=\\\"https://github.com/r-lib/gert/releases/tag/v2.0.0\\\"><code>v2.0.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/aorsf\\\" title=\\\"Accelerated Oblique
        Random Survival Forests\\\">aorsf</a> (<a href=\\\"https://github.com/ropensci/aorsf/releases/tag/v0.1.1\\\"><code>v0.1.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/beastier\\\" title=\\\"Call BEAST2\\\">beastier</a>
        (<a href=\\\"https://github.com/ropensci/beastier/releases/tag/v2.4.12\\\"><code>v2.4.12</code></a>),
        <a href=\\\"https://docs.ropensci.org/beautier\\\" title=\\\"BEAUti from R\\\">beautier</a>
        (<a href=\\\"https://github.com/ropensci/beautier/releases/tag/v2.6.9\\\"><code>v2.6.9</code></a>),
        <a href=\\\"https://docs.ropensci.org/biomartr\\\" title=\\\"Genomic Data
        Retrieval\\\">biomartr</a> (<a href=\\\"https://github.com/ropensci/biomartr/releases/tag/v1.0.5\\\"><code>v1.0.5</code></a>),
        <a href=\\\"https://docs.ropensci.org/drake\\\" title=\\\"A Pipeline Toolkit
        for Reproducible Computation at Scale\\\">drake</a> (<a href=\\\"https://github.com/ropensci/drake/releases/tag/7.13.6\\\"><code>7.13.6</code></a>),
        <a href=\\\"https://docs.ropensci.org/EDIutils\\\" title=\\\"An API Client
        for the Environmental Data Initiative Repository\\\">EDIutils</a> (<a href=\\\"https://github.com/ropensci/EDIutils/releases/tag/v1.0.3\\\"><code>v1.0.3</code></a>),
        <a href=\\\"https://docs.ropensci.org/epubr\\\" title=\\\"Read EPUB File Metadata
        and Text\\\">epubr</a> (<a href=\\\"https://github.com/ropensci/epubr/releases/tag/v0.6.4\\\"><code>v0.6.4</code></a>),
        <a href=\\\"https://docs.ropensci.org/FedData\\\" title=\\\"Functions to Automate
        Downloading Geospatial Data Available fromSeveral Federated Data Sources\\\">FedData</a>
        (<a href=\\\"https://github.com/ropensci/FedData/releases/tag/v4.0.0\\\"><code>v4.0.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/hunspell\\\" title=\\\"High-Performance
        Stemmer, Tokenizer, and Spell Checker\\\">hunspell</a> (<a href=\\\"https://github.com/ropensci/hunspell/releases/tag/v3.0.3\\\"><code>v3.0.3</code></a>),
        <a href=\\\"https://docs.ropensci.org/MODIStsp\\\" title=\\\"Find, Download
        and Process MODIS Land ProductsData\\\">MODIStsp</a> (<a href=\\\"https://github.com/ropensci/MODIStsp/releases/tag/v2.1.0\\\"><code>v2.1.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/nodbi\\\" title=\\\"NoSQL Database Connector\\\">nodbi</a>
        (<a href=\\\"https://github.com/ropensci/nodbi/releases/tag/v0.9.8\\\"><code>v0.9.8</code></a>),
        <a href=\\\"https://docs.ropensci.org/prism\\\" title=\\\"Access Data from
        the Oregon State Prism Climate Project\\\">prism</a> (<a href=\\\"https://github.com/ropensci/prism/releases/tag/v0.2.1\\\"><code>v0.2.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/rdhs\\\" title=\\\"API Client and Dataset
        Management for the Demographic and Health Survey (DHS) Data\\\">rdhs</a> (<a
        href=\\\"https://github.com/ropensci/rdhs/releases/tag/v0.8.0\\\"><code>v0.8.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/rglobi\\\" title=\\\"Interface to Global
        Biotic Interactions\\\">rglobi</a> (<a href=\\\"https://github.com/ropensci/rglobi/releases/tag/v0.3.4\\\"><code>v0.3.4</code></a>),
        <a href=\\\"https://docs.ropensci.org/rtweet\\\" title=\\\"Collecting Twitter
        Data\\\">rtweet</a> (<a href=\\\"https://github.com/ropensci/rtweet/releases/tag/v1.2.1\\\"><code>v1.2.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/stats19\\\" title=\\\"Work with Open
        Road Traffic Casualty Data from Great Britain\\\">stats19</a> (<a href=\\\"https://github.com/ropensci/stats19/releases/tag/v3.0.0\\\"><code>v3.0.0</code></a>),
        <a href=\\\"https://docs.ropensci.org/tarchetypes\\\" title=\\\"Archetypes
        for Targets\\\">tarchetypes</a> (<a href=\\\"https://github.com/ropensci/tarchetypes/releases/tag/0.7.9\\\"><code>0.7.9</code></a>),
        <a href=\\\"https://docs.ropensci.org/targets\\\" title=\\\"Dynamic Function-Oriented
        Make-Like Declarative Pipelines\\\">targets</a> (<a href=\\\"https://github.com/ropensci/targets/releases/tag/1.3.2\\\"><code>1.3.2</code></a>),
        <a href=\\\"https://docs.ropensci.org/terrainr\\\" title=\\\"Landscape Visualizations
        in R and Unity\\\">terrainr</a> (<a href=\\\"https://github.com/ropensci/terrainr/releases/tag/v0.7.5\\\"><code>v0.7.5</code></a>),
        <a href=\\\"https://docs.ropensci.org/tiler\\\" title=\\\"Create Geographic
        and Non-Geographic Map Tiles\\\">tiler</a> (<a href=\\\"https://github.com/ropensci/tiler/releases/tag/v0.3.1\\\"><code>v0.3.1</code></a>),
        <a href=\\\"https://docs.ropensci.org/tracerer\\\" title=\\\"Tracer from R\\\">tracerer</a>
        (<a href=\\\"https://github.com/ropensci/tracerer/releases/tag/v2.2.3\\\"><code>v2.2.3</code></a>),
        and <a href=\\\"https://docs.ropensci.org/waywiser\\\" title=\\\"Ergonomic
        Methods for Assessing Spatial Models\\\">waywiser</a> (<a href=\\\"https://github.com/ropensci/waywiser/releases/tag/v0.5.0\\\"><code>v0.5.0</code></a>).</p><h2
        id=\\\"software-peer-review\\\">Software Peer Review</h2><p>There are eighteen
        recently closed and active submissions and 3 submissions on hold. Issues are
        at different stages:</p><ul><li><p>Four at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A4/review(s)-in-awaiting-changes\\\">&lsquo;4/review(s)-in-awaiting-changes&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/606\\\">fastMatMR</a>,
        &ldquo;fastMatMR: High-Performance Matrix Market File Operations in R&rdquo;.
        Submitted by <a href=\\\"https://rgoswami.me\\\">Rohit Goswami</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/600\\\">naijR</a>,
        Operations to Ease Data Analyses Specific to Nigeria. Submitted by <a href=\\\"https://victorordu.wordpress.com\\\">Victor
        Ordu </a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/522\\\">wmm</a>,
        World Magnetic Model. Submitted by <a href=\\\"https://github.com/wfrierson\\\">Will
        Frierson</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/502\\\">octolog</a>,
        Better Github Action Logging. Submitted by <a href=\\\"https://github.com/assignUser\\\">Jacob
        Wujciak-Jens</a>.</p></li></ul></li><li><p>Seven at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A3/reviewer(s)-assigned\\\">&lsquo;3/reviewer(s)-assigned&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/603\\\">GLMMcosinor</a>,
        Fit a cosinor model using a generalised mixed modelling framework. Submitted
        by <a href=\\\"https://rwparsons.github.io/\\\">Rex Parsons</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/595\\\">rangr</a>,
        Mechanistic Simulation of Species Range Dynamics. Submitted by <a href=\\\"https://github.com/katarzynam-165\\\">Katarzyna
        Markowska</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/590\\\">mregions2</a>,
        Access Data from Marineregions.org: The Marine Regions Gazetteer and the Marine
        Regions Data Products. Submitted by <a href=\\\"https://salvafern.github.io/\\\">salvafern</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/575\\\">pangoling</a>,
        Access to Large Language Model Predictions. Submitted by <a href=\\\"https://bnicenboim.github.io/\\\">Bruno
        Nicenboim</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/556\\\">dfms</a>,
        Dynamic Factor Models. Submitted by <a href=\\\"https://github.com/SebKrantz\\\">Sebastian
        Krantz</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/546\\\">fwildclusterboot</a>,
        Fast Wild Cluster Bootstrap Inference for Linear Models. Submitted by <a href=\\\"https://s3alfisc.github.io/blog/\\\">Alexander
        Fischer</a>.  (Stats).</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/603\\\">GLMMcosinor</a>,
        Fit a cosinor model using a generalised mixed modelling framework. Submitted
        by <a href=\\\"https://rwparsons.github.io/\\\">Rex Parsons</a>.</p></li></ul></li><li><p>Four
        at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A2/seeking-reviewer(s)\\\">&lsquo;2/seeking-reviewer(s)&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/613\\\">comtradr</a>,
        Interface with the United Nations Comtrade API. Submitted by <a href=\\\"https://github.com/datapumpernickel\\\">paulbochtler</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/609\\\">baRulho</a>,
        Quantifying (Animal) Sound Degradation. Submitted by <a href=\\\"https://marceloarayasalas.weebly.com/\\\">Marcelo
        Araya-Salas</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/598\\\">weatherOz</a>,
        An API Client for Australian Weather and Climate Data Resources. Submitted
        by <a href=\\\"https://github.com/bozaah\\\">Rodrigo Pires</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/489\\\">bssm</a>,
        Bayesian Inference of Non-Linear and Non-Gaussian State Space. Submitted by
        <a href=\\\"https://jounihelske.netlify.app\\\">Jouni Helske</a>.  (Stats).</p></li></ul></li><li><p>Three
        at <a href=\\\"https://github.com/ropensci/software-review/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A1/editor-checks\\\">&lsquo;1/editor-checks&rsquo;</a>:</p><ul><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/608\\\">rgeeExtra</a>,
        Extensions for rgee. Submitted by <a href=\\\"http://csaybar.github.io\\\">Cesar
        Aybar</a>.</p></li><li><p><a href=\\\"https://github.com/ropensci/software-review/issues/599\\\">agromet</a>,
        \xCDndices y Estad\xEDsticos Clim\xE1ticos e Hidrol\xF3gicos. Submitted by
        <a href=\\\"http://paocorrales.github.io\\\">Paola Corrales</a>.</p></li><li><p><a
        href=\\\"https://github.com/ropensci/software-review/issues/572\\\">qualtdict</a>,
        Generating Variable Dictionaries and Labelled Data Exports of Qualtrics. Submitted
        by <a href=\\\"https://github.com/lyh970817\\\">lyh970817</a>.</p></li></ul></li></ul><p>Find
        out more about <a href=\\\"/software-review\\\">Software Peer Review</a> and
        how to get involved.</p><h2 id=\\\"on-the-blog\\\">On the blog</h2><!-- Do
        not forget to rebase your branch! --><ul><li><a href=\\\"/blog/2023/09/26/how-to-translate-a-hugo-blog-post-with-babeldown\\\">How
        to Translate a Hugo Blog Post with Babeldown</a> by Ma\xEBlle Salmon, and
        Yanina Bellini Saibene. Other languages: <a href='/es/blog/2023/09/26/c\xF3mo_traducir_una_entrada_de_blog_de_hugo_con_babeldown'
        lang='es'>C\xF3mo traducir un art\xEDculo de blog de Hugo con Babeldown (es)</a>,
        <a href='/fr/blog/2023/09/26/comment_traduire_un_billet_de_blog_hugo_avec_babeldown'
        lang='fr'>Comment traduire un billet de blog Hugo avec Babeldown (fr)</a>.</li></ul><h2
        id=\\\"call-for-maintainers\\\">Call for maintainers</h2><p>If you&rsquo;re
        interested in maintaining any of the R packages below, you might enjoy reading
        our blog post <a href=\\\"/blog/2023/02/07/what-does-it-mean-to-maintain-a-package/\\\">What
        Does It Mean to Maintain a Package?</a> (or listening to its discussion on
        the <a href=\\\"https://rweekly.fireside.fm/111\\\">R Weekly highlights podcast</a>
        hosted by Eric Nantz and Mike Thomas)!</p><ul><li><strong><a href=\\\"https://cran.r-project.org/web/packages/rvertnet/index.html\\\">rvertnet</a></strong>,
        Retrieve, map and summarize data from the VertNet.org archives (<a href=\\\"https://vertnet.org/\\\">https://vertnet.org/</a>).
        Functions allow searching by many parameters, including taxonomic names, places,
        and dates. In addition, there is an interface for conducting spatially delimited
        searches, and another for requesting large datasets via email. <a href=\\\"https://github.com/ropensci-archive/rvertnet/issues/71\\\">Issue
        for volunteering</a>.</li></ul><h3 id=\\\"call-for-co-maintainers\\\">Call
        for co-maintainers</h3><p>Refer to our somewhat <a href=\\\"/blog/2022/10/17/maintain-or-co-maintain-an-ropensci-package/#packages-looking-for-co-maintainers\\\">recent
        blog post</a> to identify other packages where help is especially wished for!See
        also our <a href=\\\"/help-wanted/\\\">help wanted page</a> &ndash; before
        opening a PR, we recommend asking in the issue whether help is still needed.</p><h2
        id=\\\"package-development-corner\\\">Package development corner</h2><p>Some
        useful tips for R package developers. \U0001F440</p><h3 id=\\\"system-dependencies-in-r-packages--automatic-testing\\\">System
        Dependencies in R Packages &amp; Automatic Testing</h3><p>Are you curious
        about, or stalled by, the installation of system dependencies in your continuous
        integration workflows?Refer to <a href=\\\"https://blog.r-hub.io/2023/09/26/system-dependency/\\\">Hugo
        Gruson&rsquo;s clear and extensive post</a> on the R-hub blog.</p><h3 id=\\\"translate-your-packages-messages-with-potools\\\">Translate
        your package&rsquo;s messages with {potools}</h3><p>The <a href=\\\"https://michaelchirico.github.io/potools/\\\">potools</a>
        package by Michael Chirico is to translation files what roxygen2 is to Rd
        documentation files: it very much simplifies your writing and maintaining
        them!Refer to <a href=\\\"https://michaelchirico.github.io/potools/\\\">potools
        documentation</a> or a recent <a href=\\\"https://masalmon.eu/2023/10/06/potools-mwe/\\\">tutorial</a>.</p><h3
        id=\\\"testthat-new-release\\\">testthat new release</h3><p>If you use testthat
        for your package tests, don&rsquo;t miss the <a href=\\\"https://www.tidyverse.org/blog/2023/10/testthat-3-2-0/\\\">release
        announcement</a> of testthat 3.2.0!That post describes the major features
        such as the return of mocking support within testthat itself.</p><p>Among
        <a href=\\\"https://testthat.r-lib.org/news/index.html#minor-features-and-bug-fixes-3-2-0\\\">minor
        features</a> you might notice the new <code>desc</code> argument of <code>testthat::test_file()</code>
        to run a single test at a time.You can now for instance run <code>devtools::test_active_file(desc
        = 'blop() runs')</code></p><h2 id=\\\"last-words\\\">Last words</h2><p>Thanks
        for reading! If you want to get involved with rOpenSci, check out our <a href=\\\"https://contributing.ropensci.org\\\">Contributing
        Guide</a> that can help direct you to the right place, whether you want to
        make code contributions, non-code contributions, or contribute in other ways
        like sharing use cases.</p><p>If you haven&rsquo;t subscribed to our newsletter
        yet, you can <a href=\\\"/news/\\\">do so via a form</a>. Until it&rsquo;s
        time for our next newsletter, you can keep in touch with us via our <a href=\\\"/\\\">website</a>
        and <a href=\\\"https://hachyderm.io/@rOpenSci\\\">Mastodon account</a>.</p>\",\"doi\":\"https://doi.org/10.59350/c6935-n5w84\",\"reference\":[],\"summary\":\"Dear
        rOpenSci friends, it\u2019s time for our monthly news roundup!\",\"tags\":[\"Newsletter\"],\"title\":\"rOpenSci
        News Digest, October 2023\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Erin
        Robinson\",\"url\":\"https://orcid.org/0000-0001-9998-0114\"},{\"name\":\"Ted
        Habermann\",\"url\":\"https://orcid.org/0000-0003-3585-6733\"}],\"blog_id\":\"pm0p222\",\"blog_name\":\"Upstream\",\"blog_slug\":\"upstream\",\"content_html\":\"<p>Project
        identifiers have been used in DataCite for nearly a decade. While this adoption
        has been inconsistent, the consistent adoption of Project IDs could aid with
        the tracking of provenance of data and other project assets, ensuring transparency
        and compliance with data management practices. Furthermore, a widespread adoption
        of Project IDs could play a role in assessing the impact of projects, which
        is vital for funding decisions and measuring the success and outcomes of research
        endeavors. A community of practice around project IDs could support data sharing,
        collaboration, and interdisciplinary research, enabling the integration of
        data and resources from various fields to address complex problems effectively.</p><p><strong>So,
        if project identifiers are a valuable tool, how can we build a consistent
        community of practice?</strong> \_To explore this question, we researched
        the application of Project IDs within the DataCite corpus. With this understanding
        of how Project IDs are used, we piloted creating Project IDs and metadata
        for the Tetiaroa Ecostation and the Gump South Pacific Research Station, two
        small scientific facilities in French Polynesia. </p><p>It is only through
        this understanding that our communities can start to build better and more
        robust usage of these invaluable identifiers. Through this research, we show
        the power of using existing infrastructure to connect projects and their downstream
        outputs.</p><h3 id=\\\"project-metadata-existing-practice-within-the-datacite-community\\\">Project
        Metadata: existing practice within the DataCite community</h3><p>The use of
        DataCite metadata schema to capture and connect project metadata is well established.
        To analyze the current practice, we reviewed how users define project information
        within the resourceType element, typically with resourceTypeGeneral = \u201COther\u201D.
        This approach allows identification of new types in a way that allows discovery
        and easy migration if the new type becomes part of the shared vocabulary on
        the road to broader adoption. </p><p>Finding DataCite metadata that includes
        \u201CProject\u201D in the resourceType is straightforward using the DataCite
        API query: <a href=\\\"https://api.datacite.org/dois?query=types.resourceType:*Project*&amp;page%5bsize%5d=1\\\">https://api.datacite.org/dois?query=types.resourceType:*Project*&amp;page[size]=1</a>
        and the facets for this query provide an overview of repositories that are
        using this type:</p><!--kg-card-begin: html--><table style=\\\"border: none;
        border-collapse: collapse;\\\"><colgroup><col width=\\\"174\\\"><col width=\\\"375\\\"><col
        width=\\\"75\\\"></colgroup><tbody><tr style=\\\"height: 15.75pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 700; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Client</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 700; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Name</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 700; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Count</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cos.osf</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Open Science Framework</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">73,828</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cern.zenodo</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Zenodo</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">28,687</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">tdl.tacc</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Texas Advanced
        Computing Center</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0,
        0); vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap:
        break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align:
        right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">621</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">gdcc.odum-library</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">UNC
        Libraries</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align: right; margin-top:
        0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt; font-family:
        Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight:
        400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">212</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">umich.library</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">University of Michigan
        Library</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align: right; margin-top:
        0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt; font-family:
        Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight:
        400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">193</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">fatj.ngeahg</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">WL - Publications</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">167</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cdl.cdl</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">California Digital
        Library</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align: right; margin-top:
        0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt; font-family:
        Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight:
        400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">93</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">unlv.ds</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">DigitalScholarship@UNLV</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">68</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">tib.hawk</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">HAWK Hildesheim
        - Hornemann Institut</span></p></td><td style=\\\"border: 1pt solid rgb(0,
        0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap:
        break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align:
        right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">58</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">tib.eurescom</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Eurescom GmbH</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">48</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">&nbsp;</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Total</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">103,975</span></p></td></tr></tbody></table><!--kg-card-end:
        html--><p><em>Table 1. \_These ten repositories have 99.75% of DataCite records
        with \\\"Project\\\" in the resourceType element. Data collected during May,
        2023.</em></p><h3 id=\\\"resourcetypes\\\">ResourceTypes</h3><p>The DataCite
        resourceType is an optional free-text field that provides more information
        about the type of a resource than the mandatory resourceTypeGeneral element.
        This dataset includes records that have \u201CProject\u201D in their resourceTypes.
        Repositories implement this free-text differently, some with simply \u201CProject\u201D
        and some with more details about the type of project or the resource:</p><!--kg-card-begin:
        html--><table style=\\\"border: none; border-collapse: collapse;\\\"><colgroup><col
        width=\\\"161\\\"><col width=\\\"461\\\"></colgroup><tbody><tr style=\\\"height:
        15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 700; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">Client</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 12pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 700; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">resourceType</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cos.osf</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 12pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Project</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cern.zenodo</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 12pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Project Deliverable,
        Project milestone</span></p></td></tr><tr style=\\\"height: 85.5pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">tdl.tacc</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project/Other,
        Project/Report, Project/Other/REU,&nbsp;</span></p><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project/Other/Dataset,
        Project/Experimental,&nbsp;</span></p><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project/Other/Check
        Sheet, Project/Other/Database, Project/Other/Poster, Project/Other/Report,&nbsp;</span></p><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">Project/Other/None, Project/Other/Code,&nbsp;</span></p><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">Project/Other/Other, Project/Simulation</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">gdcc.odum-library</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Capstone
        Project, Project</span></p></td></tr><tr style=\\\"height: 15.75pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">umich.library</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project,
        Master's Project</span></p></td></tr><tr style=\\\"height: 15.75pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">fatj.ngeahg</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Projectrapport,
        Project report</span></p></td></tr></tbody></table><!--kg-card-end: html--><p><em>Table
        2. Some repositories use simply \\\"Project\\\", others provide more detail.</em></p><p>Note
        that the largest user of DataCite projects (cos.osf) has records that clearly
        describe projects, i.e. resourceType = \u201CProject\u201D. Other repositories
        include text that describes various parts of projects.</p><h3 id=\\\"projects-as-hubs-for-resources-people-and-organizations\\\">Projects
        as Hubs for Resources, People, and Organizations</h3><p>Projects are composed
        of the resources used for planning, executing, and reporting on the work done
        during the project and the people and organizations that fund and participate
        in the project. Project metadata can be a hub for connecting all of these.</p><p>Project
        metadata serves as a hub for connecting project resources using related resources
        and relation types. <a href=\\\"https://metadatagamechangers.com/blog/2023/5/2/project-metadata-in-datacite\\\">Detailed
        analysis</a> of the DataCite project metadata shows that only two of the largest
        project repositories in DataCite include related resources and, in those cases,
        these connections are only available for a small portion of the projects.
        A few projects from Zenodo that stand out as connectors of many resources
        are shown in Figure 1.<br></p><figure class=\\\"kg-card kg-image-card\\\"><img
        src=\\\"https://lh7-us.googleusercontent.com/b9KnxbbXUaXZTsJ7YcBMmdXvAqgURlFNnexBX3cxGvSlkCylgOwuf0sVMeJUswMkMuSSsRhFpzDhudpAvDgMaGyckvALy40oguMs4LlwUphSnKJCniC1gfEmOKm6gf2fnUkK0p6v9uLxJQIRI3qJwK4\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"443\\\"></figure><p><em>Figure
        1. Several projects with diverse relations and some inter-connections.</em></p><p>Connecting
        people and organizations to projects is done with ORCIDs and RORs that identify
        them. The DataCite project metadata was examined for these kinds of connections
        and the results, shown in Figure 2, show how connectivity varies over the
        different repositories:</p><ul><li>gdcc.odum-library and cos.osf repositories,
        shown in the upper left of Figure 2, include Resource Author Affiliation Identifiers
        (RORs),</li><li>cos.osf and cern.zenodo repositories, in the center of Figure
        2, include funder and award identifiers,</li><li>cern.zenodo repository includes
        Resource Contact identifier metadata.</li><li>cern.zenodo and tcl.tacc include
        related identifiers with many relation types. </li></ul><p>Identifiers included
        in more than two repositories are not shown here. That includes resource Resource
        Author Identifiers (ORCIDs), included in four repositories, and Resource Author
        Affiliation strings without identifiers, included in five repositories.</p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/4iuaC75GCk0Et8nRW2gEWWF01L_Ni70WfC6oWUMyMUu-wNBxN5bduHxViWt5PG9b_m19jQp3SUVqP5BMYbMYeSnvCHjXcPrZJN7l4WEeolzhennSEMWTjx_yWhtYQWfWLJD9MA1hFCyFuCo7jO7m_uc\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"348\\\"></figure><p><em>Figure
        2. Identifiers found in project metadata records.</em> </p><h3 id=\\\"the-fair-island-case-study-leveraging-existing-infrastructure\\\">The
        FAIR Island Case Study: Leveraging Existing Infrastructure</h3><p>Biological
        field stations, marine labs, and other scientific facilities, are an important
        part of the global scientific research infrastructure, providing access and
        logistical support that makes scientific contributions across domains from
        ecology to archeology possible. Currently, researchers submit applications
        to do work at these field stations. These applications provide metadata, i.e.
        who, what, when, and where, about proposed projects, but they are generally
        not visible to other researchers wanting to do work in the same place. Could
        they form the basis for open project metadata?</p><p>The connections discussed
        above are great, but, if you can\u2019t see them, it can be hard to realize
        the benefits and promote adoption.The<a href=\\\"http://fairisland.org/\\\">
        FAIR Island Project</a> prototyped an <a href=\\\"https://metadatagamechangers.com/blog/2023/4/30/fair-island-experiments-with-connecting-project-resources-in-datacite\\\">approach</a>
        to work with researchers as they are planning their projects to create project
        metadata records and to mint DataCite DOIs when the projects are approved.
        Using this approach, we can leverage DataCite infrastructure and capabilities
        to 1) connect resources, people, and organizations to projects with identifiers,
        2) update the metadata records as additional relationships are created (e.g.
        protocols, datasets, papers), and 3) use DataCite Commons to visualize the
        growing list of resources related to the field station. \_</p><p>Figure 3
        shows one of these projects in the <a href=\\\"https://commons.datacite.org/doi.org/10.17913/f37p4h\\\">DataCite
        Commons</a>. The citation to the project and a description are shown on the
        Description tab while other tabs list Creators and Contributors to the project.</p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/mfzAYQsFc6K-ilF8f1ekxGeOb1vZyGKH6xeObhhumQ8VBwEN3X2HnMIDc__a9Bu5o9ihfae_67bFlxXgqksMHXKSK1NOS0aVpSEQN9oADMkRqNoJeoIWOfrNM7EXKoG6LiyPOPyNBTXumrHvBXd8z1U\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"532\\\" height=\\\"330\\\"></figure><p><em>Figure
        3. DataCite Commons page for a project with tabs showing descriptive metadata,
        creators, and contributors. The complete metadata are also available in several
        representations using the Download Metadata button.</em></p><p>Connecting
        these projects back to the field stations where they took place is another
        important goal of this work. Those connections were made by adding the field
        station as a contributor to the project along with its ROR. All of the projects
        for the Tetiaroa Ecostation are listed on the <a href=\\\"https://commons.datacite.org/ror.org/04p8xrf95?resource-type=other\\\">organization
        page</a> in the DataCite Commons (Figure 4.) This page includes a clickable
        list of creators and contributors to the projects, a time history of works
        from this organization, graphics summarizing work types and licenses (which
        still need to be added to the metadata), and links to reports with identifiers
        and more metadata on related works and funders on the far left.<br></p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/4KaYahxLaJCJmrYPCQUCX0BvWrjG5UJIrsakHf7kXv4NKx0QSKzm0TcJlHIklW_PbLxPDtSroNb8JpCHwjo7I2nhwgC3qLJ1Yrnk63rllmr6AQyuEaXDQskKcxcSFD6FfV9QouR2J-OGqC6bbhnU7B4\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"420\\\"></figure><p><em>Figure
        4. DataCite Commons page for an organization with a ROR that lists all of
        the works connected to the organization. In this case resources with the resourceType
        = other are lists as these are the projects included in this experiment.</em></p><h2
        id=\\\"conclusion-and-next-steps\\\">Conclusion and Next Steps</h2><p>Persistent
        identifiers (PIDs) are crucial for scientific projects in many fields and
        contexts. They serve to ensure the long-term accessibility, traceability,
        and discoverability of projects and facilitate the linkage and integration
        of many kinds of resources that make up research projects. </p><p>The DataCite
        community is already using project metadata for a variety of use cases. Samples
        of DataCite metadata from ten repositories currently creating project metadata
        were selected to determine how over 100,000 projects are currently described.
        All of these repositories and their users can improve utilization of existing
        capabilities to increase connectivity of their projects. The FAIR Island Project
        is working to provide working examples that demonstrate how even more of the
        DataCite infrastructure in the metadata and in the DataCite Commons can be
        leveraged.</p><p>Figure 5 illustrates the resources that make up the research
        landscape and the relationships between them. Most of the resource types in
        this landscape are currently supported by the DataCite metadata schema and
        the growing infrastructure and capabilities built on top of it. Other elements
        of the landscape are also supported by open, reliable and available infrastructure
        and data systems, e.g. ORCID, Crossref, ROR, and protocols.io.</p><p></p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/f1JYrUNzXxlOndf0HyxrbQ5gnPAzY-I1rLmxgjHJRga4fPhsmRbcCxrpgif61itPapTirNssRXLgFFk7zlc_FPoQB1vJc5CMPs65pkT8j20naqF0Myjven_0u4gBvkG_5PXxXx6dxVdEDNBvRApin4g\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"482\\\"></figure><p><em>Figure
        5. Schematic illustration of the existing global research infrastructure built
        on identifiers for a variety of resources and the partners that work together
        to add capabilities on top of it. </em></p><p>Discussions of the PID Graph,
        and the connected global research infrastructure that it represents have been
        going on for several years. This work demonstrates that the connections it
        facilitates, often hidden in metadata records, are emerging and becoming visible
        in functional capabilities. No new infrastructure was required to create these
        project IDs or the connections. We relied entirely on existing DataCite, Crossref,
        ROR, and ORCID infrastructure to identify these resources and connect them.
        <strong>This is the power of the global research infrastructure and it is
        exciting to see the connections being formed and displayed.</strong></p><p>Now
        is the time for the research and repository communities to focus on creating,
        curating, and re-curating high-quality, well-connected metadata for the variety
        of resource types shown in Figure 5 and using the existing infrastructure
        to demonstrate the return on investment of that metadata. They can work together
        to evolve the DataCite schema with improvements for their resource types and
        use cases. In addition, DataCite could lead discussions to build community
        consensus and guidelines for Project IDs and <a href=\\\"https://metadatagamechangers.com/blog/2021/12/2/dois-for-output-management-plans\\\">other
        new resource types</a>, increasing the usability and impact of these metadata.</p><p>____</p><p><em>Much
        of the analysis in this post was drawn from : </em><a href=\\\"https://metadatagamechangers.com/blog/2023/5/2/project-metadata-in-datacite\\\"><em>https://metadatagamechangers.com/blog/2023/5/2/project-metadata-in-datacite</em></a></p><p><em>The
        FAIR Island Project Metadata experiment is more fully described in this blog
        post: </em><a href=\\\"https://metadatagamechangers.com/blog/2023/4/30/fair-island-experiments-with-connecting-project-resources-in-datacite\\\"><em>https://metadatagamechangers.com/blog/2023/4/30/fair-island-experiments-with-connecting-project-resources-in-datacite</em></a><em>
        and in a talk given at the Earth Science Information Partners\u2019 July 2023
        Meeting: Recording starts at 50:00 </em><a href=\\\"https://youtu.be/nB6nrnrIcF0?feature=shared&amp;t=3008\\\"><em>https://youtu.be/nB6nrnrIcF0?feature=shared&amp;t=3008</em></a></p>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.54900/g4928-wva21\",\"id\":\"24790\",\"image\":\"https://images.unsplash.com/photo-1572177812156-58036aae439c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDR8fHByb2plY3R8ZW58MHx8fHwxNjk3MzQwODQ2fDA&ixlib=rb-4.0.3&q=80&w=2000\",\"language\":\"en\",\"published_at\":1697531478,\"reference\":[],\"relationships\":[],\"summary\":\"Project
        identifiers have been used in DataCite for nearly a decade.\",\"tags\":[\"Original
        Research\"],\"title\":\"Building a Community of Practice: Observations of
        the Current Use of DataCite DOIs as Project IDs\",\"updated_at\":1697753088,\"url\":\"https://upstream.force11.org/building-a-community-of-practice\",\"uuid\":\"3f6ae6d1-d797-485d-8605-a28e95797d89\"},\"highlight\":{\"authors\":[{\"name\":\"Erin
        Robinson\",\"url\":\"https://orcid.org/0000-0001-9998-0114\"},{\"name\":\"Ted
        Habermann\",\"url\":\"https://orcid.org/0000-0003-3585-6733\"}],\"content_html\":\"<p>Project
        identifiers have been used in DataCite for nearly a decade. While this adoption
        has been inconsistent, the consistent adoption of Project IDs could aid with
        the tracking of provenance of data and other project assets, ensuring transparency
        and compliance with data management practices. Furthermore, a widespread adoption
        of Project IDs could play a role in assessing the impact of projects, which
        is vital for funding decisions and measuring the success and outcomes of research
        endeavors. A community of practice around project IDs could support data sharing,
        collaboration, and interdisciplinary research, enabling the integration of
        data and resources from various fields to address complex problems effectively.</p><p><strong>So,
        if project identifiers are a valuable tool, how can we build a consistent
        community of practice?</strong> \_To explore this question, we researched
        the application of Project IDs within the DataCite corpus. With this understanding
        of how Project IDs are used, we piloted creating Project IDs and metadata
        for the Tetiaroa Ecostation and the Gump South Pacific Research Station, two
        small scientific facilities in French Polynesia. </p><p>It is only through
        this understanding that our communities can start to build better and more
        robust usage of these invaluable identifiers. Through this research, we show
        the power of using existing infrastructure to connect projects and their downstream
        outputs.</p><h3 id=\\\"project-metadata-existing-practice-within-the-datacite-community\\\">Project
        Metadata: existing practice within the DataCite community</h3><p>The use of
        DataCite metadata schema to capture and connect project metadata is well established.
        To analyze the current practice, we reviewed how users define project information
        within the resourceType element, typically with resourceTypeGeneral = \u201COther\u201D.
        This approach allows identification of new types in a way that allows discovery
        and easy migration if the new type becomes part of the shared vocabulary on
        the road to broader adoption. </p><p>Finding DataCite metadata that includes
        \u201CProject\u201D in the resourceType is straightforward using the DataCite
        API query: <a href=\\\"https://api.datacite.org/dois?query=types.resourceType:*Project*&amp;page%5bsize%5d=1\\\">https://api.datacite.org/dois?query=types.resourceType:*Project*&amp;page[size]=1</a>
        and the facets for this query provide an overview of repositories that are
        using this type:</p><!--kg-card-begin: html--><table style=\\\"border: none;
        border-collapse: collapse;\\\"><colgroup><col width=\\\"174\\\"><col width=\\\"375\\\"><col
        width=\\\"75\\\"></colgroup><tbody><tr style=\\\"height: 15.75pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 700; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Client</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 700; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Name</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 700; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Count</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cos.osf</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Open Science Framework</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">73,828</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cern.zenodo</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Zenodo</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">28,687</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">tdl.tacc</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Texas Advanced
        Computing Center</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0,
        0); vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap:
        break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align:
        right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">621</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">gdcc.odum-library</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">UNC
        Libraries</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align: right; margin-top:
        0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt; font-family:
        Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight:
        400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">212</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">umich.library</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">University of Michigan
        Library</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align: right; margin-top:
        0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt; font-family:
        Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight:
        400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">193</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">fatj.ngeahg</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">WL - Publications</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">167</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cdl.cdl</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">California Digital
        Library</span></p></td><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align: right; margin-top:
        0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt; font-family:
        Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight:
        400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">93</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">unlv.ds</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">DigitalScholarship@UNLV</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">68</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">tib.hawk</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">HAWK Hildesheim
        - Hornemann Institut</span></p></td><td style=\\\"border: 1pt solid rgb(0,
        0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap:
        break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38; text-align:
        right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">58</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">tib.eurescom</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 11pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Eurescom GmbH</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">48</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 11pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">&nbsp;</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Total</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; text-align: right; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        11pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">103,975</span></p></td></tr></tbody></table><!--kg-card-end:
        html--><p><em>Table 1. \_These ten repositories have 99.75% of DataCite records
        with \\\"Project\\\" in the resourceType element. Data collected during May,
        2023.</em></p><h3 id=\\\"resourcetypes\\\">ResourceTypes</h3><p>The DataCite
        resourceType is an optional free-text field that provides more information
        about the type of a resource than the mandatory resourceTypeGeneral element.
        This dataset includes records that have \u201CProject\u201D in their resourceTypes.
        Repositories implement this free-text differently, some with simply \u201CProject\u201D
        and some with more details about the type of project or the resource:</p><!--kg-card-begin:
        html--><table style=\\\"border: none; border-collapse: collapse;\\\"><colgroup><col
        width=\\\"161\\\"><col width=\\\"461\\\"></colgroup><tbody><tr style=\\\"height:
        15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align:
        top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 700; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">Client</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 12pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 700; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">resourceType</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cos.osf</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 12pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Project</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">cern.zenodo</span></p></td><td style=\\\"border:
        1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt 5pt; overflow: hidden;
        overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height: 1.38;
        margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size: 12pt;
        font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color: transparent;
        font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps:
        normal; font-variant-east-asian: normal; font-variant-position: normal; text-decoration:
        none; vertical-align: baseline; white-space: pre-wrap;\\\">Project Deliverable,
        Project milestone</span></p></td></tr><tr style=\\\"height: 85.5pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">tdl.tacc</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project/Other,
        Project/Report, Project/Other/REU,&nbsp;</span></p><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project/Other/Dataset,
        Project/Experimental,&nbsp;</span></p><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project/Other/Check
        Sheet, Project/Other/Database, Project/Other/Poster, Project/Other/Report,&nbsp;</span></p><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">Project/Other/None, Project/Other/Code,&nbsp;</span></p><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">Project/Other/Other, Project/Simulation</span></p></td></tr><tr
        style=\\\"height: 15.75pt;\\\"><td style=\\\"border: 1pt solid rgb(0, 0, 0);
        vertical-align: top; padding: 0pt 5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p
        dir=\\\"ltr\\\" style=\\\"line-height: 1.38; margin-top: 0pt; margin-bottom:
        0pt;\\\"><span style=\\\"font-size: 12pt; font-family: Arial, sans-serif;
        color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style:
        normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-east-asian:
        normal; font-variant-position: normal; text-decoration: none; vertical-align:
        baseline; white-space: pre-wrap;\\\">gdcc.odum-library</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Capstone
        Project, Project</span></p></td></tr><tr style=\\\"height: 15.75pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">umich.library</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Project,
        Master's Project</span></p></td></tr><tr style=\\\"height: 15.75pt;\\\"><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">fatj.ngeahg</span></p></td><td
        style=\\\"border: 1pt solid rgb(0, 0, 0); vertical-align: top; padding: 0pt
        5pt; overflow: hidden; overflow-wrap: break-word;\\\"><p dir=\\\"ltr\\\" style=\\\"line-height:
        1.38; margin-top: 0pt; margin-bottom: 0pt;\\\"><span style=\\\"font-size:
        12pt; font-family: Arial, sans-serif; color: rgb(0, 0, 0); background-color:
        transparent; font-weight: 400; font-style: normal; font-variant-ligatures:
        normal; font-variant-caps: normal; font-variant-east-asian: normal; font-variant-position:
        normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\\\">Projectrapport,
        Project report</span></p></td></tr></tbody></table><!--kg-card-end: html--><p><em>Table
        2. Some repositories use simply \\\"Project\\\", others provide more detail.</em></p><p>Note
        that the largest user of DataCite projects (cos.osf) has records that clearly
        describe projects, i.e. resourceType = \u201CProject\u201D. Other repositories
        include text that describes various parts of projects.</p><h3 id=\\\"projects-as-hubs-for-resources-people-and-organizations\\\">Projects
        as Hubs for Resources, People, and Organizations</h3><p>Projects are composed
        of the resources used for planning, executing, and reporting on the work done
        during the project and the people and organizations that fund and participate
        in the project. Project metadata can be a hub for connecting all of these.</p><p>Project
        metadata serves as a hub for connecting project resources using related resources
        and relation types. <a href=\\\"https://metadatagamechangers.com/blog/2023/5/2/project-metadata-in-datacite\\\">Detailed
        analysis</a> of the DataCite project metadata shows that only two of the largest
        project repositories in DataCite include related resources and, in those cases,
        these connections are only available for a small portion of the projects.
        A few projects from Zenodo that stand out as connectors of many resources
        are shown in Figure 1.<br></p><figure class=\\\"kg-card kg-image-card\\\"><img
        src=\\\"https://lh7-us.googleusercontent.com/b9KnxbbXUaXZTsJ7YcBMmdXvAqgURlFNnexBX3cxGvSlkCylgOwuf0sVMeJUswMkMuSSsRhFpzDhudpAvDgMaGyckvALy40oguMs4LlwUphSnKJCniC1gfEmOKm6gf2fnUkK0p6v9uLxJQIRI3qJwK4\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"443\\\"></figure><p><em>Figure
        1. Several projects with diverse relations and some inter-connections.</em></p><p>Connecting
        people and organizations to projects is done with ORCIDs and RORs that identify
        them. The DataCite project metadata was examined for these kinds of connections
        and the results, shown in Figure 2, show how connectivity varies over the
        different repositories:</p><ul><li>gdcc.odum-library and cos.osf repositories,
        shown in the upper left of Figure 2, include Resource Author Affiliation Identifiers
        (RORs),</li><li>cos.osf and cern.zenodo repositories, in the center of Figure
        2, include funder and award identifiers,</li><li>cern.zenodo repository includes
        Resource Contact identifier metadata.</li><li>cern.zenodo and tcl.tacc include
        related identifiers with many relation types. </li></ul><p>Identifiers included
        in more than two repositories are not shown here. That includes resource Resource
        Author Identifiers (ORCIDs), included in four repositories, and Resource Author
        Affiliation strings without identifiers, included in five repositories.</p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/4iuaC75GCk0Et8nRW2gEWWF01L_Ni70WfC6oWUMyMUu-wNBxN5bduHxViWt5PG9b_m19jQp3SUVqP5BMYbMYeSnvCHjXcPrZJN7l4WEeolzhennSEMWTjx_yWhtYQWfWLJD9MA1hFCyFuCo7jO7m_uc\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"348\\\"></figure><p><em>Figure
        2. Identifiers found in project metadata records.</em> </p><h3 id=\\\"the-fair-island-case-study-leveraging-existing-infrastructure\\\">The
        FAIR Island Case Study: Leveraging Existing Infrastructure</h3><p>Biological
        field stations, marine labs, and other scientific facilities, are an important
        part of the global scientific research infrastructure, providing access and
        logistical support that makes scientific contributions across domains from
        ecology to archeology possible. Currently, researchers submit applications
        to do work at these field stations. These applications provide metadata, i.e.
        who, what, when, and where, about proposed projects, but they are generally
        not visible to other researchers wanting to do work in the same place. Could
        they form the basis for open project metadata?</p><p>The connections discussed
        above are great, but, if you can\u2019t see them, it can be hard to realize
        the benefits and promote adoption.The<a href=\\\"http://fairisland.org/\\\">
        FAIR Island Project</a> prototyped an <a href=\\\"https://metadatagamechangers.com/blog/2023/4/30/fair-island-experiments-with-connecting-project-resources-in-datacite\\\">approach</a>
        to work with researchers as they are planning their projects to create project
        metadata records and to mint DataCite DOIs when the projects are approved.
        Using this approach, we can leverage DataCite infrastructure and capabilities
        to 1) connect resources, people, and organizations to projects with identifiers,
        2) update the metadata records as additional relationships are created (e.g.
        protocols, datasets, papers), and 3) use DataCite Commons to visualize the
        growing list of resources related to the field station. \_</p><p>Figure 3
        shows one of these projects in the <a href=\\\"https://commons.datacite.org/doi.org/10.17913/f37p4h\\\">DataCite
        Commons</a>. The citation to the project and a description are shown on the
        Description tab while other tabs list Creators and Contributors to the project.</p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/mfzAYQsFc6K-ilF8f1ekxGeOb1vZyGKH6xeObhhumQ8VBwEN3X2HnMIDc__a9Bu5o9ihfae_67bFlxXgqksMHXKSK1NOS0aVpSEQN9oADMkRqNoJeoIWOfrNM7EXKoG6LiyPOPyNBTXumrHvBXd8z1U\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"532\\\" height=\\\"330\\\"></figure><p><em>Figure
        3. DataCite Commons page for a project with tabs showing descriptive metadata,
        creators, and contributors. The complete metadata are also available in several
        representations using the Download Metadata button.</em></p><p>Connecting
        these projects back to the field stations where they took place is another
        important goal of this work. Those connections were made by adding the field
        station as a contributor to the project along with its ROR. All of the projects
        for the Tetiaroa Ecostation are listed on the <a href=\\\"https://commons.datacite.org/ror.org/04p8xrf95?resource-type=other\\\">organization
        page</a> in the DataCite Commons (Figure 4.) This page includes a clickable
        list of creators and contributors to the projects, a time history of works
        from this organization, graphics summarizing work types and licenses (which
        still need to be added to the metadata), and links to reports with identifiers
        and more metadata on related works and funders on the far left.<br></p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/4KaYahxLaJCJmrYPCQUCX0BvWrjG5UJIrsakHf7kXv4NKx0QSKzm0TcJlHIklW_PbLxPDtSroNb8JpCHwjo7I2nhwgC3qLJ1Yrnk63rllmr6AQyuEaXDQskKcxcSFD6FfV9QouR2J-OGqC6bbhnU7B4\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"420\\\"></figure><p><em>Figure
        4. DataCite Commons page for an organization with a ROR that lists all of
        the works connected to the organization. In this case resources with the resourceType
        = other are lists as these are the projects included in this experiment.</em></p><h2
        id=\\\"conclusion-and-next-steps\\\">Conclusion and Next Steps</h2><p>Persistent
        identifiers (PIDs) are crucial for scientific projects in many fields and
        contexts. They serve to ensure the long-term accessibility, traceability,
        and discoverability of projects and facilitate the linkage and integration
        of many kinds of resources that make up research projects. </p><p>The DataCite
        community is already using project metadata for a variety of use cases. Samples
        of DataCite metadata from ten repositories currently creating project metadata
        were selected to determine how over 100,000 projects are currently described.
        All of these repositories and their users can improve utilization of existing
        capabilities to increase connectivity of their projects. The FAIR Island Project
        is working to provide working examples that demonstrate how even more of the
        DataCite infrastructure in the metadata and in the DataCite Commons can be
        leveraged.</p><p>Figure 5 illustrates the resources that make up the research
        landscape and the relationships between them. Most of the resource types in
        this landscape are currently supported by the DataCite metadata schema and
        the growing infrastructure and capabilities built on top of it. Other elements
        of the landscape are also supported by open, reliable and available infrastructure
        and data systems, e.g. ORCID, Crossref, ROR, and protocols.io.</p><p></p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://lh7-us.googleusercontent.com/f1JYrUNzXxlOndf0HyxrbQ5gnPAzY-I1rLmxgjHJRga4fPhsmRbcCxrpgif61itPapTirNssRXLgFFk7zlc_FPoQB1vJc5CMPs65pkT8j20naqF0Myjven_0u4gBvkG_5PXxXx6dxVdEDNBvRApin4g\\\"
        class=\\\"kg-image\\\" alt loading=\\\"lazy\\\" width=\\\"624\\\" height=\\\"482\\\"></figure><p><em>Figure
        5. Schematic illustration of the existing global research infrastructure built
        on identifiers for a variety of resources and the partners that work together
        to add capabilities on top of it. </em></p><p>Discussions of the PID Graph,
        and the connected global research infrastructure that it represents have been
        going on for several years. This work demonstrates that the connections it
        facilitates, often hidden in metadata records, are emerging and becoming visible
        in functional capabilities. No new infrastructure was required to create these
        project IDs or the connections. We relied entirely on existing DataCite, Crossref,
        ROR, and ORCID infrastructure to identify these resources and connect them.
        <strong>This is the power of the global research infrastructure and it is
        exciting to see the connections being formed and displayed.</strong></p><p>Now
        is the time for the research and repository communities to focus on creating,
        curating, and re-curating high-quality, well-connected metadata for the variety
        of resource types shown in Figure 5 and using the existing infrastructure
        to demonstrate the return on investment of that metadata. They can work together
        to evolve the DataCite schema with improvements for their resource types and
        use cases. In addition, DataCite could lead discussions to build community
        consensus and guidelines for Project IDs and <a href=\\\"https://metadatagamechangers.com/blog/2021/12/2/dois-for-output-management-plans\\\">other
        new resource types</a>, increasing the usability and impact of these metadata.</p><p>____</p><p><em>Much
        of the analysis in this post was drawn from : </em><a href=\\\"https://metadatagamechangers.com/blog/2023/5/2/project-metadata-in-datacite\\\"><em>https://metadatagamechangers.com/blog/2023/5/2/project-metadata-in-datacite</em></a></p><p><em>The
        FAIR Island Project Metadata experiment is more fully described in this blog
        post: </em><a href=\\\"https://metadatagamechangers.com/blog/2023/4/30/fair-island-experiments-with-connecting-project-resources-in-datacite\\\"><em>https://metadatagamechangers.com/blog/2023/4/30/fair-island-experiments-with-connecting-project-resources-in-datacite</em></a><em>
        and in a talk given at the Earth Science Information Partners\u2019 July 2023
        Meeting: Recording starts at 50:00 </em><a href=\\\"https://youtu.be/nB6nrnrIcF0?feature=shared&amp;t=3008\\\"><em>https://youtu.be/nB6nrnrIcF0?feature=shared&amp;t=3008</em></a></p>\",\"doi\":\"https://doi.org/10.54900/g4928-wva21\",\"reference\":[],\"summary\":\"Project
        identifiers have been used in DataCite for nearly a decade.\",\"tags\":[\"Original
        Research\"],\"title\":\"Building a Community of Practice: Observations of
        the Current Use of DataCite DOIs as Project IDs\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Sascha
        Sch\xF6nig\",\"url\":\"\"}],\"blog_id\":\"526jy42\",\"blog_name\":\"Elephant
        in the Lab\",\"blog_slug\":\"elephantinthelab\",\"content_html\":\"\\n<p><em>Science
        communication is often considered equal with public relations or media coverage.
        However, the phenomenon is significantly more complex, and its most important
        aspects are not given enough attention. For instance, science includes how
        science can not only communicate but also interact with societal groups, and
        the potential impacts this can have on the perception of research in the public
        eye. Dr. Volker Meyer-Guckel, Chair of the Donors\u2019 Association, explained
        in an interview with \u201CElephant in the Lab\u201D why our understanding
        is outdated and how effective science communication can be achieved.</em></p>\\n\\n\\n\\n<p><strong>What
        is the goal of science communication for you? What can science communication
        achieve?</strong></p>\\n\\n\\n\\n<p>The goals of science communication should
        evolve in alignment with how science, communication, and society are changing;
        it cannot be reduced to a single factor. Currently, about 90 percent of science
        communication is rather irrelevant to society and primarily serves as self-promotion
        for research institutions or is a form of political lobbying on its own behalf.
        It is primarily concerned with presenting research institutions and unilaterally
        mediatizing research results. This form is insufficient, yet it consumes many
        resources. Science communication becomes intriguing when science interacts
        with society. Peter Weingart, a renowned researcher in this field, once wrote,
        \u2018The alleged gap between science and society is a political construct.\u2019
        I would like to question this thesis, suggesting that there is no gap.</p>\\n\\n\\n\\n<p><strong>To
        what extent?</strong></p>\\n\\n\\n\\n<p>First and foremost, it can be noted
        that many of the current societal debates about how we should live, how we
        should conduct our economy, how we should communicate, and what we should
        fear are significantly influenced by science. There are indeed various positions
        and debates within and outside of science, to say the least. However, there
        are also other reference points, communication logics, and value systems.
        Addressing this would be a major task for science communication. But there\u2019s
        more to it: when life sciences deeply interfere with the genetic codes of
        living organisms, when researcher-created artificial intelligence fundamentally
        changes the way we communicate, exchange knowledge, and produce insights,
        or when epidemiologists, armed with both secure knowledge and uncertain findings,
        make decisions that touch upon societal principles of freedom, one cannot,
        in my opinion, retreat and say that the societal relevance of the state of
        scientific debates is solely the realm of politics. We are part of this space;
        scientific discourse spaces are increasingly intertwined with political and
        societal ones. Here, there is a need for many more bridges to be built, bridges
        that are systematically thought out and not left to the media behavior of
        individual scientists or institutions \u2013 a vast field for science communication
        that is hardly explored.</p>\\n\\n\\n\\n<p><strong>What role does the attention
        economy play in this, meaning the fact that attention is a scarce resource?
        Which research findings actually gain access to public and political debates?</strong></p>\\n\\n\\n\\n<p>In
        a so-called \u201Cattention economy\u201D, research generally has little success
        chances because in order to grasp attention traditional triggers are required
        such as sensation, morality, and outrage. However, there are areas where voices
        from the scientific community are expected, especially in societal conflicts
        and transformation processes fueled by specific research findings but leading
        to ethical, economic, or social controversies. For me, this is the truly interesting
        and largely unexplored space for science communication. There is not enough
        understanding of how science operates in these spaces, how it should operate,
        and how it changes itself and society as a result. Much more attention should
        be paid to this, both within the scientific community and in the realm of
        science communication and science funding. Furthermore, there is a need for
        spaces where researchers with experience in science communication can exchange
        their insights with those actively engaging in societal debates.</p>\\n\\n\\n\\n<p><strong>Who
        is THE science that communicates?</strong></p>\\n\\n\\n\\n<p>A good question.
        There is, of course, no single science, but rather different disciplines and
        perspectives on societal challenges, phenomena, and issues. A common problem
        with scientific policy advice is that it often represents only a small spectrum
        of the available expertise. For example, during the Covid-19 pandemic, the
        Leopoldina issued a recommendation regarding school closures, which was based
        on scientific expertise but authored by a relatively small number of researchers
        and disciplines. Knowledge holders outside the academic community were not
        involved in these recommendations at all, even though it would have been crucial
        to include the expertise of as many as possible.</p>\\n\\n\\n\\n<p>When it
        comes to science communication, it\u2019s important to consider and make transparent
        the disciplinary differences. The culture of communication in the social sciences
        is undoubtedly different from that in the natural sciences, and this, in turn,
        influences how communication and collaboration with society are conducted.
        For me, a central task of science communication is to highlight and make understandable
        the cultural differences among disciplines for society.</p>\\n\\n\\n\\n<p>However,
        it must be acknowledged that the scientific discourse and publication space
        are primarily characterized by self-referentiality and (hyper) specialization.
        Practical knowledge or solutions to societal challenges are rarely generated
        or enjoy a lower status in the academic reputation system. As long as this
        is the case, we will not make significant progress in science communication.
        On the contrary, it will remain the responsibility of research institutions\u2019
        communication departments, rather than becoming an integral part of the tasks
        of researchers and disciplines.</p>\\n\\n\\n\\n<p><strong>What are the limits
        of science communication? What can and should science accomplish in communication,
        and where does competence reach its limits?</strong></p>\\n\\n\\n\\n<p>I am
        convinced that science has drawn its boundaries too narrowly thus far. My
        plea would be to explore these boundaries beyond fundamental debates and not
        only in the realm of communication but also in the concrete transformation
        spaces within society. This happens far too rarely, primarily because there
        are currently not enough resources for it. When I mention \u201Ctransformation
        spaces,\u201D I\u2019m referring, for instance, to real-world laboratories
        involving many stakeholders, where the role of science can be tested beyond
        traditional research and education, showcasing what it can achieve, where
        it faces limits, and how it can potentially evolve in such situations. With
        these experiences, we could collaboratively work on \u201Cexpectation management\u201D
        with society. The government increasingly hopes that social transformation
        processes can automatically succeed through the involvement of science \u2013
        this is, of course, a misconception. In these cases, politics and society
        sometimes place demands on science that it cannot fulfill, and it\u2019s precisely
        these boundaries that should be empirically tested, without excluding anything
        from the outset, for example, out of a pure fear that scientific integrity
        could be compromised in such processes.</p>\\n\\n\\n\\n<p>This also assumes
        that researchers better understand the communication spaces where citizens,
        politicians, or journalists operate. To borrow from Luhmann: in the media
        space, it\u2019s not primarily about truth as it is in the scientific realm,
        but mainly about newsworthiness, or attention. Similarly, in politics, it\u2019s
        not about truth but primarily about power. Reflecting on these connections,
        understanding them, and then illuminating and designing one\u2019s own role
        as a communicator and actor based on them is a process that can be learned.
        I have recently heard about initial postgraduate programs on the role of science
        in policy advice. I find this very intelligent because it allows for systematic
        reflection on these subsystems, into which one enters and which operate by
        entirely different rules than scientific discourses.</p>\\n\\n\\n\\n<p>This
        reflection also includes how science communicates in the public sphere\u2014or
        perhaps it would be better to say: in the various public spheres\u2014not
        just with clear research results and established knowledge but how it communicates
        scientific processes as a whole. This includes discussing uncertain knowledge,
        skepticism, criticism, and method diversity. It involves making clear and
        comprehensible how science operates. Here, people outside of the scientific
        community should be able to handle complexities. Thus, confidently communicating
        uncertainties and ambiguities, rather than succumbing to the fear that too
        much transparency about the limitations of knowledge might cede the playing
        field to populist simplifiers. This could happen because of the belief that
        communicating doubts could be instrumentalized to delegitimize research and
        politics. Explaining the scientific system is part of societal enlightenment
        and is at the core of science communication.</p>\\n\\n\\n\\n<p><strong>Should
        the scientists themselves handle this, or should specialized positions be
        created for it? Also, considering the limited resources of the scientists?</strong></p>\\n\\n\\n\\n<p>Communication
        offices within research institutions have proliferated in recent years, mushrooming
        like never before. At times, more communication roles have been created than
        actual research positions. However, this personnel typically focuses on the
        area of public relations, which, in my view, has been inflated and often comes
        across as uninteresting. Research institutions are thus delegating tasks of
        science to supposed specialists because they assume that researchers need
        translators to speak in the way that is commonly used in the media space.
        This approach is neither authentic nor particularly goal-oriented. We need
        active scientists for communication in societal spaces.</p>\\n\\n\\n\\n<p>Does
        every researcher need to be a communicator now? I would say no, but if a person
        works on a topic that in any way touches upon societal transformation, perhaps
        through technology application or social practices, they should have reflected
        on their role in the societal space. And if they choose to play such a role
        actively or even as an activist, they should gain clarity about the speaker
        position they assume in various contexts when entering an interaction space
        with society. Where am I still THE scientist, and where am I THE citizen?
        Do I have a different speaker position than the person next door when I carry
        a \u201Cfollow the science\u201D sign at a demonstration? How do I interact
        with knowledge holders outside the scientific community? How does this change
        my own work as a scientist? And so forth.</p>\\n\\n\\n\\n<p><strong>So, science
        should communicate but not be too political?</strong></p>\\n\\n\\n\\n<p>To
        quote Paul Watzlawik, there is no non-communication. Even non-communication
        is a form of communication. In reality, science is constantly communicating
        in the political space, and acknowledging this is extremely important. This
        involves a changing space of reflection between science and society, which
        science must better research and explore, especially with a focus on the bidirectionality
        often invoked in different memoranda. In articles addressing science communication,
        this aspect is often overlooked. Most publications on science communication
        barely go beyond reflecting on the communication of research results. This
        is too limited, and I\u2019ll emphasize it again: in our transformational
        society, we are in a situation where science plays a very different role in
        societal processes than before, and the expectations of science are changing
        simultaneously. It is seen as a problem solver and a driver of transformation,
        not just as a producer of knowledge. Science communication needs to adapt
        to this shift.</p>\\n\\n\\n\\n<p><strong>Does this form of interactive science
        communication pose the risk of diluting the role of scientists and, as a result,
        diminishing trust in science?</strong></p>\\n\\n\\n\\n<p>I am convinced that
        scientific presence and integrity in societal transformation and communication
        spaces strengthen trust in science. There are very few relevant topics that
        are not influenced or shaped by science. This is precisely why it is important
        to clarify who plays what roles in such situations that require societal understanding,
        political decisions, or new practices. It is an important thought process
        that one must go through and enrich with experiences because there is currently
        a lot of confusion in the different expectations of the actors involved. In
        the end, this process sharpens the role of science rather than dilutes it.</p>\\n\\n\\n\\n<p><strong>Do
        you have an ideal or example of what effective science communication looks
        like?</strong></p>\\n\\n\\n\\n<p>Those who interact, communicate: Where does
        communication happen in your family? At the kitchen table, where they eat
        together. Communication takes place there because they are doing something
        collectively. I see a similar perspective when it comes to science. When you
        start to interact more with societal actor groups, communication happens automatically,
        and there\u2019s no need for artificially created communication spaces or
        specialists that require a lot of money and resources.</p>\\n\\n\\n\\n<p>In
        Germany, institutions like the Weizenbaum Institute or the Humboldt Institute
        for Internet and Society are setting good examples because they are intertwining
        science communication with transfer-oriented and transdisciplinary scientific
        concepts that develop, in part, from interactions with society. Others can
        learn from this: it should change the general approach and understanding of
        science communication. Effective science communication takes place in societal
        spaces with other knowledge holders and actor groups, where social, technical,
        or economic processes of change are reflected upon and addressed. We are generally
        far from such a practice in Germany. Other countries, such as those in Scandinavia,
        are more advanced in this regard.</p>\\n\\n\\n\\n<p>An interview by Teresa
        V\xF6lker.</p>\\n\",\"content_text\":\"content_text\",\"id\":\"67842572-f3ff-48ce-b4ce-93e44d53f878\",\"image\":\"https://elephantinthelab.org/wp-content/uploads/2022/05/tree-736885.jpg\",\"language\":\"en\",\"published_at\":1697522100,\"reference\":[],\"relationships\":[],\"summary\":\"<em>Science
        communication is often considered equal with public relations or media coverage.
        However, the phenomenon is significantly more complex, and its most important
        aspects are not given enough attention. For instance, science includes how
        science can not only communicate but also interact with societal groups, and
        the potential impacts this can have on the perception of research in the public
        eye.\",\"tags\":[\"Infrastructure\",\"Open Science\",\"Interview\"],\"title\":\"Science
        and Society need more interaction instead of mere communication. An Interview
        with Volker Meyer-Guckel\",\"updated_at\":1697533800,\"url\":\"https://elephantinthelab.org/science-and-society-need-more-interaction-instead-of-mere-communication-an-interview-with-volker-meyer-guckel\"},\"highlight\":{\"authors\":[{\"name\":\"Sascha
        Sch\xF6nig\",\"url\":\"\"}],\"content_html\":\"\\n<p><em>Science communication
        is often considered equal with public relations or media coverage. However,
        the phenomenon is significantly more complex, and its most important aspects
        are not given enough attention. For instance, science includes how science
        can not only communicate but also interact with societal groups, and the potential
        impacts this can have on the perception of research in the public eye. Dr.
        Volker Meyer-Guckel, Chair of the Donors\u2019 Association, explained in an
        interview with \u201CElephant in the Lab\u201D why our understanding is outdated
        and how effective science communication can be achieved.</em></p>\\n\\n\\n\\n<p><strong>What
        is the goal of science communication for you? What can science communication
        achieve?</strong></p>\\n\\n\\n\\n<p>The goals of science communication should
        evolve in alignment with how science, communication, and society are changing;
        it cannot be reduced to a single factor. Currently, about 90 percent of science
        communication is rather irrelevant to society and primarily serves as self-promotion
        for research institutions or is a form of political lobbying on its own behalf.
        It is primarily concerned with presenting research institutions and unilaterally
        mediatizing research results. This form is insufficient, yet it consumes many
        resources. Science communication becomes intriguing when science interacts
        with society. Peter Weingart, a renowned researcher in this field, once wrote,
        \u2018The alleged gap between science and society is a political construct.\u2019
        I would like to question this thesis, suggesting that there is no gap.</p>\\n\\n\\n\\n<p><strong>To
        what extent?</strong></p>\\n\\n\\n\\n<p>First and foremost, it can be noted
        that many of the current societal debates about how we should live, how we
        should conduct our economy, how we should communicate, and what we should
        fear are significantly influenced by science. There are indeed various positions
        and debates within and outside of science, to say the least. However, there
        are also other reference points, communication logics, and value systems.
        Addressing this would be a major task for science communication. But there\u2019s
        more to it: when life sciences deeply interfere with the genetic codes of
        living organisms, when researcher-created artificial intelligence fundamentally
        changes the way we communicate, exchange knowledge, and produce insights,
        or when epidemiologists, armed with both secure knowledge and uncertain findings,
        make decisions that touch upon societal principles of freedom, one cannot,
        in my opinion, retreat and say that the societal relevance of the state of
        scientific debates is solely the realm of politics. We are part of this space;
        scientific discourse spaces are increasingly intertwined with political and
        societal ones. Here, there is a need for many more bridges to be built, bridges
        that are systematically thought out and not left to the media behavior of
        individual scientists or institutions \u2013 a vast field for science communication
        that is hardly explored.</p>\\n\\n\\n\\n<p><strong>What role does the attention
        economy play in this, meaning the fact that attention is a scarce resource?
        Which research findings actually gain access to public and political debates?</strong></p>\\n\\n\\n\\n<p>In
        a so-called \u201Cattention economy\u201D, research generally has little success
        chances because in order to grasp attention traditional triggers are required
        such as sensation, morality, and outrage. However, there are areas where voices
        from the scientific community are expected, especially in societal conflicts
        and transformation processes fueled by specific research findings but leading
        to ethical, economic, or social controversies. For me, this is the truly interesting
        and largely unexplored space for science communication. There is not enough
        understanding of how science operates in these spaces, how it should operate,
        and how it changes itself and society as a result. Much more attention should
        be paid to this, both within the scientific community and in the realm of
        science communication and science funding. Furthermore, there is a need for
        spaces where researchers with experience in science communication can exchange
        their insights with those actively engaging in societal debates.</p>\\n\\n\\n\\n<p><strong>Who
        is THE science that communicates?</strong></p>\\n\\n\\n\\n<p>A good question.
        There is, of course, no single science, but rather different disciplines and
        perspectives on societal challenges, phenomena, and issues. A common problem
        with scientific policy advice is that it often represents only a small spectrum
        of the available expertise. For example, during the Covid-19 pandemic, the
        Leopoldina issued a recommendation regarding school closures, which was based
        on scientific expertise but authored by a relatively small number of researchers
        and disciplines. Knowledge holders outside the academic community were not
        involved in these recommendations at all, even though it would have been crucial
        to include the expertise of as many as possible.</p>\\n\\n\\n\\n<p>When it
        comes to science communication, it\u2019s important to consider and make transparent
        the disciplinary differences. The culture of communication in the social sciences
        is undoubtedly different from that in the natural sciences, and this, in turn,
        influences how communication and collaboration with society are conducted.
        For me, a central task of science communication is to highlight and make understandable
        the cultural differences among disciplines for society.</p>\\n\\n\\n\\n<p>However,
        it must be acknowledged that the scientific discourse and publication space
        are primarily characterized by self-referentiality and (hyper) specialization.
        Practical knowledge or solutions to societal challenges are rarely generated
        or enjoy a lower status in the academic reputation system. As long as this
        is the case, we will not make significant progress in science communication.
        On the contrary, it will remain the responsibility of research institutions\u2019
        communication departments, rather than becoming an integral part of the tasks
        of researchers and disciplines.</p>\\n\\n\\n\\n<p><strong>What are the limits
        of science communication? What can and should science accomplish in communication,
        and where does competence reach its limits?</strong></p>\\n\\n\\n\\n<p>I am
        convinced that science has drawn its boundaries too narrowly thus far. My
        plea would be to explore these boundaries beyond fundamental debates and not
        only in the realm of communication but also in the concrete transformation
        spaces within society. This happens far too rarely, primarily because there
        are currently not enough resources for it. When I mention \u201Ctransformation
        spaces,\u201D I\u2019m referring, for instance, to real-world laboratories
        involving many stakeholders, where the role of science can be tested beyond
        traditional research and education, showcasing what it can achieve, where
        it faces limits, and how it can potentially evolve in such situations. With
        these experiences, we could collaboratively work on \u201Cexpectation management\u201D
        with society. The government increasingly hopes that social transformation
        processes can automatically succeed through the involvement of science \u2013
        this is, of course, a misconception. In these cases, politics and society
        sometimes place demands on science that it cannot fulfill, and it\u2019s precisely
        these boundaries that should be empirically tested, without excluding anything
        from the outset, for example, out of a pure fear that scientific integrity
        could be compromised in such processes.</p>\\n\\n\\n\\n<p>This also assumes
        that researchers better understand the communication spaces where citizens,
        politicians, or journalists operate. To borrow from Luhmann: in the media
        space, it\u2019s not primarily about truth as it is in the scientific realm,
        but mainly about newsworthiness, or attention. Similarly, in politics, it\u2019s
        not about truth but primarily about power. Reflecting on these connections,
        understanding them, and then illuminating and designing one\u2019s own role
        as a communicator and actor based on them is a process that can be learned.
        I have recently heard about initial postgraduate programs on the role of science
        in policy advice. I find this very intelligent because it allows for systematic
        reflection on these subsystems, into which one enters and which operate by
        entirely different rules than scientific discourses.</p>\\n\\n\\n\\n<p>This
        reflection also includes how science communicates in the public sphere\u2014or
        perhaps it would be better to say: in the various public spheres\u2014not
        just with clear research results and established knowledge but how it communicates
        scientific processes as a whole. This includes discussing uncertain knowledge,
        skepticism, criticism, and method diversity. It involves making clear and
        comprehensible how science operates. Here, people outside of the scientific
        community should be able to handle complexities. Thus, confidently communicating
        uncertainties and ambiguities, rather than succumbing to the fear that too
        much transparency about the limitations of knowledge might cede the playing
        field to populist simplifiers. This could happen because of the belief that
        communicating doubts could be instrumentalized to delegitimize research and
        politics. Explaining the scientific system is part of societal enlightenment
        and is at the core of science communication.</p>\\n\\n\\n\\n<p><strong>Should
        the scientists themselves handle this, or should specialized positions be
        created for it? Also, considering the limited resources of the scientists?</strong></p>\\n\\n\\n\\n<p>Communication
        offices within research institutions have proliferated in recent years, mushrooming
        like never before. At times, more communication roles have been created than
        actual research positions. However, this personnel typically focuses on the
        area of public relations, which, in my view, has been inflated and often comes
        across as uninteresting. Research institutions are thus delegating tasks of
        science to supposed specialists because they assume that researchers need
        translators to speak in the way that is commonly used in the media space.
        This approach is neither authentic nor particularly goal-oriented. We need
        active scientists for communication in societal spaces.</p>\\n\\n\\n\\n<p>Does
        every researcher need to be a communicator now? I would say no, but if a person
        works on a topic that in any way touches upon societal transformation, perhaps
        through technology application or social practices, they should have reflected
        on their role in the societal space. And if they choose to play such a role
        actively or even as an activist, they should gain clarity about the speaker
        position they assume in various contexts when entering an interaction space
        with society. Where am I still THE scientist, and where am I THE citizen?
        Do I have a different speaker position than the person next door when I carry
        a \u201Cfollow the science\u201D sign at a demonstration? How do I interact
        with knowledge holders outside the scientific community? How does this change
        my own work as a scientist? And so forth.</p>\\n\\n\\n\\n<p><strong>So, science
        should communicate but not be too political?</strong></p>\\n\\n\\n\\n<p>To
        quote Paul Watzlawik, there is no non-communication. Even non-communication
        is a form of communication. In reality, science is constantly communicating
        in the political space, and acknowledging this is extremely important. This
        involves a changing space of reflection between science and society, which
        science must better research and explore, especially with a focus on the bidirectionality
        often invoked in different memoranda. In articles addressing science communication,
        this aspect is often overlooked. Most publications on science communication
        barely go beyond reflecting on the communication of research results. This
        is too limited, and I\u2019ll emphasize it again: in our transformational
        society, we are in a situation where science plays a very different role in
        societal processes than before, and the expectations of science are changing
        simultaneously. It is seen as a problem solver and a driver of transformation,
        not just as a producer of knowledge. Science communication needs to adapt
        to this shift.</p>\\n\\n\\n\\n<p><strong>Does this form of interactive science
        communication pose the risk of diluting the role of scientists and, as a result,
        diminishing trust in science?</strong></p>\\n\\n\\n\\n<p>I am convinced that
        scientific presence and integrity in societal transformation and communication
        spaces strengthen trust in science. There are very few relevant topics that
        are not influenced or shaped by science. This is precisely why it is important
        to clarify who plays what roles in such situations that require societal understanding,
        political decisions, or new practices. It is an important thought process
        that one must go through and enrich with experiences because there is currently
        a lot of confusion in the different expectations of the actors involved. In
        the end, this process sharpens the role of science rather than dilutes it.</p>\\n\\n\\n\\n<p><strong>Do
        you have an ideal or example of what effective science communication looks
        like?</strong></p>\\n\\n\\n\\n<p>Those who interact, communicate: Where does
        communication happen in your family? At the kitchen table, where they eat
        together. Communication takes place there because they are doing something
        collectively. I see a similar perspective when it comes to science. When you
        start to interact more with societal actor groups, communication happens automatically,
        and there\u2019s no need for artificially created communication spaces or
        specialists that require a lot of money and resources.</p>\\n\\n\\n\\n<p>In
        Germany, institutions like the Weizenbaum Institute or the Humboldt Institute
        for Internet and Society are setting good examples because they are intertwining
        science communication with transfer-oriented and transdisciplinary scientific
        concepts that develop, in part, from interactions with society. Others can
        learn from this: it should change the general approach and understanding of
        science communication. Effective science communication takes place in societal
        spaces with other knowledge holders and actor groups, where social, technical,
        or economic processes of change are reflected upon and addressed. We are generally
        far from such a practice in Germany. Other countries, such as those in Scandinavia,
        are more advanced in this regard.</p>\\n\\n\\n\\n<p>An interview by Teresa
        V\xF6lker.</p>\\n\",\"reference\":[],\"summary\":\"<em>Science communication
        is often considered equal with public relations or media coverage. However,
        the phenomenon is significantly more complex, and its most important aspects
        are not given enough attention. For instance, science includes how science
        can not only communicate but also interact with societal groups, and the potential
        impacts this can have on the perception of research in the public eye.\",\"tags\":[\"Infrastructure\",\"Open
        Science\",\"Interview\"],\"title\":\"Science and Society need more interaction
        instead of mere communication. An Interview with Volker Meyer-Guckel\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[],\"blog_id\":\"med7n78\",\"blog_name\":\"Musings
        about Librarianship - Aaron Tay\",\"blog_slug\":\"musings\",\"content_html\":\"<img
        src=\\\"https://api.follow.it/track-rss-story-loaded/v1/yJlu_thecP3spGBqFX-6U6CY3L745TL_\\\"
        border=0 width=\\\"1\\\" height=\\\"1\\\" alt=\\\"ChatGPT Plus - new DALL-E
        3 (image creation) &  Vision (image recognition) capability - A quick overview
        & why I am disappointed.\\\" title=\\\"ChatGPT Plus - new DALL-E 3 (image
        creation) &  Vision (image recognition) capability - A quick overview & why
        I am disappointed.\\\"> <p>\_On September 2023, <a href=\\\"https://openai.com/blog/chatgpt-can-now-see-hear-and-speak\\\">OpenAI
        announced that ChatGPT Plus would be enhanced in three ways</a></p><p><br
        /></p><p>1. It would allow you to speak directly with GPT and it would also
        be able to reply in voice</p><p>2. It would be able to create images using
        DALL-E 3, OpenAI's image generation model</p><p>3. It would be able to accept
        image inputs</p><p><br /></p><p>Since I finally gained access to these features,
        I will briefly review them with my thoughts on how impactful they might be
        for library work. To anticipate, the conclusion, these features are very powerful
        but yet I am disappointed. Because each of these abilities are unlocked separately
        and cannot be combined. In other words, this is still not what I consider
        a true multi-modal model that can accept input in multi-modalities (eg text,
        audio, images) and output formats (eg text , audio , images)</p><p><i>Note:
        There's a \\\"new\\\" - Browse with Bing feature as well. I put new in quotes
        because ChatGPT Plus came with at least two earlier versions of this plugin
        that enhanced ChatGPT with results from the web. The <a href=\\\"https://www.cmswire.com/digital-experience/openai-disables-chatgpt-bing-web-browser-plugin/\\\">second
        version also based on Bing was taken down earlier this year because people
        found a way to use it to bypass paywalls.</a></i></p><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBKD-s6-rlOaNK1bxRO7TY_YjX4G4dVT2ZyhlRHxrafksUR9uewWnqEP_L-xvyV_qgjIfN9xhH7WLcB79RasxZuVAo7dUh3ei01Buo4Rv88QL3K3TC0aXwCy8LQ8LbL6ZUH81o2HSij7MVueWDYT82M0bjmch3n0Xaqka1ohXoZ79Q-sprtpCG1BJg1pkW/s400/chatgpt-browsewithbing.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"400\\\"
        data-original-width=\\\"241\\\" height=\\\"320\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBKD-s6-rlOaNK1bxRO7TY_YjX4G4dVT2ZyhlRHxrafksUR9uewWnqEP_L-xvyV_qgjIfN9xhH7WLcB79RasxZuVAo7dUh3ei01Buo4Rv88QL3K3TC0aXwCy8LQ8LbL6ZUH81o2HSij7MVueWDYT82M0bjmch3n0Xaqka1ohXoZ79Q-sprtpCG1BJg1pkW/s320/chatgpt-browsewithbing.png\\\"
        width=\\\"193\\\" /></a></div><br /><p><i>My quick tests show it is nothing
        special, we have been playing with similar features in Bing Chat, Perplexity.ai
        (and of course my blog charts the progress of academic search engines that
        use RAG), and there's nothing that suggests Browse with Bing is notably superior.</i></p><h2
        style=\\\"text-align: left;\\\"><b>1.\_</b>\_It would allow you to speak directly
        with GPT and it would also be able to reply in voice</h2><div>I don't have
        full access to this capability, my Android ChatGPT app allows me to talk to
        it (with very high accuracy - based on Whisper?) but it does not respond back
        in voice.</div><div><br /></div><div>This capability is probably the least
        impactful in my view since we had smart assistants for a while now with pretty
        good voice recognition. That said these smart assistants were always dumb
        in the responses they gave so perhaps it will feel totally different if they
        respond intelligently with voice!</div><div><br /></div><div>For example -\_Tim
        Spalding is very impressed by the voice feature saying it gives \\\"Jarvis
        vibes\\\" and is comparing it to the first time he used a Mac, Gopher and
        World Wide Web and even ChatGPT itself!</div><blockquote class=\\\"twitter-tweet\\\"><p
        dir=\\\"ltr\\\" lang=\\\"en\\\">The new \u201CVoice Conversations\u201D on
        the ChatGPT app is\u2026 well, I think this goes up there with the first time
        I used a Mac, Gopher, the World Wide Web, and ChatGPT itself. Serious Jarvis
        vibes.</p>\u2014 Tim Spalding \U0001F1FA\U0001F1E6 (@librarythingtim) <a href=\\\"https://twitter.com/librarythingtim/status/1710810082265481320?ref_src=twsrc%5Etfw\\\">October
        8, 2023</a></blockquote><p>If he is correct, and he is a very smart dude on
        such matters, once such technologies are in smart assistant/homes, we will
        be amazed.</p><p><br /></p><h2 style=\\\"text-align: left;\\\">\_2. It would
        be able to create images using DALL-E 3, OpenAI's image generation model</h2><div>When
        OpenAI first launched DALL-E in 2021 and <a href=\\\"https://openai.com/dall-e-2\\\">DALL-E
        2</a> in 2022 people were amazed. This were two ground-breaking text to image
        generators.They were quickly followed by competitor's such as Google's\_Imagen
        (against the original DALL-E) and <a href=\\\"https://stability.ai/stable-diffusion\\\">Stable
        Diffusion</a> and <a href=\\\"https://docs.midjourney.com/docs/quick-start\\\">Midjourney</a>
        (against DALL-E 2).</div><div><br /></div><div>In particular, Stable Diffusion
        grabbed a lot of attention by being available Open Source and in terms of
        capability Stable Diffusion and Midjourney (commercial) among others seems
        to have improved rapidly to surpass DALL-E 2's capabilities.</div><div><br
        /></div><div>I have been particularly impressed by Stable Diffusion's capabilities
        including text to image, inpainting and outpainting. (<a href=\\\"https://clipdrop.co/stable-diffusion?utm_campaign=stable_diffusion_promo&utm_medium=cta_button&utm_source=stability_ai\\\">try
        free here</a>), though it might be some of it's capabilities comes from the
        fact that Stable diffusion is trained on an extreme number of images scraped
        from the web and has less guardrails to prevent 'unsafe' images from being
        generated.</div><div><br /></div><div>However, OpenAI has finally struck back
        with DALL-E 3, and it claims to nail one of the last weaknesses of text to
        image generators. Up to this point, you could describe an image and these
        tools would be pretty good at understanding what you want, but if you asked
        it to create an image of something with the words \\\"Happy Birthday\\\" at
        the bottom, most of them would fail terribly at generating the words.</div><div><br
        /></div><div>To use DALL-E 3 in ChatGPT Plus you need to select a special
        mode - DALLE-3</div><div><br /></div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGLitvHvhtaiKv9tdJ3vrLTW17mzLK2iT4JJ0TXFH1B2B-dddrSmzCWgDB31-eelBVT_yhfcsxxy3DFO76PuANPFIc7nEp0gBg8E78t7PMOLGDmfwS5G8aQK_y2QQV0zw1hmn_LaBKK3JU7QLTxYsh58LlDn5z6Qe4lM_ijoFYhsmLvgppqqaEIWGHsftP/s399/chatgpt-dalle3.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"399\\\"
        data-original-width=\\\"269\\\" height=\\\"320\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGLitvHvhtaiKv9tdJ3vrLTW17mzLK2iT4JJ0TXFH1B2B-dddrSmzCWgDB31-eelBVT_yhfcsxxy3DFO76PuANPFIc7nEp0gBg8E78t7PMOLGDmfwS5G8aQK_y2QQV0zw1hmn_LaBKK3JU7QLTxYsh58LlDn5z6Qe4lM_ijoFYhsmLvgppqqaEIWGHsftP/s320/chatgpt-dalle3.png\\\"
        width=\\\"216\\\" /></a></div><br /><div><br /></div><div><br /></div><div><br
        /></div><div>I did a couple of tests using the prompt -\_</div><div><div><br
        /></div><div>\\\"Draw a picture of a Terminator robot from the moves face
        to face with a human librarian\_ At the bottom are the words \\\"AI vs Human\\\"
        and repeated it twice. This is what ChatGPT with DALL-E plugin shows.</div><div><br
        /></div><div><br /></div><div class=\\\"separator\\\" style=\\\"clear: both;
        text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHoJU3S3syBUgJwOolqcr24Xfs4JryYHHneobaPfXBn7fCBIZ7tADQ3cEak9oCZq7B2V1-kOCqcTijSelVTbt_SPkQ6dD8pO1LE1F4R_P0BmXdir79Cuffhl0B_MYpG9uWcGxOx1EHmvUhyphenhyphenTD16neqQ_R2NjMhU62bWpmRONTYbExFkooczeTYtaTMpCCA/s653/dalle-1.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"619\\\"
        data-original-width=\\\"653\\\" height=\\\"606\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHoJU3S3syBUgJwOolqcr24Xfs4JryYHHneobaPfXBn7fCBIZ7tADQ3cEak9oCZq7B2V1-kOCqcTijSelVTbt_SPkQ6dD8pO1LE1F4R_P0BmXdir79Cuffhl0B_MYpG9uWcGxOx1EHmvUhyphenhyphenTD16neqQ_R2NjMhU62bWpmRONTYbExFkooczeTYtaTMpCCA/w640-h606/dalle-1.png\\\"
        width=\\\"640\\\" /></a></div><br /><div><br /></div><div><br /></div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhN6RsLKH9VhpxqULI9FRk83jYny2lb389jZAtnUfw8EXMUfW2P67YWhCQWvHh8UonAkDKiBGVZ72Q6dzY8L5mRO3r88ImVLW9mYpuqZUVnbAhkGa3LZ9PTed3YSwH2S_LHsJpgsoKX7xpj8hP4PCkzoneRkRo2_wnJQJk0c97AEJK6wL1mCqBRvylPO0lb/s651/dalle-2.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"612\\\"
        data-original-width=\\\"651\\\" height=\\\"602\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhN6RsLKH9VhpxqULI9FRk83jYny2lb389jZAtnUfw8EXMUfW2P67YWhCQWvHh8UonAkDKiBGVZ72Q6dzY8L5mRO3r88ImVLW9mYpuqZUVnbAhkGa3LZ9PTed3YSwH2S_LHsJpgsoKX7xpj8hP4PCkzoneRkRo2_wnJQJk0c97AEJK6wL1mCqBRvylPO0lb/w640-h602/dalle-2.png\\\"
        width=\\\"640\\\" /></a></div><br /><div><br /></div><div><br /></div><div><br
        /></div><div>The results are not perfect, in the first batch of four, two
        don't even have the words! In the second batch of four, they all have the
        words, sort of anyway. The last one for some reason misspells it as HUIMAN.
        Still the fact it gets it sometimes right is impressive since most other image
        generators will totally fail most times.</div><div><br /></div><div>Because
        DALL-E 3 is now invoked via ChatGPT (or Bing Chat), you can modify the images
        using natural language. For example, you can ask it to change a male to a
        female or replace the terminator with a demon and it understands very well.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDjgUzF8CKOzStBfGvDSAb4fqYpSL-NrgGq63HfhrgS-pwlXnFAcLhxQjv3WTJ2uTHJn-UHrR71KOlTdy3o4rctQGh3r0xo6IRiaQ-Z26DKHHFDbEhLeFosNOn6EtaOrdSu8zv5fPAwoFQoXmw0W_8R4LFvLckCZI7otEzyVS7FNeYgv1IcVZmYZwdgLpO/s587/dalle3.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"537\\\"
        data-original-width=\\\"587\\\" height=\\\"293\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDjgUzF8CKOzStBfGvDSAb4fqYpSL-NrgGq63HfhrgS-pwlXnFAcLhxQjv3WTJ2uTHJn-UHrR71KOlTdy3o4rctQGh3r0xo6IRiaQ-Z26DKHHFDbEhLeFosNOn6EtaOrdSu8zv5fPAwoFQoXmw0W_8R4LFvLckCZI7otEzyVS7FNeYgv1IcVZmYZwdgLpO/s320/dalle3.png\\\"
        width=\\\"320\\\" /></a></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8pcka2eY_tfoJDZxn5SYo8U42yaN8Dz93FCj93ThhgmcEQ9_rS3kzu-WpB-QwvfG639dldPQ7-ZGCzNdQCGYz20KCM024nvaFwzKvgGsMKIdqSqVzGaDAFgHIVzX1upBr3b57XjpHF9Pio6MeHVbJF9ms9AWWzsTjLIMCNw_oMXpw76yeuROWsICUnqWL/s583/dalle-4.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"544\\\"
        data-original-width=\\\"583\\\" height=\\\"299\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8pcka2eY_tfoJDZxn5SYo8U42yaN8Dz93FCj93ThhgmcEQ9_rS3kzu-WpB-QwvfG639dldPQ7-ZGCzNdQCGYz20KCM024nvaFwzKvgGsMKIdqSqVzGaDAFgHIVzX1upBr3b57XjpHF9Pio6MeHVbJF9ms9AWWzsTjLIMCNw_oMXpw76yeuROWsICUnqWL/s320/dalle-4.png\\\"
        width=\\\"320\\\" /></a></div><br /><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div><div><br /></div><div>To be honest
        this wasn't the feature I was very excited to get in ChatGPT plus, because<a
        href=\\\"https://twitter.com/aarontay/status/1708148983183597983\\\"> I had
        already tested the same function </a>which you can get free via <a href=\\\"https://www.bing.com/create\\\">Bing
        Image Creator which is powered by DALL-E 3</a>\_and the results were similar.\_</div><div><br
        /></div></div><div>Also, while being able to specify text to add is cool,
        it isn't particularly difficult to add text labels to a generated graphic....
        Though I suppose one possible workflow would be for the system to summarise
        a paper , the use that summarise to try to create a Scientific Poster or Visual
        abstract.\_ But so far, I am not successful.</div><div><br /></div><div>For
        fun, I tried uploading a simple visual abstract to ChatGPT and using its vision
        capability it described the visual abstract. I then fed it to DALLE-3 to create
        the visual abstract. The results were weird...</div><div><br /></div><div>For
        example, while it could describe the following simple visual abstract well.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUkeGE8-ioH06dd7dIkJALtNByXJrhHsLgSsHNOfFN0BgU2oKlQjsE-MonB1uA6UvfGIt8uorqnpEf1D3O5Gm1eTGmxK1ZDUd0RfxKHZqTyeVDaOiN7lH3Iv4xZQqEo12g9qCaXxSIyxnXxBpi2L5dY4JxZ98YFUN8JQtmfEJ4FKnEwzYhYgm0A4PxFnvq/s1433/visualabstract-describe.png\\\"
        imageanchor=\\\"1\\\" style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img
        border=\\\"0\\\" data-original-height=\\\"1046\\\" data-original-width=\\\"1433\\\"
        height=\\\"468\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUkeGE8-ioH06dd7dIkJALtNByXJrhHsLgSsHNOfFN0BgU2oKlQjsE-MonB1uA6UvfGIt8uorqnpEf1D3O5Gm1eTGmxK1ZDUd0RfxKHZqTyeVDaOiN7lH3Iv4xZQqEo12g9qCaXxSIyxnXxBpi2L5dY4JxZ98YFUN8JQtmfEJ4FKnEwzYhYgm0A4PxFnvq/w640-h468/visualabstract-describe.png\\\"
        width=\\\"640\\\" /></a></div><br /><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhyv9XNkKZv2gn3cClaAzxVaVkGLVw8ntvC2hJ5bDp_ejgJ16szCxNR6jVze4SpDOyIrwd8p0p0odXuiEgrkulPsDhTnnAJYqI6gz06EwTt_KZiGm30-kTR_Yxbd5BxyqItgJ8QVV6yp-kIZY8W_ONGj2M2w_NBDT6O31zrqn9iP1t3xbAeS6BDUs904Q8/s795/visualabstract-describe-2.png\\\"
        imageanchor=\\\"1\\\" style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img
        border=\\\"0\\\" data-original-height=\\\"629\\\" data-original-width=\\\"795\\\"
        height=\\\"506\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhyv9XNkKZv2gn3cClaAzxVaVkGLVw8ntvC2hJ5bDp_ejgJ16szCxNR6jVze4SpDOyIrwd8p0p0odXuiEgrkulPsDhTnnAJYqI6gz06EwTt_KZiGm30-kTR_Yxbd5BxyqItgJ8QVV6yp-kIZY8W_ONGj2M2w_NBDT6O31zrqn9iP1t3xbAeS6BDUs904Q8/w640-h506/visualabstract-describe-2.png\\\"
        width=\\\"640\\\" /></a></div><br /><div>Feeding the same description to create
        a visual abstract with DALL-E 3 leads to weird results.</div><div><br /></div><div
        class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHTAN0ukWap7wQrGomWAEWAJMaZB0aeiOdWmrfckEdliPSZslaVkqfniL3H7M7xSx3UIC4ENZxUB-KHBtOYWtLDDS6Dkd3zM7bN7dbJGedTYVtgHD7g6c9nChPOd9WThQzuErnE_sS0PGFhCUtORU6XZK91Hflcau2Y1Kz-TA706nmv7cFFZ3N6924nmnf/s707/visualabstract-describe-3.png\\\"
        imageanchor=\\\"1\\\" style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img
        border=\\\"0\\\" data-original-height=\\\"707\\\" data-original-width=\\\"677\\\"
        height=\\\"320\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHTAN0ukWap7wQrGomWAEWAJMaZB0aeiOdWmrfckEdliPSZslaVkqfniL3H7M7xSx3UIC4ENZxUB-KHBtOYWtLDDS6Dkd3zM7bN7dbJGedTYVtgHD7g6c9nChPOd9WThQzuErnE_sS0PGFhCUtORU6XZK91Hflcau2Y1Kz-TA706nmv7cFFZ3N6924nmnf/s320/visualabstract-describe-3.png\\\"
        width=\\\"306\\\" /></a></div><br /><div><br /></div><div><br /></div><div>Stable
        Diffusion I think is still more capable in some ways than DALL-E-3 because
        it is less filtered. For example, you can easily create photos based on celebrities
        or <a href=\\\"https://stable-diffusion-art.com/consistent-face/#Multiple_celebrity_names\\\">even
        create faces that are 20% celebrity X and 80% Celebrity Y</a>, while Dall-E-3
        will refuse to generate images of any individuals.</div><div><br /></div><div>The
        image data that Stable Diffusion trained on is very broad, it even includes
        an image of me! For example, based on the \\\"<a href=\\\"https://haveibeentrained.com/\\\">Have
        I been trained?</a>\\\" database, <a href=\\\"https://haveibeentrained.com/?search_text=%22Aaron%20Tay%22\\\">there
        is at least one photo of me included!</a></div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKEw6OrVNfnJSa6IN9sjfBtXPJigHdTSIt4ce4yDxbfeZQl6yVbReuURtVbK-o9jTo8xBtOycguqI-szcdJPxnFKsLLT49cXx4E39ed10zHaLQFCUaTiM-FgxProM7doYfYkOS8W2M06poiKMx1iza4v93vAVF4O1inoRLKPfU4ob_s53ZvpM6AoEZE-Rg/s1869/Aarontay-Have%20I%20Been%20Trained_.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"834\\\"
        data-original-width=\\\"1869\\\" height=\\\"286\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKEw6OrVNfnJSa6IN9sjfBtXPJigHdTSIt4ce4yDxbfeZQl6yVbReuURtVbK-o9jTo8xBtOycguqI-szcdJPxnFKsLLT49cXx4E39ed10zHaLQFCUaTiM-FgxProM7doYfYkOS8W2M06poiKMx1iza4v93vAVF4O1inoRLKPfU4ob_s53ZvpM6AoEZE-Rg/w640-h286/Aarontay-Have%20I%20Been%20Trained_.png\\\"
        width=\\\"640\\\" /></a></div><br /><div><br /></div><div>Fortunately, or
        unfortunately there are far more photos of other \\\"Aaron Tay\\\", so when
        such systems try to generate a prompt with input \\\"Aaron Tay\\\" it is unlikely
        to get an image close to me (though it will likely generate Chinese facial
        features). For celebrities it pretty much nails it of course since almost
        all the training images will be of their likeness (try say\_<a href=\\\"https://haveibeentrained.com/?search_text=Angelina%20jolie\\\">Angelina
        Jolie</a>)</div><div><br /></div><div>DALL-E 3 when used via ChatGPT plus
        also has a host of other restrictions, if <a href=\\\"https://twitter.com/aarontay/status/1713972855384490301\\\">the
        system prompt here is accurate</a> other restrictions include instructing
        the model to\_</div><div><br /></div><div><ul style=\\\"text-align: left;\\\"><li>not
        \\\"create images in the style of artists whose last work was created within
        the last 100 years\\\"<br /></li><li>not \\\"create any imagery that would
        be offensive.\\\"</li><li>\\\"Silently modify descriptions that include names
        or hints or references of specific people or celebritie by carefully selecting
        a few minimal modifications to substitute references to the people with generic
        descriptions that don't divulge any information about their identities, except
        for their genders and physiques\\\"<br /><br /></li></ul></div><div><br /></div><h2
        style=\\\"text-align: left;\\\">3. It would be able to accept image inputs</h2><div>This
        is probably the function that is getting the most attention right now. This
        gives ChatGPT the ability to understand images you upload.</div><div><br /></div><div><a
        href=\\\"https://openai.com/blog/chatgpt-can-now-see-hear-and-speak\\\">OpenAI
        says</a></div><div><blockquote>Image understanding is powered by multimodal
        GPT-3.5 and GPT-4. These models apply their language reasoning skills to a
        wide range of images, such as photographs, screenshots, and documents containing
        both text and images.</blockquote></div><div>People are really impressed by
        this capability and so am I. For one, it can read screenshots of English test,
        figures, tables from papers. This is clearly going to be useful for interpreting
        papers.</div><div><br /></div><div>Twitter/X is full of amazing examples,
        here I try an example by uploading a visual abstract created by my colleagues.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi20GdyIcHAtp6wK2OaFFQgv9Ur-FmDo9PeJPHohaTDd6BK59oWZmOsq97-Y1fBi7btDMLe8elsWBctcx3DBYZ7JPNuWFRQZeUeXbAHE8RPI2J7hiiPPGE14pzfRTS77ZRa6gedd8UQp197YiL4h2aHxoe4-fI3exfrQJm-rJM3yffT4sli6M6WIil8iStV/s1211/dalle-visualabstract.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"659\\\"
        data-original-width=\\\"1211\\\" height=\\\"348\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi20GdyIcHAtp6wK2OaFFQgv9Ur-FmDo9PeJPHohaTDd6BK59oWZmOsq97-Y1fBi7btDMLe8elsWBctcx3DBYZ7JPNuWFRQZeUeXbAHE8RPI2J7hiiPPGE14pzfRTS77ZRa6gedd8UQp197YiL4h2aHxoe4-fI3exfrQJm-rJM3yffT4sli6M6WIil8iStV/w640-h348/dalle-visualabstract.png\\\"
        width=\\\"640\\\" /></a></div><br /><div>This is what ChatGPT with vision
        sees.</div><div><br /></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6dC-bcU-OcXIWe2S6gdiPq5sW1OM64zcODINDTXePSuMFZNVlcN6APTUxleAHhBIfVfckesLMrY0fuERUUoRQUuS1QCwedrx9Tp8e5h0VegO_2ELA-j9IB8FS232C1lk1Ag96VWoEYbDchi1m_ZFOqRxyQUhTHIhiJbD7gvRB8kD6nL0tjed03jBZcaKK/s1010/dalle-visualabstract-2.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"1010\\\"
        data-original-width=\\\"678\\\" height=\\\"640\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6dC-bcU-OcXIWe2S6gdiPq5sW1OM64zcODINDTXePSuMFZNVlcN6APTUxleAHhBIfVfckesLMrY0fuERUUoRQUuS1QCwedrx9Tp8e5h0VegO_2ELA-j9IB8FS232C1lk1Ag96VWoEYbDchi1m_ZFOqRxyQUhTHIhiJbD7gvRB8kD6nL0tjed03jBZcaKK/w430-h640/dalle-visualabstract-2.png\\\"
        width=\\\"430\\\" /></a></div><br /><div>I tried a <a href=\\\"https://twitter.com/aarontay/status/1713865247273205969\\\">few
        more examples of visual abstracts</a> and it is not perfect but still impressive</div><div><br
        /></div>\\n<blockquote class=\\\"twitter-tweet\\\" data-conversation=\\\"none\\\"><p
        dir=\\\"ltr\\\" lang=\\\"en\\\">Okay I tried this visual abstract on GPT4
        vision. It has trouble understanding that there is an icon for RCT and Population
        based Cohorts. It thinks that double arrow icon means < (less than) , (1)
        <a href=\\\"https://t.co/K9b6n5r4M1\\\">pic.twitter.com/K9b6n5r4M1</a></p>\u2014
        Aaron Tay (@aarontay) <a href=\\\"https://twitter.com/aarontay/status/1713865247273205969?ref_src=twsrc%5Etfw\\\">October
        16, 2023</a></blockquote> \\n\\n<blockquote class=\\\"twitter-tweet\\\" data-conversation=\\\"none\\\"><p
        dir=\\\"ltr\\\" lang=\\\"en\\\">I asked GPT4 for the 95% CI for intubation,
        at first it correctly refused saying it isn't labelled. I ask it to estimate
        anyway and it basically makes things up (multiple tries) - (2) <a href=\\\"https://t.co/4ZTAvAvyFQ\\\">pic.twitter.com/4ZTAvAvyFQ</a></p>\u2014
        Aaron Tay (@aarontay) <a href=\\\"https://twitter.com/aarontay/status/1713866127108112584?ref_src=twsrc%5Etfw\\\">October
        16, 2023</a></blockquote> \\n\\n\\n\\n<div><br /></div><div><br /></div><h2
        style=\\\"text-align: left;\\\">A true multi-model Large Language Model will
        be amazing</h2><div>Despite all the amazing new capabilities, I was still
        somewhat disappointed because each of these capabilities can only be used
        seperately.</div><div><br /></div><div>By default, when you start a prompt
        in ChatGPT you must choose from the following options</div><div><br /></div><div><div
        class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/a/AVvXsEj1nLHu_ccPy2bMNLvoTmiUDKWl8ZOzaXZ61fUQmir49EePaD6h5aZ-pqyYBMYajhWxSq5-4SNpbOGaCvN_m7ZI4tM-h-ntQHt8f1T79Qa8JAAlxMKFtsS3UobZhm2RQyWNEN7J45_pEuKEcgCDX5bhrh4r1-xJal6ogHb3Cd3dHCrupwmsXd0KjQ5Ivh2Y\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img alt=\\\"\\\" data-original-height=\\\"320\\\"
        data-original-width=\\\"193\\\" height=\\\"240\\\" src=\\\"https://blogger.googleusercontent.com/img/a/AVvXsEj1nLHu_ccPy2bMNLvoTmiUDKWl8ZOzaXZ61fUQmir49EePaD6h5aZ-pqyYBMYajhWxSq5-4SNpbOGaCvN_m7ZI4tM-h-ntQHt8f1T79Qa8JAAlxMKFtsS3UobZhm2RQyWNEN7J45_pEuKEcgCDX5bhrh4r1-xJal6ogHb3Cd3dHCrupwmsXd0KjQ5Ivh2Y\\\"
        width=\\\"145\\\" /></a></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div>Each of these modes are mutually
        exclusive. There's isn't a mode you can choose for uploading images, but it
        only appears when you choose \\\"Default\\\"</div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVeaMcOkd9lejIMMJUFT6xJS36PYymfQr0-cuwtPb1onnd4OuUTMHiaQUmA13hCIV7vImlEMAcoJZe4pvf4hyphenhyphenKOBQQU537Fu8v4tCdqd_cnTS5J27jmMAD91z5i5j-9OQH441WuRN0h6NxZHjjlQXOcgnLWaD7dQDgXv11NzxpuzAOi9qd_R2KmdIFEXlU/s575/chatgpt-vision.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"271\\\"
        data-original-width=\\\"575\\\" height=\\\"302\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVeaMcOkd9lejIMMJUFT6xJS36PYymfQr0-cuwtPb1onnd4OuUTMHiaQUmA13hCIV7vImlEMAcoJZe4pvf4hyphenhyphenKOBQQU537Fu8v4tCdqd_cnTS5J27jmMAD91z5i5j-9OQH441WuRN0h6NxZHjjlQXOcgnLWaD7dQDgXv11NzxpuzAOi9qd_R2KmdIFEXlU/w640-h302/chatgpt-vision.png\\\"
        width=\\\"640\\\" /></a></div><div style=\\\"text-align: center;\\\">Option
        to upload image only appears if you select default GPT-4 mode. Any other mode
        this option is not there</div><div><br /></div><div>This is such a shame,
        because I was looking forward to combining ChatGPT's new vision capabilities
        with the existing \\\"Advanced Data Analysis\\\" capabilities (formerly known
        as Code Interpreter).</div><div><br /></div><div>As <a href=\\\"https://musingsaboutlibrarianship.blogspot.com/2023/08/gpt4code-interpreter-playing-electronic.html\\\">I
        covered in past post</a>\_, this is the mode that adds a \\\"Code Sandbox\\\"
        for GPT.</div><div><br /></div><div><div>OpenAI <a href=\\\"https://openai.com/blog/chatgpt-plugins#code-interpreter\\\">describes
        it as</a></div><div><blockquote>We provide our models with a working Python
        interpreter in a sandboxed, firewalled execution environment, along with some
        ephemeral disk space. Code run by our interpreter plugin is evaluated in a
        persistent session that is alive for the duration of a chat conversation (with
        an upper-bound timeout) and subsequent calls can build on top of each other.
        We support uploading files to the current conversation workspace and downloading
        the results of your work.</blockquote></div></div><div>This mode also allows
        you to upload all types of files including csv, text, Python scripts, PDFs.
        It is then capable of running code in the Code Sandbox to extract information
        about the files you upload.</div><div><br /></div><div>In the earlier blog
        post, I\_uploaded <a href=\\\"https://researchdata.smu.edu.sg/articles/dataset/Data_and_code_for_Does_bedtime_music_listening_improve_subjective_sleep_quality_and_next-morning_well-being_in_young_adults/21252285\\\">a
        zipped file of a research dataset that was already deposited into our data
        repository</a> and <a href=\\\"https://musingsaboutlibrarianship.blogspot.com/2023/08/gpt4code-interpreter-playing-electronic.html#rdm\\\">asked
        it to analyse the files.<br /></a></div><div><br /></div><div>In particular,
        I ask it to interprete the files uploaded and suggest ways to improve the
        quality of the deposit.</div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC-tSrGTfRWvukRqTI_iqnCN2gRGIbSihTxxbB6gTodQth2vHUiAu6jd1S9p81kcXiTztcWgTimmGUSNJLMdZu5-h4IFJa-nF2yf8VpHi_tn6xvT77hEF_g-Sx3ZH0mNkPP8BMVP4kn5S3mBVl-XzsM6ynh5KNlzwjKH9Q2h_LLAPUaZ8mFjeiJbYGaBTt/s800/unsub-analysis7.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"669\\\"
        data-original-width=\\\"800\\\" height=\\\"536\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC-tSrGTfRWvukRqTI_iqnCN2gRGIbSihTxxbB6gTodQth2vHUiAu6jd1S9p81kcXiTztcWgTimmGUSNJLMdZu5-h4IFJa-nF2yf8VpHi_tn6xvT77hEF_g-Sx3ZH0mNkPP8BMVP4kn5S3mBVl-XzsM6ynh5KNlzwjKH9Q2h_LLAPUaZ8mFjeiJbYGaBTt/w640-h536/unsub-analysis7.png\\\"
        width=\\\"640\\\" /></a></div><div><br /></div><div>Using its python interpreter,
        it could load csv files of codebooks, data files and handle it pretty well.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKsfsfcbnvgEuXk2DFcjSPqCp6vywouCrSwF_iyyA1RCPWiVwhtcNL_xre0a0uDCY4RoRDhG0uKMvPKaOaVQQzhuhpSlsdomSpzZ6nQQfrPvIEWhEcRnHeXzZDmmeceuMf8JDKC11PTdG8se2uXvipl1ShiOPH-6RiXKeqceHLu4lYWl_HwAgiXpWBUeK-/s616/unsub-analysis2.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"539\\\"
        data-original-width=\\\"616\\\" height=\\\"280\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKsfsfcbnvgEuXk2DFcjSPqCp6vywouCrSwF_iyyA1RCPWiVwhtcNL_xre0a0uDCY4RoRDhG0uKMvPKaOaVQQzhuhpSlsdomSpzZ6nQQfrPvIEWhEcRnHeXzZDmmeceuMf8JDKC11PTdG8se2uXvipl1ShiOPH-6RiXKeqceHLu4lYWl_HwAgiXpWBUeK-/s320/unsub-analysis2.png\\\"
        width=\\\"320\\\" /></a></div><div><br /></div>However, when it comes to images
        (figures) or PDF, it has to use Python libraries to try to extract the text
        with uneven results to 'see' what is in there.<div><br /></div><div>It seems
        to me if the newest image/vision recognition mode was included into this mode,
        results would be much better!<br /><div><br /></div><div>Similarly, the ability
        of ChatGPT Plus to interpret image and draw images are two separate modes
        that currently can't be combined.</div><div><br /></div><div>In other words,
        you can upload an image for it to be described but you can't then use DALL-E
        3 to edit it. This is a surprising weakness since other competitors like Stable
        diffusion do allow this.</div><div><br /></div><div>All in all, I expect when
        LLMs are truly multimodal and can accept input in different formats (e.g.
        text, audio, images, video) and generate output in diff formats (e.g. text,
        audio, image , video), we going to have even more wild use cases.</div><div><br
        /></div><div>Imagine uploading a zipped file of different data formats and
        asking it to analyse then amend and generate analysis in anything from text
        to image to videos!\_ The possibilities are endless from writing/editing a
        paper, creating a short summary of a paper in a poster , video etc.\_</div><div><br
        /></div><div>True <a href=\\\"https://www.searchenginejournal.com/google-gemini-what-we-know-so-far/496494/\\\">multmodal
        language models are rumored to be coming next in Google's next generation
        large language model -Gemini (due 3Q/4Q 2023)</a>, so we shall see what the
        future brings...</div><div><br /></div> </div>\",\"content_text\":\"content_text\",\"id\":\"f2f31f4b-ee09-4748-bc5f-1462cd041c59\",\"language\":\"en\",\"published_at\":1697485014,\"reference\":[],\"relationships\":[],\"summary\":\"\",\"tags\":[],\"title\":\"ChatGPT
        Plus - new DALL-E 3 (image creation) &amp;  Vision (image recognition) capability
        - A quick overview &amp; why I am disappointed.\",\"updated_at\":1697485014,\"url\":\"https://api.follow.it/track-rss-story-click/v3/seIFwoSimx5sOlcHAHO4i7b9ICM-PHur\"},\"highlight\":{\"authors\":[],\"content_html\":\"<img
        src=\\\"https://api.follow.it/track-rss-story-loaded/v1/yJlu_thecP3spGBqFX-6U6CY3L745TL_\\\"
        border=0 width=\\\"1\\\" height=\\\"1\\\" alt=\\\"ChatGPT Plus - new DALL-E
        3 (image creation) &  Vision (image recognition) capability - A quick overview
        & why I am disappointed.\\\" title=\\\"ChatGPT Plus - new DALL-E 3 (image
        creation) &  Vision (image recognition) capability - A quick overview & why
        I am disappointed.\\\"> <p>\_On September 2023, <a href=\\\"https://openai.com/blog/chatgpt-can-now-see-hear-and-speak\\\">OpenAI
        announced that ChatGPT Plus would be enhanced in three ways</a></p><p><br
        /></p><p>1. It would allow you to speak directly with GPT and it would also
        be able to reply in voice</p><p>2. It would be able to create images using
        DALL-E 3, OpenAI's image generation model</p><p>3. It would be able to accept
        image inputs</p><p><br /></p><p>Since I finally gained access to these features,
        I will briefly review them with my thoughts on how impactful they might be
        for library work. To anticipate, the conclusion, these features are very powerful
        but yet I am disappointed. Because each of these abilities are unlocked separately
        and cannot be combined. In other words, this is still not what I consider
        a true multi-modal model that can accept input in multi-modalities (eg text,
        audio, images) and output formats (eg text , audio , images)</p><p><i>Note:
        There's a \\\"new\\\" - Browse with Bing feature as well. I put new in quotes
        because ChatGPT Plus came with at least two earlier versions of this plugin
        that enhanced ChatGPT with results from the web. The <a href=\\\"https://www.cmswire.com/digital-experience/openai-disables-chatgpt-bing-web-browser-plugin/\\\">second
        version also based on Bing was taken down earlier this year because people
        found a way to use it to bypass paywalls.</a></i></p><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBKD-s6-rlOaNK1bxRO7TY_YjX4G4dVT2ZyhlRHxrafksUR9uewWnqEP_L-xvyV_qgjIfN9xhH7WLcB79RasxZuVAo7dUh3ei01Buo4Rv88QL3K3TC0aXwCy8LQ8LbL6ZUH81o2HSij7MVueWDYT82M0bjmch3n0Xaqka1ohXoZ79Q-sprtpCG1BJg1pkW/s400/chatgpt-browsewithbing.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"400\\\"
        data-original-width=\\\"241\\\" height=\\\"320\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBKD-s6-rlOaNK1bxRO7TY_YjX4G4dVT2ZyhlRHxrafksUR9uewWnqEP_L-xvyV_qgjIfN9xhH7WLcB79RasxZuVAo7dUh3ei01Buo4Rv88QL3K3TC0aXwCy8LQ8LbL6ZUH81o2HSij7MVueWDYT82M0bjmch3n0Xaqka1ohXoZ79Q-sprtpCG1BJg1pkW/s320/chatgpt-browsewithbing.png\\\"
        width=\\\"193\\\" /></a></div><br /><p><i>My quick tests show it is nothing
        special, we have been playing with similar features in Bing Chat, Perplexity.ai
        (and of course my blog charts the progress of academic search engines that
        use RAG), and there's nothing that suggests Browse with Bing is notably superior.</i></p><h2
        style=\\\"text-align: left;\\\"><b>1.\_</b>\_It would allow you to speak directly
        with GPT and it would also be able to reply in voice</h2><div>I don't have
        full access to this capability, my Android ChatGPT app allows me to talk to
        it (with very high accuracy - based on Whisper?) but it does not respond back
        in voice.</div><div><br /></div><div>This capability is probably the least
        impactful in my view since we had smart assistants for a while now with pretty
        good voice recognition. That said these smart assistants were always dumb
        in the responses they gave so perhaps it will feel totally different if they
        respond intelligently with voice!</div><div><br /></div><div>For example -\_Tim
        Spalding is very impressed by the voice feature saying it gives \\\"Jarvis
        vibes\\\" and is comparing it to the first time he used a Mac, Gopher and
        World Wide Web and even ChatGPT itself!</div><blockquote class=\\\"twitter-tweet\\\"><p
        dir=\\\"ltr\\\" lang=\\\"en\\\">The new \u201CVoice Conversations\u201D on
        the ChatGPT app is\u2026 well, I think this goes up there with the first time
        I used a Mac, Gopher, the World Wide Web, and ChatGPT itself. Serious Jarvis
        vibes.</p>\u2014 Tim Spalding \U0001F1FA\U0001F1E6 (@librarythingtim) <a href=\\\"https://twitter.com/librarythingtim/status/1710810082265481320?ref_src=twsrc%5Etfw\\\">October
        8, 2023</a></blockquote><p>If he is correct, and he is a very smart dude on
        such matters, once such technologies are in smart assistant/homes, we will
        be amazed.</p><p><br /></p><h2 style=\\\"text-align: left;\\\">\_2. It would
        be able to create images using DALL-E 3, OpenAI's image generation model</h2><div>When
        OpenAI first launched DALL-E in 2021 and <a href=\\\"https://openai.com/dall-e-2\\\">DALL-E
        2</a> in 2022 people were amazed. This were two ground-breaking text to image
        generators.They were quickly followed by competitor's such as Google's\_Imagen
        (against the original DALL-E) and <a href=\\\"https://stability.ai/stable-diffusion\\\">Stable
        Diffusion</a> and <a href=\\\"https://docs.midjourney.com/docs/quick-start\\\">Midjourney</a>
        (against DALL-E 2).</div><div><br /></div><div>In particular, Stable Diffusion
        grabbed a lot of attention by being available Open Source and in terms of
        capability Stable Diffusion and Midjourney (commercial) among others seems
        to have improved rapidly to surpass DALL-E 2's capabilities.</div><div><br
        /></div><div>I have been particularly impressed by Stable Diffusion's capabilities
        including text to image, inpainting and outpainting. (<a href=\\\"https://clipdrop.co/stable-diffusion?utm_campaign=stable_diffusion_promo&utm_medium=cta_button&utm_source=stability_ai\\\">try
        free here</a>), though it might be some of it's capabilities comes from the
        fact that Stable diffusion is trained on an extreme number of images scraped
        from the web and has less guardrails to prevent 'unsafe' images from being
        generated.</div><div><br /></div><div>However, OpenAI has finally struck back
        with DALL-E 3, and it claims to nail one of the last weaknesses of text to
        image generators. Up to this point, you could describe an image and these
        tools would be pretty good at understanding what you want, but if you asked
        it to create an image of something with the words \\\"Happy Birthday\\\" at
        the bottom, most of them would fail terribly at generating the words.</div><div><br
        /></div><div>To use DALL-E 3 in ChatGPT Plus you need to select a special
        mode - DALLE-3</div><div><br /></div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGLitvHvhtaiKv9tdJ3vrLTW17mzLK2iT4JJ0TXFH1B2B-dddrSmzCWgDB31-eelBVT_yhfcsxxy3DFO76PuANPFIc7nEp0gBg8E78t7PMOLGDmfwS5G8aQK_y2QQV0zw1hmn_LaBKK3JU7QLTxYsh58LlDn5z6Qe4lM_ijoFYhsmLvgppqqaEIWGHsftP/s399/chatgpt-dalle3.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"399\\\"
        data-original-width=\\\"269\\\" height=\\\"320\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGLitvHvhtaiKv9tdJ3vrLTW17mzLK2iT4JJ0TXFH1B2B-dddrSmzCWgDB31-eelBVT_yhfcsxxy3DFO76PuANPFIc7nEp0gBg8E78t7PMOLGDmfwS5G8aQK_y2QQV0zw1hmn_LaBKK3JU7QLTxYsh58LlDn5z6Qe4lM_ijoFYhsmLvgppqqaEIWGHsftP/s320/chatgpt-dalle3.png\\\"
        width=\\\"216\\\" /></a></div><br /><div><br /></div><div><br /></div><div><br
        /></div><div>I did a couple of tests using the prompt -\_</div><div><div><br
        /></div><div>\\\"Draw a picture of a Terminator robot from the moves face
        to face with a human librarian\_ At the bottom are the words \\\"AI vs Human\\\"
        and repeated it twice. This is what ChatGPT with DALL-E plugin shows.</div><div><br
        /></div><div><br /></div><div class=\\\"separator\\\" style=\\\"clear: both;
        text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHoJU3S3syBUgJwOolqcr24Xfs4JryYHHneobaPfXBn7fCBIZ7tADQ3cEak9oCZq7B2V1-kOCqcTijSelVTbt_SPkQ6dD8pO1LE1F4R_P0BmXdir79Cuffhl0B_MYpG9uWcGxOx1EHmvUhyphenhyphenTD16neqQ_R2NjMhU62bWpmRONTYbExFkooczeTYtaTMpCCA/s653/dalle-1.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"619\\\"
        data-original-width=\\\"653\\\" height=\\\"606\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHoJU3S3syBUgJwOolqcr24Xfs4JryYHHneobaPfXBn7fCBIZ7tADQ3cEak9oCZq7B2V1-kOCqcTijSelVTbt_SPkQ6dD8pO1LE1F4R_P0BmXdir79Cuffhl0B_MYpG9uWcGxOx1EHmvUhyphenhyphenTD16neqQ_R2NjMhU62bWpmRONTYbExFkooczeTYtaTMpCCA/w640-h606/dalle-1.png\\\"
        width=\\\"640\\\" /></a></div><br /><div><br /></div><div><br /></div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhN6RsLKH9VhpxqULI9FRk83jYny2lb389jZAtnUfw8EXMUfW2P67YWhCQWvHh8UonAkDKiBGVZ72Q6dzY8L5mRO3r88ImVLW9mYpuqZUVnbAhkGa3LZ9PTed3YSwH2S_LHsJpgsoKX7xpj8hP4PCkzoneRkRo2_wnJQJk0c97AEJK6wL1mCqBRvylPO0lb/s651/dalle-2.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"612\\\"
        data-original-width=\\\"651\\\" height=\\\"602\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhN6RsLKH9VhpxqULI9FRk83jYny2lb389jZAtnUfw8EXMUfW2P67YWhCQWvHh8UonAkDKiBGVZ72Q6dzY8L5mRO3r88ImVLW9mYpuqZUVnbAhkGa3LZ9PTed3YSwH2S_LHsJpgsoKX7xpj8hP4PCkzoneRkRo2_wnJQJk0c97AEJK6wL1mCqBRvylPO0lb/w640-h602/dalle-2.png\\\"
        width=\\\"640\\\" /></a></div><br /><div><br /></div><div><br /></div><div><br
        /></div><div>The results are not perfect, in the first batch of four, two
        don't even have the words! In the second batch of four, they all have the
        words, sort of anyway. The last one for some reason misspells it as HUIMAN.
        Still the fact it gets it sometimes right is impressive since most other image
        generators will totally fail most times.</div><div><br /></div><div>Because
        DALL-E 3 is now invoked via ChatGPT (or Bing Chat), you can modify the images
        using natural language. For example, you can ask it to change a male to a
        female or replace the terminator with a demon and it understands very well.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDjgUzF8CKOzStBfGvDSAb4fqYpSL-NrgGq63HfhrgS-pwlXnFAcLhxQjv3WTJ2uTHJn-UHrR71KOlTdy3o4rctQGh3r0xo6IRiaQ-Z26DKHHFDbEhLeFosNOn6EtaOrdSu8zv5fPAwoFQoXmw0W_8R4LFvLckCZI7otEzyVS7FNeYgv1IcVZmYZwdgLpO/s587/dalle3.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"537\\\"
        data-original-width=\\\"587\\\" height=\\\"293\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDjgUzF8CKOzStBfGvDSAb4fqYpSL-NrgGq63HfhrgS-pwlXnFAcLhxQjv3WTJ2uTHJn-UHrR71KOlTdy3o4rctQGh3r0xo6IRiaQ-Z26DKHHFDbEhLeFosNOn6EtaOrdSu8zv5fPAwoFQoXmw0W_8R4LFvLckCZI7otEzyVS7FNeYgv1IcVZmYZwdgLpO/s320/dalle3.png\\\"
        width=\\\"320\\\" /></a></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8pcka2eY_tfoJDZxn5SYo8U42yaN8Dz93FCj93ThhgmcEQ9_rS3kzu-WpB-QwvfG639dldPQ7-ZGCzNdQCGYz20KCM024nvaFwzKvgGsMKIdqSqVzGaDAFgHIVzX1upBr3b57XjpHF9Pio6MeHVbJF9ms9AWWzsTjLIMCNw_oMXpw76yeuROWsICUnqWL/s583/dalle-4.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"544\\\"
        data-original-width=\\\"583\\\" height=\\\"299\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8pcka2eY_tfoJDZxn5SYo8U42yaN8Dz93FCj93ThhgmcEQ9_rS3kzu-WpB-QwvfG639dldPQ7-ZGCzNdQCGYz20KCM024nvaFwzKvgGsMKIdqSqVzGaDAFgHIVzX1upBr3b57XjpHF9Pio6MeHVbJF9ms9AWWzsTjLIMCNw_oMXpw76yeuROWsICUnqWL/s320/dalle-4.png\\\"
        width=\\\"320\\\" /></a></div><br /><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div><div><br /></div><div>To be honest
        this wasn't the feature I was very excited to get in ChatGPT plus, because<a
        href=\\\"https://twitter.com/aarontay/status/1708148983183597983\\\"> I had
        already tested the same function </a>which you can get free via <a href=\\\"https://www.bing.com/create\\\">Bing
        Image Creator which is powered by DALL-E 3</a>\_and the results were similar.\_</div><div><br
        /></div></div><div>Also, while being able to specify text to add is cool,
        it isn't particularly difficult to add text labels to a generated graphic....
        Though I suppose one possible workflow would be for the system to summarise
        a paper , the use that summarise to try to create a Scientific Poster or Visual
        abstract.\_ But so far, I am not successful.</div><div><br /></div><div>For
        fun, I tried uploading a simple visual abstract to ChatGPT and using its vision
        capability it described the visual abstract. I then fed it to DALLE-3 to create
        the visual abstract. The results were weird...</div><div><br /></div><div>For
        example, while it could describe the following simple visual abstract well.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUkeGE8-ioH06dd7dIkJALtNByXJrhHsLgSsHNOfFN0BgU2oKlQjsE-MonB1uA6UvfGIt8uorqnpEf1D3O5Gm1eTGmxK1ZDUd0RfxKHZqTyeVDaOiN7lH3Iv4xZQqEo12g9qCaXxSIyxnXxBpi2L5dY4JxZ98YFUN8JQtmfEJ4FKnEwzYhYgm0A4PxFnvq/s1433/visualabstract-describe.png\\\"
        imageanchor=\\\"1\\\" style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img
        border=\\\"0\\\" data-original-height=\\\"1046\\\" data-original-width=\\\"1433\\\"
        height=\\\"468\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUkeGE8-ioH06dd7dIkJALtNByXJrhHsLgSsHNOfFN0BgU2oKlQjsE-MonB1uA6UvfGIt8uorqnpEf1D3O5Gm1eTGmxK1ZDUd0RfxKHZqTyeVDaOiN7lH3Iv4xZQqEo12g9qCaXxSIyxnXxBpi2L5dY4JxZ98YFUN8JQtmfEJ4FKnEwzYhYgm0A4PxFnvq/w640-h468/visualabstract-describe.png\\\"
        width=\\\"640\\\" /></a></div><br /><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhyv9XNkKZv2gn3cClaAzxVaVkGLVw8ntvC2hJ5bDp_ejgJ16szCxNR6jVze4SpDOyIrwd8p0p0odXuiEgrkulPsDhTnnAJYqI6gz06EwTt_KZiGm30-kTR_Yxbd5BxyqItgJ8QVV6yp-kIZY8W_ONGj2M2w_NBDT6O31zrqn9iP1t3xbAeS6BDUs904Q8/s795/visualabstract-describe-2.png\\\"
        imageanchor=\\\"1\\\" style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img
        border=\\\"0\\\" data-original-height=\\\"629\\\" data-original-width=\\\"795\\\"
        height=\\\"506\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhyv9XNkKZv2gn3cClaAzxVaVkGLVw8ntvC2hJ5bDp_ejgJ16szCxNR6jVze4SpDOyIrwd8p0p0odXuiEgrkulPsDhTnnAJYqI6gz06EwTt_KZiGm30-kTR_Yxbd5BxyqItgJ8QVV6yp-kIZY8W_ONGj2M2w_NBDT6O31zrqn9iP1t3xbAeS6BDUs904Q8/w640-h506/visualabstract-describe-2.png\\\"
        width=\\\"640\\\" /></a></div><br /><div>Feeding the same description to create
        a visual abstract with DALL-E 3 leads to weird results.</div><div><br /></div><div
        class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHTAN0ukWap7wQrGomWAEWAJMaZB0aeiOdWmrfckEdliPSZslaVkqfniL3H7M7xSx3UIC4ENZxUB-KHBtOYWtLDDS6Dkd3zM7bN7dbJGedTYVtgHD7g6c9nChPOd9WThQzuErnE_sS0PGFhCUtORU6XZK91Hflcau2Y1Kz-TA706nmv7cFFZ3N6924nmnf/s707/visualabstract-describe-3.png\\\"
        imageanchor=\\\"1\\\" style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img
        border=\\\"0\\\" data-original-height=\\\"707\\\" data-original-width=\\\"677\\\"
        height=\\\"320\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHTAN0ukWap7wQrGomWAEWAJMaZB0aeiOdWmrfckEdliPSZslaVkqfniL3H7M7xSx3UIC4ENZxUB-KHBtOYWtLDDS6Dkd3zM7bN7dbJGedTYVtgHD7g6c9nChPOd9WThQzuErnE_sS0PGFhCUtORU6XZK91Hflcau2Y1Kz-TA706nmv7cFFZ3N6924nmnf/s320/visualabstract-describe-3.png\\\"
        width=\\\"306\\\" /></a></div><br /><div><br /></div><div><br /></div><div>Stable
        Diffusion I think is still more capable in some ways than DALL-E-3 because
        it is less filtered. For example, you can easily create photos based on celebrities
        or <a href=\\\"https://stable-diffusion-art.com/consistent-face/#Multiple_celebrity_names\\\">even
        create faces that are 20% celebrity X and 80% Celebrity Y</a>, while Dall-E-3
        will refuse to generate images of any individuals.</div><div><br /></div><div>The
        image data that Stable Diffusion trained on is very broad, it even includes
        an image of me! For example, based on the \\\"<a href=\\\"https://haveibeentrained.com/\\\">Have
        I been trained?</a>\\\" database, <a href=\\\"https://haveibeentrained.com/?search_text=%22Aaron%20Tay%22\\\">there
        is at least one photo of me included!</a></div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKEw6OrVNfnJSa6IN9sjfBtXPJigHdTSIt4ce4yDxbfeZQl6yVbReuURtVbK-o9jTo8xBtOycguqI-szcdJPxnFKsLLT49cXx4E39ed10zHaLQFCUaTiM-FgxProM7doYfYkOS8W2M06poiKMx1iza4v93vAVF4O1inoRLKPfU4ob_s53ZvpM6AoEZE-Rg/s1869/Aarontay-Have%20I%20Been%20Trained_.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"834\\\"
        data-original-width=\\\"1869\\\" height=\\\"286\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKEw6OrVNfnJSa6IN9sjfBtXPJigHdTSIt4ce4yDxbfeZQl6yVbReuURtVbK-o9jTo8xBtOycguqI-szcdJPxnFKsLLT49cXx4E39ed10zHaLQFCUaTiM-FgxProM7doYfYkOS8W2M06poiKMx1iza4v93vAVF4O1inoRLKPfU4ob_s53ZvpM6AoEZE-Rg/w640-h286/Aarontay-Have%20I%20Been%20Trained_.png\\\"
        width=\\\"640\\\" /></a></div><br /><div><br /></div><div>Fortunately, or
        unfortunately there are far more photos of other \\\"Aaron Tay\\\", so when
        such systems try to generate a prompt with input \\\"Aaron Tay\\\" it is unlikely
        to get an image close to me (though it will likely generate Chinese facial
        features). For celebrities it pretty much nails it of course since almost
        all the training images will be of their likeness (try say\_<a href=\\\"https://haveibeentrained.com/?search_text=Angelina%20jolie\\\">Angelina
        Jolie</a>)</div><div><br /></div><div>DALL-E 3 when used via ChatGPT plus
        also has a host of other restrictions, if <a href=\\\"https://twitter.com/aarontay/status/1713972855384490301\\\">the
        system prompt here is accurate</a> other restrictions include instructing
        the model to\_</div><div><br /></div><div><ul style=\\\"text-align: left;\\\"><li>not
        \\\"create images in the style of artists whose last work was created within
        the last 100 years\\\"<br /></li><li>not \\\"create any imagery that would
        be offensive.\\\"</li><li>\\\"Silently modify descriptions that include names
        or hints or references of specific people or celebritie by carefully selecting
        a few minimal modifications to substitute references to the people with generic
        descriptions that don't divulge any information about their identities, except
        for their genders and physiques\\\"<br /><br /></li></ul></div><div><br /></div><h2
        style=\\\"text-align: left;\\\">3. It would be able to accept image inputs</h2><div>This
        is probably the function that is getting the most attention right now. This
        gives ChatGPT the ability to understand images you upload.</div><div><br /></div><div><a
        href=\\\"https://openai.com/blog/chatgpt-can-now-see-hear-and-speak\\\">OpenAI
        says</a></div><div><blockquote>Image understanding is powered by multimodal
        GPT-3.5 and GPT-4. These models apply their language reasoning skills to a
        wide range of images, such as photographs, screenshots, and documents containing
        both text and images.</blockquote></div><div>People are really impressed by
        this capability and so am I. For one, it can read screenshots of English test,
        figures, tables from papers. This is clearly going to be useful for interpreting
        papers.</div><div><br /></div><div>Twitter/X is full of amazing examples,
        here I try an example by uploading a visual abstract created by my colleagues.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi20GdyIcHAtp6wK2OaFFQgv9Ur-FmDo9PeJPHohaTDd6BK59oWZmOsq97-Y1fBi7btDMLe8elsWBctcx3DBYZ7JPNuWFRQZeUeXbAHE8RPI2J7hiiPPGE14pzfRTS77ZRa6gedd8UQp197YiL4h2aHxoe4-fI3exfrQJm-rJM3yffT4sli6M6WIil8iStV/s1211/dalle-visualabstract.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"659\\\"
        data-original-width=\\\"1211\\\" height=\\\"348\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi20GdyIcHAtp6wK2OaFFQgv9Ur-FmDo9PeJPHohaTDd6BK59oWZmOsq97-Y1fBi7btDMLe8elsWBctcx3DBYZ7JPNuWFRQZeUeXbAHE8RPI2J7hiiPPGE14pzfRTS77ZRa6gedd8UQp197YiL4h2aHxoe4-fI3exfrQJm-rJM3yffT4sli6M6WIil8iStV/w640-h348/dalle-visualabstract.png\\\"
        width=\\\"640\\\" /></a></div><br /><div>This is what ChatGPT with vision
        sees.</div><div><br /></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6dC-bcU-OcXIWe2S6gdiPq5sW1OM64zcODINDTXePSuMFZNVlcN6APTUxleAHhBIfVfckesLMrY0fuERUUoRQUuS1QCwedrx9Tp8e5h0VegO_2ELA-j9IB8FS232C1lk1Ag96VWoEYbDchi1m_ZFOqRxyQUhTHIhiJbD7gvRB8kD6nL0tjed03jBZcaKK/s1010/dalle-visualabstract-2.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"1010\\\"
        data-original-width=\\\"678\\\" height=\\\"640\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6dC-bcU-OcXIWe2S6gdiPq5sW1OM64zcODINDTXePSuMFZNVlcN6APTUxleAHhBIfVfckesLMrY0fuERUUoRQUuS1QCwedrx9Tp8e5h0VegO_2ELA-j9IB8FS232C1lk1Ag96VWoEYbDchi1m_ZFOqRxyQUhTHIhiJbD7gvRB8kD6nL0tjed03jBZcaKK/w430-h640/dalle-visualabstract-2.png\\\"
        width=\\\"430\\\" /></a></div><br /><div>I tried a <a href=\\\"https://twitter.com/aarontay/status/1713865247273205969\\\">few
        more examples of visual abstracts</a> and it is not perfect but still impressive</div><div><br
        /></div>\\n<blockquote class=\\\"twitter-tweet\\\" data-conversation=\\\"none\\\"><p
        dir=\\\"ltr\\\" lang=\\\"en\\\">Okay I tried this visual abstract on GPT4
        vision. It has trouble understanding that there is an icon for RCT and Population
        based Cohorts. It thinks that double arrow icon means < (less than) , (1)
        <a href=\\\"https://t.co/K9b6n5r4M1\\\">pic.twitter.com/K9b6n5r4M1</a></p>\u2014
        Aaron Tay (@aarontay) <a href=\\\"https://twitter.com/aarontay/status/1713865247273205969?ref_src=twsrc%5Etfw\\\">October
        16, 2023</a></blockquote> \\n\\n<blockquote class=\\\"twitter-tweet\\\" data-conversation=\\\"none\\\"><p
        dir=\\\"ltr\\\" lang=\\\"en\\\">I asked GPT4 for the 95% CI for intubation,
        at first it correctly refused saying it isn't labelled. I ask it to estimate
        anyway and it basically makes things up (multiple tries) - (2) <a href=\\\"https://t.co/4ZTAvAvyFQ\\\">pic.twitter.com/4ZTAvAvyFQ</a></p>\u2014
        Aaron Tay (@aarontay) <a href=\\\"https://twitter.com/aarontay/status/1713866127108112584?ref_src=twsrc%5Etfw\\\">October
        16, 2023</a></blockquote> \\n\\n\\n\\n<div><br /></div><div><br /></div><h2
        style=\\\"text-align: left;\\\">A true multi-model Large Language Model will
        be amazing</h2><div>Despite all the amazing new capabilities, I was still
        somewhat disappointed because each of these capabilities can only be used
        seperately.</div><div><br /></div><div>By default, when you start a prompt
        in ChatGPT you must choose from the following options</div><div><br /></div><div><div
        class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/a/AVvXsEj1nLHu_ccPy2bMNLvoTmiUDKWl8ZOzaXZ61fUQmir49EePaD6h5aZ-pqyYBMYajhWxSq5-4SNpbOGaCvN_m7ZI4tM-h-ntQHt8f1T79Qa8JAAlxMKFtsS3UobZhm2RQyWNEN7J45_pEuKEcgCDX5bhrh4r1-xJal6ogHb3Cd3dHCrupwmsXd0KjQ5Ivh2Y\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img alt=\\\"\\\" data-original-height=\\\"320\\\"
        data-original-width=\\\"193\\\" height=\\\"240\\\" src=\\\"https://blogger.googleusercontent.com/img/a/AVvXsEj1nLHu_ccPy2bMNLvoTmiUDKWl8ZOzaXZ61fUQmir49EePaD6h5aZ-pqyYBMYajhWxSq5-4SNpbOGaCvN_m7ZI4tM-h-ntQHt8f1T79Qa8JAAlxMKFtsS3UobZhm2RQyWNEN7J45_pEuKEcgCDX5bhrh4r1-xJal6ogHb3Cd3dHCrupwmsXd0KjQ5Ivh2Y\\\"
        width=\\\"145\\\" /></a></div><div class=\\\"separator\\\" style=\\\"clear:
        both; text-align: center;\\\"><br /></div>Each of these modes are mutually
        exclusive. There's isn't a mode you can choose for uploading images, but it
        only appears when you choose \\\"Default\\\"</div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVeaMcOkd9lejIMMJUFT6xJS36PYymfQr0-cuwtPb1onnd4OuUTMHiaQUmA13hCIV7vImlEMAcoJZe4pvf4hyphenhyphenKOBQQU537Fu8v4tCdqd_cnTS5J27jmMAD91z5i5j-9OQH441WuRN0h6NxZHjjlQXOcgnLWaD7dQDgXv11NzxpuzAOi9qd_R2KmdIFEXlU/s575/chatgpt-vision.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"271\\\"
        data-original-width=\\\"575\\\" height=\\\"302\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVeaMcOkd9lejIMMJUFT6xJS36PYymfQr0-cuwtPb1onnd4OuUTMHiaQUmA13hCIV7vImlEMAcoJZe4pvf4hyphenhyphenKOBQQU537Fu8v4tCdqd_cnTS5J27jmMAD91z5i5j-9OQH441WuRN0h6NxZHjjlQXOcgnLWaD7dQDgXv11NzxpuzAOi9qd_R2KmdIFEXlU/w640-h302/chatgpt-vision.png\\\"
        width=\\\"640\\\" /></a></div><div style=\\\"text-align: center;\\\">Option
        to upload image only appears if you select default GPT-4 mode. Any other mode
        this option is not there</div><div><br /></div><div>This is such a shame,
        because I was looking forward to combining ChatGPT's new vision capabilities
        with the existing \\\"Advanced Data Analysis\\\" capabilities (formerly known
        as Code Interpreter).</div><div><br /></div><div>As <a href=\\\"https://musingsaboutlibrarianship.blogspot.com/2023/08/gpt4code-interpreter-playing-electronic.html\\\">I
        covered in past post</a>\_, this is the mode that adds a \\\"Code Sandbox\\\"
        for GPT.</div><div><br /></div><div><div>OpenAI <a href=\\\"https://openai.com/blog/chatgpt-plugins#code-interpreter\\\">describes
        it as</a></div><div><blockquote>We provide our models with a working Python
        interpreter in a sandboxed, firewalled execution environment, along with some
        ephemeral disk space. Code run by our interpreter plugin is evaluated in a
        persistent session that is alive for the duration of a chat conversation (with
        an upper-bound timeout) and subsequent calls can build on top of each other.
        We support uploading files to the current conversation workspace and downloading
        the results of your work.</blockquote></div></div><div>This mode also allows
        you to upload all types of files including csv, text, Python scripts, PDFs.
        It is then capable of running code in the Code Sandbox to extract information
        about the files you upload.</div><div><br /></div><div>In the earlier blog
        post, I\_uploaded <a href=\\\"https://researchdata.smu.edu.sg/articles/dataset/Data_and_code_for_Does_bedtime_music_listening_improve_subjective_sleep_quality_and_next-morning_well-being_in_young_adults/21252285\\\">a
        zipped file of a research dataset that was already deposited into our data
        repository</a> and <a href=\\\"https://musingsaboutlibrarianship.blogspot.com/2023/08/gpt4code-interpreter-playing-electronic.html#rdm\\\">asked
        it to analyse the files.<br /></a></div><div><br /></div><div>In particular,
        I ask it to interprete the files uploaded and suggest ways to improve the
        quality of the deposit.</div><div><br /></div><div class=\\\"separator\\\"
        style=\\\"clear: both; text-align: center;\\\"><a href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC-tSrGTfRWvukRqTI_iqnCN2gRGIbSihTxxbB6gTodQth2vHUiAu6jd1S9p81kcXiTztcWgTimmGUSNJLMdZu5-h4IFJa-nF2yf8VpHi_tn6xvT77hEF_g-Sx3ZH0mNkPP8BMVP4kn5S3mBVl-XzsM6ynh5KNlzwjKH9Q2h_LLAPUaZ8mFjeiJbYGaBTt/s800/unsub-analysis7.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"669\\\"
        data-original-width=\\\"800\\\" height=\\\"536\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC-tSrGTfRWvukRqTI_iqnCN2gRGIbSihTxxbB6gTodQth2vHUiAu6jd1S9p81kcXiTztcWgTimmGUSNJLMdZu5-h4IFJa-nF2yf8VpHi_tn6xvT77hEF_g-Sx3ZH0mNkPP8BMVP4kn5S3mBVl-XzsM6ynh5KNlzwjKH9Q2h_LLAPUaZ8mFjeiJbYGaBTt/w640-h536/unsub-analysis7.png\\\"
        width=\\\"640\\\" /></a></div><div><br /></div><div>Using its python interpreter,
        it could load csv files of codebooks, data files and handle it pretty well.</div><div><br
        /></div><div class=\\\"separator\\\" style=\\\"clear: both; text-align: center;\\\"><a
        href=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKsfsfcbnvgEuXk2DFcjSPqCp6vywouCrSwF_iyyA1RCPWiVwhtcNL_xre0a0uDCY4RoRDhG0uKMvPKaOaVQQzhuhpSlsdomSpzZ6nQQfrPvIEWhEcRnHeXzZDmmeceuMf8JDKC11PTdG8se2uXvipl1ShiOPH-6RiXKeqceHLu4lYWl_HwAgiXpWBUeK-/s616/unsub-analysis2.png\\\"
        style=\\\"margin-left: 1em; margin-right: 1em;\\\"><img border=\\\"0\\\" data-original-height=\\\"539\\\"
        data-original-width=\\\"616\\\" height=\\\"280\\\" src=\\\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKsfsfcbnvgEuXk2DFcjSPqCp6vywouCrSwF_iyyA1RCPWiVwhtcNL_xre0a0uDCY4RoRDhG0uKMvPKaOaVQQzhuhpSlsdomSpzZ6nQQfrPvIEWhEcRnHeXzZDmmeceuMf8JDKC11PTdG8se2uXvipl1ShiOPH-6RiXKeqceHLu4lYWl_HwAgiXpWBUeK-/s320/unsub-analysis2.png\\\"
        width=\\\"320\\\" /></a></div><div><br /></div>However, when it comes to images
        (figures) or PDF, it has to use Python libraries to try to extract the text
        with uneven results to 'see' what is in there.<div><br /></div><div>It seems
        to me if the newest image/vision recognition mode was included into this mode,
        results would be much better!<br /><div><br /></div><div>Similarly, the ability
        of ChatGPT Plus to interpret image and draw images are two separate modes
        that currently can't be combined.</div><div><br /></div><div>In other words,
        you can upload an image for it to be described but you can't then use DALL-E
        3 to edit it. This is a surprising weakness since other competitors like Stable
        diffusion do allow this.</div><div><br /></div><div>All in all, I expect when
        LLMs are truly multimodal and can accept input in different formats (e.g.
        text, audio, images, video) and generate output in diff formats (e.g. text,
        audio, image , video), we going to have even more wild use cases.</div><div><br
        /></div><div>Imagine uploading a zipped file of different data formats and
        asking it to analyse then amend and generate analysis in anything from text
        to image to videos!\_ The possibilities are endless from writing/editing a
        paper, creating a short summary of a paper in a poster , video etc.\_</div><div><br
        /></div><div>True <a href=\\\"https://www.searchenginejournal.com/google-gemini-what-we-know-so-far/496494/\\\">multmodal
        language models are rumored to be coming next in Google's next generation
        large language model -Gemini (due 3Q/4Q 2023)</a>, so we shall see what the
        future brings...</div><div><br /></div> </div>\",\"reference\":[],\"summary\":\"\",\"tags\":[],\"title\":\"ChatGPT
        Plus - new DALL-E 3 (image creation) &amp;  Vision (image recognition) capability
        - A quick overview &amp; why I am disappointed.\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Sven
        Lieber\",\"url\":\"https://orcid.org/0000-0002-7304-3787\"}],\"blog_id\":\"4z1mf58\",\"blog_name\":\"Sven
        Lieber\",\"blog_slug\":\"sven_lieber\",\"content_html\":\"<p>What do the books
        \u201CThe invention of Nature\u201D and \u201CDe uitvinder van de natuur\u201D
        have in common?\\nWell, they are both different versions of the same <em>work</em>
        \u201CThe invention of nature\u201D by Andrea Wulf,\\nwhether it is in a different
        format or a different language.\\nIn this blog post I will briefly introduce
        the advantages of keeping\\nwork-level records in library catalogs.\\nFurthermore,
        I will introduce <strong>a fast Python implementation</strong> (<a href=\\\"https://zenodo.org/doi/10.5281/zenodo.10011416\\\"
        target=\\\"_blank\\\">DOI: 10.5281/zenodo.10011416</a>)\\nwhich we used in
        the BELTRANS project to identify the works in a corpus of book translations
        #FRBRization.</p>\\n<h2>Context</h2>\\n<p>Library catalogs usually contain
        records about different versions of a book.\\nIn technical terms, these records
        are so-called manifestations,\\nbut for simplicity I will refer to them simply
        as book versions or book editions.\\nSuch a conceptual division between different
        levels of description\\nis part of the Functional Requirements for Bibliographic
        Records (FRBR)\\nor how it is referred to nowadays, the IFLA Library Reference
        Model (IFLA-LRM).</p>\\n<h2>Better user experience and better data quality
        due to identified works</h2>\\n<p>Imagine that you are searching for a specific
        book in a library catalog.\\nYou remember that it had the word \u201Cinvention\u201D
        in its title.\\nInstead of seeing two search results <strong>\u201CThe invention
        of nature\u201D by Andrea Wulf</strong>\\nand <strong>\u201CInvention: A Life
        of Learning through Failure\u201D by James Dyson</strong>,\\nyou are overwhelmed
        with hundreds of versions of both books and others\\nsuch as ebooks, paperbacks,
        reprints, etc.\\nActually you are interested in finding out which works the
        library has that contain the title.\\nWhich specific version maybe is of less
        priority to you.\\nIn such a scenario it might be better to only show works
        in the search results.\\nAfter clicking on the work you looked for,\\nyou
        still can be presented with all the different versions of the book.</p>\\n<p>There
        is also an advantage for the library personnel that works with the book data.\\nCreating
        catalog records often is a manual process, or at least it was for a long time.\\nSome
        of the records might contain more information than others,\\nfor example the
        genre according to a specific categorization.\\nIdentifying all the different
        versions of a book in your own catalog\\nor in a collection of (other) catalogs
        can help to fill gaps in the data.\\nIf one book record is classified as a
        Comic and mentions a specific person as its illustrator,\\nanother record
        of a different version of that book which maybe has no such information,\\ncan
        be completed/enriched with this information.</p>\\n<h2>Alright, you convinced
        me, how can I do it?</h2>\\n<p>There are different ways to do this.\\nI have
        read <a href=\\\"https://orkg.org/list/R608176\\\" target=\\\"_blank\\\">a
        bit of literature</a> and it seems that there is one very common way to do
        this.\\nLet me introduce you to <strong>the work-set clustering algorithm</strong>,
        initially devised by OCLC.\\nThe idea is simple: create several possible descriptive
        keys of a book version\\nbased on a combination of work-level information
        such as title and author,\\nthen check if two book versions have at least
        one key in common.\\nSounds complicated? Check out the image below which hopefully
        makes it clear.</p>\\n<p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</p><figure>\\n
        \ <div>\\n    <div><img alt=\\\"The English and Dutch version of a book that
        can be matched based on a common descriptive key\\\" srcset=\\\"/media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_8e5329395a5fd3575c043cd1e7149339.webp
        400w, /media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_b7e29052741b4312d88a720154a6e9c0.webp
        760w, /media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_1200x1200_fit_q75_h2_lanczos.webp
        1200w\\\" src=\\\"https://sven-lieber.org/media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_8e5329395a5fd3575c043cd1e7149339.webp\\\"
        width=\\\"760\\\" height=\\\"260\\\" loading=\\\"lazy\\\" /></div>\\n  </div></figure>\\n<p></p>\\n<p>In
        this example you can see two book records (red),\\neach describing a book
        version.\\nFor each book version descriptive keys (blue) are generated\\nto
        assist in finding matches.\\nAs you can see, both records have at least one
        key in common.\\nWe found a match!\\nIn this case only because we also considered
        a possible original title as part of the key.\\nSimilarly we would find a
        match between the shown Dutch version of the printed book\\nand a hypothetical
        ebook version: the ISBN would be different,\\nbut there would be a <strong>match
        based on the title-author or title-translator combination</strong>.</p>\\n<h2>Nice
        idea, how do I write a program to do this?</h2>\\n<p>That\u2019s the one million
        dollar question.\\nDifferent scientific papers talk about the algorithm, a
        few share code.\\nBut a code implementation always depends on the context
        it was written for,\\nfor example by using XSLT rules for XML records.\\nI
        tried to write a program in Python that is as generic as possible\\nand came
        up with three solutions.\\nWhy three? Well, it turned out the first two were
        not fast enough\\nfor the amount of translations we have in the BELTRANS project.</p>\\n<h3>First
        attempt: reusing the Python package sklearn</h3>\\n<p>Most of the papers I
        have read are already a few years old,\\nthey mainly use software stacks that
        are less common nowadays.\\nI needed a solution that I can quickly run without
        a large setup.\\nCurrently a lot of code libraries for common languages such
        as Python exist.\\nSo why not reuse an existing one?!\\nThe class <code>AgglomerativeClustering</code>
        from the Python package sklearn implements a hierarchical clustering algorithm.</p>\\n<p>Actually
        I am not interested in different levels of clusters,\\nonly the highest level
        with the least number of clusters.\\nSo I did what a lot of people do nowadays,
        <strong>I had a chat with ChatGPT to brainstorm</strong>\\npossible solutions
        with AgglomerativeClustering.\\nAnd it came up with a smart idea!</p>\\n<p>The
        input for the clustering is a so-called distance matrix:\\nbasically a table
        listing all elements as rows\\nand all elements as columns,\\nwhere the value
        in a cell indicates how similar an element in row X is with the element in
        column Y.\\nWe don\u2019t need a sophisticated similarity measure,\\na simple
        \u201Cyes\u201D for an overlap would suffice.</p>\\n<p>ChatGPTs idea was to
        use negative distance values.\\nLike this, the whole clustering just takes
        two lines of code\\nwhere I instruct the AgglomerativeClustering library to
        return clusters with distance threshold zero.</p>\\n<div><pre><code><span><span><span>from</span>
        <span>sklearn.cluster</span> <span>import</span> <span>AgglomerativeClustering</span>\\n</span></span><span><span>\\n</span></span><span><span><span>model</span>
        <span>=</span> <span>AgglomerativeClustering</span><span>(</span><span>n_clusters</span><span>=</span><span>None</span><span>,</span>
        \\\\\\n</span></span><span><span>                                <span>affinity</span><span>=</span><span>'precomputed'</span><span>,</span>
        \\\\\\n</span></span><span><span>                                <span>linkage</span><span>=</span><span>'single'</span><span>,</span>
        \\\\\\n</span></span><span><span>                                <span>distance_threshold</span><span>=</span><span>0</span><span>)</span>\\n</span></span><span><span>\\n</span></span><span><span><span>#
        get a list where each index </span>\\n</span></span><span><span><span># corresponds
        to an element in elementIDs</span>\\n</span></span><span><span><span># e.g.
        [0,2,0,1] the first element is in cluster 0, </span>\\n</span></span><span><span><span>#
        the second in cluster 2, </span>\\n</span></span><span><span><span># the third
        in cluster 0 and the 4th in cluster 1</span>\\n</span></span><span><span><span>clusterLabels</span>
        <span>=</span> <span>model</span><span>.</span><span>fit_predict</span><span>(</span><span>distanceMatrix</span><span>)</span>\\n</span></span></code></pre></div><p>This
        went well for a relevant subset of roughly 20 thousand records in our BELTRANS
        project.\\nAfter a few minutes we got correct and directly reusable results!\\nHowever,
        when we wanted to achieve the same for more than 150 thousand records,\\nthis
        implementation of the clustering was not performant enough.\\nEven on a powerful
        server with more than 32 GB of RAM,\\nthe computation of the distance matrix
        takes too much time and space.</p>\\n<div>\\n  <div>\\n    One element likely
        has very few or no matches with all the other elements.\\nA sparse matrix
        can be used, where not <em>every</em> value is stored, only not-zero values.\\nUnfortunately
        the <code>AgglomerativeClustering</code> module does not support this.\\n
        \ </div>\\n</div>\\n<h3>Second attempt: implementing the algorithm myself</h3>\\n<p>A
        little disappointed from the last solution,\\nI thought I will try implementing
        it myself,\\nall I need: a few data structures and loops.\\nThe idea of the
        algorithm is to start with every element in its own cluster.\\nThen comparing
        it to other clusters until we find a match.\\nWhen a match is found the loop
        is stopped,\\nthe clusters are merged and we loop again over the (now updated)
        list of clusters.\\nWe do all this until no more merges are possible.</p>\\n<p>This
        algorithm does not need a large distance matrix to start with.\\nTesting if
        there is an overlap between two clusters can be reduced to a single intersection
        operation\\nbetween two Python sets that represent the descriptive keys of
        the respective clusters.</p>\\n<p>Unfortunately all that takes too long.\\nWith
        a few example data I could verify that it worked as expected,\\nbut as soon
        as there are a few thousand elements the algorithm keeps running \u2026</p>\\n<div>\\n
        \ <div>\\n    I tried to remedy the situation by putting parts of the computation\\nin
        a reusable function that I can call in parallel by using\\n<code>ThreadPoolExecutor</code>
        or <code>ProcessPoolExecutor</code>, but it did not help much.\\n  </div>\\n</div>\\n<h3>Third
        attempt: inverted index for clustering in no time</h3>\\n<p>Even more frustrated
        I went back to the drawing board, or pen and paper in my case.\\nWhat I need
        is a solution that ideally only runs once over all elements\\nthat need to
        be clustered.\\nBut how do I find out possible overlaps without comparing
        everything?</p>\\n<p>The disadvantages of the previous solution were\\nthat
        I had to perform set intersections to detect overlaps\\nwhile iterating over
        all clusters,\\nstop the iteration to update the clusters and start iterating
        again.</p>\\n<p>Actually the data already includes the matches between elements
        implicitly,\\nand I used this to compute clusters in no time.\\nEvery element
        has one or more descriptive keys.\\nBut instead of only storing the mapping
        element-&gt;descriptive keys in a Python dictionary,\\nI also store descriptive
        key-&gt;elements in another dictionary.\\n<strong>This is my <em>inverted
        index</em> which I can use to look up overlaps.</strong></p>\\n<p>Eventually
        I start with zero clusters and I simply iterate over the inverted index.\\nIf
        one of the elements from a key already is part of a cluster,\\nI add the other
        elements to it.\\nIf no cluster was found, I create a new one and add the
        elements.\\nBasically all elements of a descriptive key I check,\\neither
        go to an existing cluster or end up in a new cluster.\\nIf different elements
        for a descriptive key are already in different clusters,\\na new merged cluster
        is created and the old ones deleted.</p>\\n<p>This solution works as well,
        and wow, it is fast too.\\nInterestingly <strong>it takes longer to upload
        the computed cluster information to our database\\nthan performing the clustering
        itself.</strong>\\nInverted indexes can do a lot of heavy lifting!</p>\\n<p>You
        can find the source code on GitHub (<a href=\\\"https://zenodo.org/doi/10.5281/zenodo.10011416\\\"
        target=\\\"_blank\\\">DOI: 10.5281/zenodo.10011416</a>).</p>\\n<h3>Why is
        it so fast?</h3>\\n<p>It all has to do with scalability.\\nI only iterate
        <em>once</em> over all elements to build the inverted index dictionary.\\nThen
        I iterate <em>once</em> over all elements in the inverted index to compute
        the clusters.\\nFor 100 input elements this roughly means iterating 200 times,\\nfor
        100,000 input elements roughly 200,000 times.\\nAn algorithm that <strong>scales
        linear to its input</strong>.</p>\\n<p>Compare this to compute a distance
        matrix of 100*100 elements\\n(compute and store 10 thousand results)\\nand
        100,000*100,000 elements (compute and store 10 billion results).\\nAn algorithm
        that <strong>scales quadratic to its input</strong>.</p>\\n<h2>Final remarks</h2>\\n<p>Having
        work-level information brings advantages for both librarians and users.\\nThe
        presented solution is <em>just one possible</em> implementation of <em>one
        possible</em> algorithm.\\nWhich data fields you use to create descriptive
        keys\\nand how you extract them from your bibliographic data explicitly was
        not covered in this post.\\nI also did not cover how you could use the information
        of the clusters to actually achieve the advantages.\\nThe main purpose of
        this blog post was to <strong>introduce and share the generic Python implementation</strong>,\\nbecause
        I had the feeling there is not much out there that can be reused easily.</p>\\n<p>For
        the BELTRANS project we extracted descriptive keys\\nwith a SPARQL query from
        an Resource Description Framework (RDF) representation of bibliographic information.\\nSimilarly,
        we used SPARQL INSERT queries to explicitly create RDF representations\\nfor
        each work cluster and link its manifestations to it using the fabio ontology.\\n<a
        href=\\\"https://github.com/kbrbe/beltrans-data-integration/tree/main/data-integration/sparql-queries/clustering\\\"
        target=\\\"_blank\\\">Here</a> you can have a look at our SPARQL queries.</p>\\n<p>Curious
        about more content like this or some behind-the-scenes?\\nConsider subscribing
        to my bi-weekly Newsletter <em>FAIR Data Digest</em>\\nto receive more interesting
        content about Linked Data every other Tuesday!</p>\\n\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/4hd4r-1tk44\",\"id\":\"96613ce9-bd1d-421f-a259-a5b5173eecc5\",\"image\":\"https://sven-lieber.org/media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_8e5329395a5fd3575c043cd1e7149339.webp\",\"language\":\"en\",\"published_at\":1697479200,\"reference\":[],\"relationships\":[],\"summary\":\"What
        do the books \u201CThe invention of Nature\u201D and \u201CDe uitvinder van
        de natuur\u201D have in common? Well, they are both different versions of
        the same <em>work</em> \u201CThe invention of nature\u201D by Andrea Wulf,
        whether it is in a different format or a different language. In this blog
        post I will briefly introduce the advantages of keeping work-level records
        in library catalogs.\",\"tags\":[],\"title\":\"Clustering Book editions\",\"updated_at\":1697479200,\"url\":\"https://sven-lieber.org/en/2023/10/16/clustering-book-editions\"},\"highlight\":{\"authors\":[{\"name\":\"Sven
        Lieber\",\"url\":\"https://orcid.org/0000-0002-7304-3787\"}],\"content_html\":\"<p>What
        do the books \u201CThe invention of Nature\u201D and \u201CDe uitvinder van
        de natuur\u201D have in common?\\nWell, they are both different versions of
        the same <em>work</em> \u201CThe invention of nature\u201D by Andrea Wulf,\\nwhether
        it is in a different format or a different language.\\nIn this blog post I
        will briefly introduce the advantages of keeping\\nwork-level records in library
        catalogs.\\nFurthermore, I will introduce <strong>a fast Python implementation</strong>
        (<a href=\\\"https://zenodo.org/doi/10.5281/zenodo.10011416\\\" target=\\\"_blank\\\">DOI:
        10.5281/zenodo.10011416</a>)\\nwhich we used in the BELTRANS project to identify
        the works in a corpus of book translations #FRBRization.</p>\\n<h2>Context</h2>\\n<p>Library
        catalogs usually contain records about different versions of a book.\\nIn
        technical terms, these records are so-called manifestations,\\nbut for simplicity
        I will refer to them simply as book versions or book editions.\\nSuch a conceptual
        division between different levels of description\\nis part of the Functional
        Requirements for Bibliographic Records (FRBR)\\nor how it is referred to nowadays,
        the IFLA Library Reference Model (IFLA-LRM).</p>\\n<h2>Better user experience
        and better data quality due to identified works</h2>\\n<p>Imagine that you
        are searching for a specific book in a library catalog.\\nYou remember that
        it had the word \u201Cinvention\u201D in its title.\\nInstead of seeing two
        search results <strong>\u201CThe invention of nature\u201D by Andrea Wulf</strong>\\nand
        <strong>\u201CInvention: A Life of Learning through Failure\u201D by James
        Dyson</strong>,\\nyou are overwhelmed with hundreds of versions of both books
        and others\\nsuch as ebooks, paperbacks, reprints, etc.\\nActually you are
        interested in finding out which works the library has that contain the title.\\nWhich
        specific version maybe is of less priority to you.\\nIn such a scenario it
        might be better to only show works in the search results.\\nAfter clicking
        on the work you looked for,\\nyou still can be presented with all the different
        versions of the book.</p>\\n<p>There is also an advantage for the library
        personnel that works with the book data.\\nCreating catalog records often
        is a manual process, or at least it was for a long time.\\nSome of the records
        might contain more information than others,\\nfor example the genre according
        to a specific categorization.\\nIdentifying all the different versions of
        a book in your own catalog\\nor in a collection of (other) catalogs can help
        to fill gaps in the data.\\nIf one book record is classified as a Comic and
        mentions a specific person as its illustrator,\\nanother record of a different
        version of that book which maybe has no such information,\\ncan be completed/enriched
        with this information.</p>\\n<h2>Alright, you convinced me, how can I do it?</h2>\\n<p>There
        are different ways to do this.\\nI have read <a href=\\\"https://orkg.org/list/R608176\\\"
        target=\\\"_blank\\\">a bit of literature</a> and it seems that there is one
        very common way to do this.\\nLet me introduce you to <strong>the work-set
        clustering algorithm</strong>, initially devised by OCLC.\\nThe idea is simple:
        create several possible descriptive keys of a book version\\nbased on a combination
        of work-level information such as title and author,\\nthen check if two book
        versions have at least one key in common.\\nSounds complicated? Check out
        the image below which hopefully makes it clear.</p>\\n<p>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</p><figure>\\n
        \ <div>\\n    <div><img alt=\\\"The English and Dutch version of a book that
        can be matched based on a common descriptive key\\\" srcset=\\\"/media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_8e5329395a5fd3575c043cd1e7149339.webp
        400w, /media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_b7e29052741b4312d88a720154a6e9c0.webp
        760w, /media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_1200x1200_fit_q75_h2_lanczos.webp
        1200w\\\" src=\\\"https://sven-lieber.org/media/clustering-book-editions/work-set-clustering-descriptive-keys_hu2914f170801a4fd4098bf76695b64d75_88986_8e5329395a5fd3575c043cd1e7149339.webp\\\"
        width=\\\"760\\\" height=\\\"260\\\" loading=\\\"lazy\\\" /></div>\\n  </div></figure>\\n<p></p>\\n<p>In
        this example you can see two book records (red),\\neach describing a book
        version.\\nFor each book version descriptive keys (blue) are generated\\nto
        assist in finding matches.\\nAs you can see, both records have at least one
        key in common.\\nWe found a match!\\nIn this case only because we also considered
        a possible original title as part of the key.\\nSimilarly we would find a
        match between the shown Dutch version of the printed book\\nand a hypothetical
        ebook version: the ISBN would be different,\\nbut there would be a <strong>match
        based on the title-author or title-translator combination</strong>.</p>\\n<h2>Nice
        idea, how do I write a program to do this?</h2>\\n<p>That\u2019s the one million
        dollar question.\\nDifferent scientific papers talk about the algorithm, a
        few share code.\\nBut a code implementation always depends on the context
        it was written for,\\nfor example by using XSLT rules for XML records.\\nI
        tried to write a program in Python that is as generic as possible\\nand came
        up with three solutions.\\nWhy three? Well, it turned out the first two were
        not fast enough\\nfor the amount of translations we have in the BELTRANS project.</p>\\n<h3>First
        attempt: reusing the Python package sklearn</h3>\\n<p>Most of the papers I
        have read are already a few years old,\\nthey mainly use software stacks that
        are less common nowadays.\\nI needed a solution that I can quickly run without
        a large setup.\\nCurrently a lot of code libraries for common languages such
        as Python exist.\\nSo why not reuse an existing one?!\\nThe class <code>AgglomerativeClustering</code>
        from the Python package sklearn implements a hierarchical clustering algorithm.</p>\\n<p>Actually
        I am not interested in different levels of clusters,\\nonly the highest level
        with the least number of clusters.\\nSo I did what a lot of people do nowadays,
        <strong>I had a chat with ChatGPT to brainstorm</strong>\\npossible solutions
        with AgglomerativeClustering.\\nAnd it came up with a smart idea!</p>\\n<p>The
        input for the clustering is a so-called distance matrix:\\nbasically a table
        listing all elements as rows\\nand all elements as columns,\\nwhere the value
        in a cell indicates how similar an element in row X is with the element in
        column Y.\\nWe don\u2019t need a sophisticated similarity measure,\\na simple
        \u201Cyes\u201D for an overlap would suffice.</p>\\n<p>ChatGPTs idea was to
        use negative distance values.\\nLike this, the whole clustering just takes
        two lines of code\\nwhere I instruct the AgglomerativeClustering library to
        return clusters with distance threshold zero.</p>\\n<div><pre><code><span><span><span>from</span>
        <span>sklearn.cluster</span> <span>import</span> <span>AgglomerativeClustering</span>\\n</span></span><span><span>\\n</span></span><span><span><span>model</span>
        <span>=</span> <span>AgglomerativeClustering</span><span>(</span><span>n_clusters</span><span>=</span><span>None</span><span>,</span>
        \\\\\\n</span></span><span><span>                                <span>affinity</span><span>=</span><span>'precomputed'</span><span>,</span>
        \\\\\\n</span></span><span><span>                                <span>linkage</span><span>=</span><span>'single'</span><span>,</span>
        \\\\\\n</span></span><span><span>                                <span>distance_threshold</span><span>=</span><span>0</span><span>)</span>\\n</span></span><span><span>\\n</span></span><span><span><span>#
        get a list where each index </span>\\n</span></span><span><span><span># corresponds
        to an element in elementIDs</span>\\n</span></span><span><span><span># e.g.
        [0,2,0,1] the first element is in cluster 0, </span>\\n</span></span><span><span><span>#
        the second in cluster 2, </span>\\n</span></span><span><span><span># the third
        in cluster 0 and the 4th in cluster 1</span>\\n</span></span><span><span><span>clusterLabels</span>
        <span>=</span> <span>model</span><span>.</span><span>fit_predict</span><span>(</span><span>distanceMatrix</span><span>)</span>\\n</span></span></code></pre></div><p>This
        went well for a relevant subset of roughly 20 thousand records in our BELTRANS
        project.\\nAfter a few minutes we got correct and directly reusable results!\\nHowever,
        when we wanted to achieve the same for more than 150 thousand records,\\nthis
        implementation of the clustering was not performant enough.\\nEven on a powerful
        server with more than 32 GB of RAM,\\nthe computation of the distance matrix
        takes too much time and space.</p>\\n<div>\\n  <div>\\n    One element likely
        has very few or no matches with all the other elements.\\nA sparse matrix
        can be used, where not <em>every</em> value is stored, only not-zero values.\\nUnfortunately
        the <code>AgglomerativeClustering</code> module does not support this.\\n
        \ </div>\\n</div>\\n<h3>Second attempt: implementing the algorithm myself</h3>\\n<p>A
        little disappointed from the last solution,\\nI thought I will try implementing
        it myself,\\nall I need: a few data structures and loops.\\nThe idea of the
        algorithm is to start with every element in its own cluster.\\nThen comparing
        it to other clusters until we find a match.\\nWhen a match is found the loop
        is stopped,\\nthe clusters are merged and we loop again over the (now updated)
        list of clusters.\\nWe do all this until no more merges are possible.</p>\\n<p>This
        algorithm does not need a large distance matrix to start with.\\nTesting if
        there is an overlap between two clusters can be reduced to a single intersection
        operation\\nbetween two Python sets that represent the descriptive keys of
        the respective clusters.</p>\\n<p>Unfortunately all that takes too long.\\nWith
        a few example data I could verify that it worked as expected,\\nbut as soon
        as there are a few thousand elements the algorithm keeps running \u2026</p>\\n<div>\\n
        \ <div>\\n    I tried to remedy the situation by putting parts of the computation\\nin
        a reusable function that I can call in parallel by using\\n<code>ThreadPoolExecutor</code>
        or <code>ProcessPoolExecutor</code>, but it did not help much.\\n  </div>\\n</div>\\n<h3>Third
        attempt: inverted index for clustering in no time</h3>\\n<p>Even more frustrated
        I went back to the drawing board, or pen and paper in my case.\\nWhat I need
        is a solution that ideally only runs once over all elements\\nthat need to
        be clustered.\\nBut how do I find out possible overlaps without comparing
        everything?</p>\\n<p>The disadvantages of the previous solution were\\nthat
        I had to perform set intersections to detect overlaps\\nwhile iterating over
        all clusters,\\nstop the iteration to update the clusters and start iterating
        again.</p>\\n<p>Actually the data already includes the matches between elements
        implicitly,\\nand I used this to compute clusters in no time.\\nEvery element
        has one or more descriptive keys.\\nBut instead of only storing the mapping
        element-&gt;descriptive keys in a Python dictionary,\\nI also store descriptive
        key-&gt;elements in another dictionary.\\n<strong>This is my <em>inverted
        index</em> which I can use to look up overlaps.</strong></p>\\n<p>Eventually
        I start with zero clusters and I simply iterate over the inverted index.\\nIf
        one of the elements from a key already is part of a cluster,\\nI add the other
        elements to it.\\nIf no cluster was found, I create a new one and add the
        elements.\\nBasically all elements of a descriptive key I check,\\neither
        go to an existing cluster or end up in a new cluster.\\nIf different elements
        for a descriptive key are already in different clusters,\\na new merged cluster
        is created and the old ones deleted.</p>\\n<p>This solution works as well,
        and wow, it is fast too.\\nInterestingly <strong>it takes longer to upload
        the computed cluster information to our database\\nthan performing the clustering
        itself.</strong>\\nInverted indexes can do a lot of heavy lifting!</p>\\n<p>You
        can find the source code on GitHub (<a href=\\\"https://zenodo.org/doi/10.5281/zenodo.10011416\\\"
        target=\\\"_blank\\\">DOI: 10.5281/zenodo.10011416</a>).</p>\\n<h3>Why is
        it so fast?</h3>\\n<p>It all has to do with scalability.\\nI only iterate
        <em>once</em> over all elements to build the inverted index dictionary.\\nThen
        I iterate <em>once</em> over all elements in the inverted index to compute
        the clusters.\\nFor 100 input elements this roughly means iterating 200 times,\\nfor
        100,000 input elements roughly 200,000 times.\\nAn algorithm that <strong>scales
        linear to its input</strong>.</p>\\n<p>Compare this to compute a distance
        matrix of 100*100 elements\\n(compute and store 10 thousand results)\\nand
        100,000*100,000 elements (compute and store 10 billion results).\\nAn algorithm
        that <strong>scales quadratic to its input</strong>.</p>\\n<h2>Final remarks</h2>\\n<p>Having
        work-level information brings advantages for both librarians and users.\\nThe
        presented solution is <em>just one possible</em> implementation of <em>one
        possible</em> algorithm.\\nWhich data fields you use to create descriptive
        keys\\nand how you extract them from your bibliographic data explicitly was
        not covered in this post.\\nI also did not cover how you could use the information
        of the clusters to actually achieve the advantages.\\nThe main purpose of
        this blog post was to <strong>introduce and share the generic Python implementation</strong>,\\nbecause
        I had the feeling there is not much out there that can be reused easily.</p>\\n<p>For
        the BELTRANS project we extracted descriptive keys\\nwith a SPARQL query from
        an Resource Description Framework (RDF) representation of bibliographic information.\\nSimilarly,
        we used SPARQL INSERT queries to explicitly create RDF representations\\nfor
        each work cluster and link its manifestations to it using the fabio ontology.\\n<a
        href=\\\"https://github.com/kbrbe/beltrans-data-integration/tree/main/data-integration/sparql-queries/clustering\\\"
        target=\\\"_blank\\\">Here</a> you can have a look at our SPARQL queries.</p>\\n<p>Curious
        about more content like this or some behind-the-scenes?\\nConsider subscribing
        to my bi-weekly Newsletter <em>FAIR Data Digest</em>\\nto receive more interesting
        content about Linked Data every other Tuesday!</p>\\n\",\"doi\":\"https://doi.org/10.59350/4hd4r-1tk44\",\"reference\":[],\"summary\":\"What
        do the books \u201CThe invention of Nature\u201D and \u201CDe uitvinder van
        de natuur\u201D have in common? Well, they are both different versions of
        the same <em>work</em> \u201CThe invention of nature\u201D by Andrea Wulf,
        whether it is in a different format or a different language. In this blog
        post I will briefly introduce the advantages of keeping work-level records
        in library catalogs.\",\"tags\":[],\"title\":\"Clustering Book editions\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Henry
        Rzepa\",\"url\":\"https://orcid.org/0000-0002-8635-8390\"}],\"blog_id\":\"jqdyv23\",\"blog_name\":\"Henry
        Rzepa's Blog\",\"blog_slug\":\"rzepa\",\"content_html\":\"<div>\\n<p>In an
        earlier post on this topic,<span><a href=\\\"#ITEM-26523-0\\\">[1]</a></span><sup>\u2021</sup>
        I described how the curly-arrows describing the mechanism of a nucleophilic
        addition at a carbonyl group choreograph in two distinct ways, as seen in
        red or blue below. The arrows in red can be described as firstly addition
        to the carbonyl group to form either a transient intermediate (a two-step
        process) or instead a formal transition state state as a concerted single-step
        mechanism. The blue arrows do the reverse; firstly elimination and then followed
        by addition. I will use the shorthand <strong>AE</strong> for the first type
        and <strong>EA</strong> for the second type. Here I explore some more nucleophiles
        to see which of these two mechanisms they follow. Data for these results can
        be found at <a href=\\\"https://doi.org/10.14469/hpc/13171\\\">10.14469/hpc/13171</a><br
        />\\n<a href=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/double-headed4.svg\\\"><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/double-headed4.svg\\\"
        width=\\\"450\\\" /></a><strong>N- carbon ylid:</strong> This is a very facile
        (low-barrier) reaction with a C-O bond length response that initially increases
        steeply, followed by a more modest decline and hence corresponds to an <strong>AE</strong>
        mechanism.</p>\\n<p></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-C-Nuc_tot_ener.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-C-Nuc_tot_ener.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-add-C-nuc-DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-C-Nuc-BL.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><strong>P carbon-Ylid:</strong>\_ Essentially
        identical to the previous example, and again an <strong>AE</strong> mechanism.</p>\\n<p><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><a href=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_tot_ener.svg\\\"><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_tot_ener.svg\\\"
        width=\\\"450\\\" /></a></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_BL12.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><strong>S carbon-ylid: </strong>Again, an AE
        mechanism.</p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid_tot_ener.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid_DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid_BL12.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><strong>S-nucleophile:\_</strong> This one is
        different, showing a larger barrier and initial small decrease in the C-O
        length followed by a larger increase. This one is an <strong>EA</strong> mechanism.</p>\\n<p><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_tot_ener.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_tot_ener.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_BL12.svg\\\"
        width=\\\"450\\\" /></p>\\n<p>As I noted previously, it would be useful to
        have two double headed curly arrows available in palletes of these; <strong>&lt;\u2014&gt;</strong>
        (AE) and <strong>&gt;\u2014&lt;</strong> (EA) to illustrate the difference
        between the two mechanistic types.</p>\\n<hr />\\n<p><sup>\u2021</sup>This
        is the first instance where I cite a blog using a CrossRef DOI generated for
        it. Previous such citations used a DataCite DOI, which the bibliographic software
        used here to add them to the post (Kcite) does not support.</p>\\n<hr />\\n<div><a
        href=\\\"https://orcid.org/0000-0002-8635-8390\\\" target=\\\"_blank\\\">https://orcid.org/0000-0002-8635-8390</a></div><h2>References</h2>\\n
        \   <ol>\\n    <li><a name=\\\"ITEM-26523-0\\\"></a>\\nH. Rzepa, \\\"The \u201Cdouble-headed\u201D
        curly arrow as used in mechanistic representations.\\\", 2023. <a href=\\\"http://dx.doi.org/10.59350/f00wf-5tq46\\\">http://dx.doi.org/10.59350/f00wf-5tq46</a>\\n\\n\\n</li>\\n</ol>\\n\\n</div>
        \",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/h7vdj-cky15\",\"id\":\"de10c9d9-2c7c-421f-ada4-a96decf11b5d\",\"image\":\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/double-headed4.svg\",\"language\":\"en\",\"published_at\":1697111844,\"reference\":[{\"doi\":\"http://doi.org/10.59350/f00wf-5tq46\",\"key\":\"ref1\"}],\"relationships\":[],\"summary\":\"In
        an earlier post on this topic,[1]<sup>\u2021</sup> I described how the curly-arrows
        describing the mechanism of a nucleophilic addition at a carbonyl group choreograph
        in two distinct ways, as seen in red or blue below. The arrows in red can
        be described as firstly addition to the carbonyl group to form either a transient
        intermediate (a two-step process) or instead a formal transition state state
        as a concerted single-step mechanism.\",\"tags\":[\"Interesting Chemistry\"],\"title\":\"More
        examples of &#8220;double-headed&#8221; curly arrows: S and C Nucleophiles
        attacking acetyl chloride\",\"updated_at\":1697112211,\"url\":\"https://www.ch.imperial.ac.uk/rzepa/blog?p=26523\"},\"highlight\":{\"authors\":[{\"name\":\"Henry
        Rzepa\",\"url\":\"https://orcid.org/0000-0002-8635-8390\"}],\"content_html\":\"<div>\\n<p>In
        an earlier post on this topic,<span><a href=\\\"#ITEM-26523-0\\\">[1]</a></span><sup>\u2021</sup>
        I described how the curly-arrows describing the mechanism of a nucleophilic
        addition at a carbonyl group choreograph in two distinct ways, as seen in
        red or blue below. The arrows in red can be described as firstly addition
        to the carbonyl group to form either a transient intermediate (a two-step
        process) or instead a formal transition state state as a concerted single-step
        mechanism. The blue arrows do the reverse; firstly elimination and then followed
        by addition. I will use the shorthand <strong>AE</strong> for the first type
        and <strong>EA</strong> for the second type. Here I explore some more nucleophiles
        to see which of these two mechanisms they follow. Data for these results can
        be found at <a href=\\\"https://doi.org/10.14469/hpc/13171\\\">10.14469/hpc/13171</a><br
        />\\n<a href=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/double-headed4.svg\\\"><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/double-headed4.svg\\\"
        width=\\\"450\\\" /></a><strong>N- carbon ylid:</strong> This is a very facile
        (low-barrier) reaction with a C-O bond length response that initially increases
        steeply, followed by a more modest decline and hence corresponds to an <strong>AE</strong>
        mechanism.</p>\\n<p></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-C-Nuc_tot_ener.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-C-Nuc_tot_ener.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-add-C-nuc-DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chlorine-C-Nuc-BL.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><strong>P carbon-Ylid:</strong>\_ Essentially
        identical to the previous example, and again an <strong>AE</strong> mechanism.</p>\\n<p><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><a href=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_tot_ener.svg\\\"><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_tot_ener.svg\\\"
        width=\\\"450\\\" /></a></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-P-ylid_BL12.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><strong>S carbon-ylid: </strong>Again, an AE
        mechanism.</p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid_tot_ener.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid_DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-S-ylid_BL12.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><strong>S-nucleophile:\_</strong> This one is
        different, showing a larger barrier and initial small decrease in the C-O
        length followed by a larger increase. This one is an <strong>EA</strong> mechanism.</p>\\n<p><img
        src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_tot_ener.gif\\\"
        width=\\\"540\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_tot_ener.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_DM.svg\\\"
        width=\\\"450\\\" /></p>\\n<p><img src=\\\"https://www.ch.ic.ac.uk/rzepa/blog/wp-content/uploads/2023/10/acetyl-chloride-MeSH_BL12.svg\\\"
        width=\\\"450\\\" /></p>\\n<p>As I noted previously, it would be useful to
        have two double headed curly arrows available in palletes of these; <strong>&lt;\u2014&gt;</strong>
        (AE) and <strong>&gt;\u2014&lt;</strong> (EA) to illustrate the difference
        between the two mechanistic types.</p>\\n<hr />\\n<p><sup>\u2021</sup>This
        is the first instance where I cite a blog using a CrossRef DOI generated for
        it. Previous such citations used a DataCite DOI, which the bibliographic software
        used here to add them to the post (Kcite) does not support.</p>\\n<hr />\\n<div><a
        href=\\\"https://orcid.org/0000-0002-8635-8390\\\" target=\\\"_blank\\\">https://orcid.org/0000-0002-8635-8390</a></div><h2>References</h2>\\n
        \   <ol>\\n    <li><a name=\\\"ITEM-26523-0\\\"></a>\\nH. Rzepa, \\\"The \u201Cdouble-headed\u201D
        curly arrow as used in mechanistic representations.\\\", 2023. <a href=\\\"http://dx.doi.org/10.59350/f00wf-5tq46\\\">http://dx.doi.org/10.59350/f00wf-5tq46</a>\\n\\n\\n</li>\\n</ol>\\n\\n</div>
        \",\"doi\":\"https://doi.org/10.59350/h7vdj-cky15\",\"reference\":[{\"doi\":\"http://doi.org/10.59350/f00wf-5tq46\",\"key\":\"ref1\"}],\"summary\":\"In
        an earlier post on this topic,[1]<sup>\u2021</sup> I described how the curly-arrows
        describing the mechanism of a nucleophilic addition at a carbonyl group choreograph
        in two distinct ways, as seen in red or blue below. The arrows in red can
        be described as firstly addition to the carbonyl group to form either a transient
        intermediate (a two-step process) or instead a formal transition state state
        as a concerted single-step mechanism.\",\"tags\":[\"Interesting Chemistry\"],\"title\":\"More
        examples of &#8220;double-headed&#8221; curly arrows: S and C Nucleophiles
        attacking acetyl chloride\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Matt
        Buys\",\"url\":\"https://orcid.org/0000-0001-7234-3684\"},{\"name\":\"Ginny
        Hendricks\",\"url\":\"https://orcid.org/0000-0002-0353-2702\"}],\"blog_id\":\"pm0p222\",\"blog_name\":\"Upstream\",\"blog_slug\":\"upstream\",\"content_html\":\"<p>Persistent
        Identifiers, or \u201CPIDs\u201D, have become a popular topic for anyone involved
        in the research communications world. PIDs play a crucial role in the scholarly
        ecosystem by providing long-lasting references to and between all kinds of
        digital resources. PIDs usually consist of a unique identifier and a service
        that resolves resource references over time. While various types of PIDs are
        available, the Digital Object Identifier (DOI) is perhaps the most recognized
        in scholarly communities and has a particular focus on the \u2018P\u2019:
        Persistence. As organisations explore the world of PIDs and try to navigate
        what is increasingly becoming a subjective landscape with unintentional obfuscation,
        it becomes evident that the choice of identifier infrastructure can significantly
        impact global inclusivity and accessibility, especially for lower-income countries.</p><p>In
        this post, we present the facts about DataCite and Crossref DOIs and their
        infrastructures, their use, their growth, their governance, their transparency,
        and their cost\u2014so that organisations that care about the rigour of the
        scholarly record can make more informed decisions. By doing so, we dispel
        some misconceptions that may risk damaging a healthy open research ecosystem
        for future generations.</p><h2 id=\\\"datacite-and-crossref-dois-can-lower-financial-barriers\\\">DataCite
        and Crossref DOIs Can Lower Financial Barriers</h2><p>One of the key advantages
        of DataCite and Crossref DOIs is their cost-effectiveness. Unlike other identifier
        systems that require substantial development resources to implement and maintain,
        DOIs offer a more budget-friendly alternative. Hidden costs associated with
        implementing some other PIDs, such as development and maintenance expenses,
        can often outweigh the perceived upfront savings, posing financial barriers
        to their long-term sustainability. These lower costs can make a significant
        difference for many organizations, particularly those in lower-income countries.
        It's crucial to recognize that DataCite and Crossref DOIs are not merely identifiers
        but valuable metadata resources for downstream services, fostering economies
        of scale within the entire research communications ecosystem. This collective
        and reciprocal network ultimately supports broader dissemination, reuse, and
        recognition of outputs and resources on a global scale.</p><p>Together, the
        nine scholarly-related DOI Registration Agencies have enabled the community
        to create and manage 300 million DOI records. These DOIs are used approximately
        1.3 billion times every month - and growing. With an estimated 8.8 million
        researchers worldwide, that\u2019s the equivalent of one article, dataset,
        preprint, report, sample or other output being read or used 150 times a month.
        The users are not just individuals but also the systems and tools that are
        baked into research and academia. These open, free-to-use, mineable DOI records
        are incorporated into platforms that span academia, government, and industry.
        </p><figure class=\\\"kg-card kg-image-card kg-card-hascaption\\\"><img src=\\\"https://lh3.googleusercontent.com/pajGdj7Bw4P0xqEC2spRlw-fRVL2hYBIrtqhBi-9mVSGZq7Ag-Rs0_ZTd9A_JL38PeBd2AsKBQ2sOGRs7ra1OPQs2YeTobdBBXn19ERGPNSFI-rd0Tk_iLWDQmzwFQMQgX4hUdImGo986z_VTNMppNw\\\"
        class=\\\"kg-image\\\" alt=\\\"CHart showing growth in DOI usage (resolutions)
        over time, topping at almost 1.4 billion in a month\\\" loading=\\\"lazy\\\"
        width=\\\"624\\\" height=\\\"420\\\"><figcaption>Source <a href=\\\"https://www.doi.org/the-identifier/what-is-a-doi/#doi-resolutions\\\">doi.org/the-identifier/what-is-a-doi/#doi-resolutions</a>,
        retrieved 2023-Oct-02</figcaption></figure><p>Recognising the disparity in
        trying to run a \u201Cglobal\u201D system and meet the needs of the very wealthy
        as well as the least wealthy, both organisations have long-run programs that
        encourage all low-income countries to participate, offering education, support,
        and outreach alongside fee waivers and support programmes such as collective
        savings like consortia and sponsorship. </p><h2 id=\\\"datacite-and-crossref-work-globally-acknowledging-there-is-more-to-be-done\\\">DataCite
        and Crossref Work Globally, Acknowledging There Is More To Be Done</h2><p>DataCite
        is a worldwide community with members in 52 countries, as well as over 2,950
        repositories in many other countries across the globe. It's important to note
        that repositories are not required to be members themselves; instead, they
        can be affiliated with a DataCite Consortium Organization or Member. Moreover,
        there are 20 national or regional consortia operating in emerging regions.
        DataCite's open infrastructure services are a valuable resource for repositories
        in emerging regions, including the <em>National Academic Digital Repository
        (Ethiopia), International Institute of Tropical Agriculture, IITA (Nigeria),
        Universit\xE9 Gaston Berger (Senegal), Corporacion Ecuatoriana Para El Desarrollo
        De La Investigacion Y La Academia (CEDIA) (Ecuador), NRCT Data Center (Thailand),
        International Centre for Integrated Mountain Development Regional Database
        System (Nepal),</em> and countless others across the globe.</p><p>Crossref
        has members in 151 countries, creating and stewarding DOI records. Crossref\u2019s
        research of ISSN data shows that it already includes over 50% of the journals
        produced in the following low-income countries: <em>Bhutan</em>,<em> Central
        African Republic</em>,<em> Kyrgyzstan</em>,<em> Mali</em>,<em> Sudan</em>,<em>
        </em>and<em> Tajikistan</em>. And it is actively working towards filling in
        the gaps where it covers fewer than 49% of the journals published in the countries
        where research is also increasingly produced: <em>Bangladesh</em>, <em>Cote
        d'Ivoire</em>, <em>Ethiopia</em>, <em>Ghana</em>, <em>Madagascar</em>, <em>Nepal</em>,
        <em>Nicaragua</em>, <em>Senegal</em>, and <em>Tanzania</em> \u2014 among others.</p><p>DataCite\u2019s
        <a href=\\\"https://datacite.org/global-access-program/\\\">GAP Program</a>
        is an ongoing initiative to improve access and enable communities in lesser-represented
        regions to further benefit from DataCite's open infrastructure services. Crossref\u2019s
        <a href=\\\"https://www.crossref.org/gem/\\\">GEM Program</a> has seen a tripling
        of members in some of the lowest-income countries in the world from 100 to
        305 from January to September 2023. The two organisations, with other partners,
        are working on co-creating a guide to consider how academic and government
        bodies might collaborate at a country level to get involved with foundational
        global open infrastructures in a sustainable way.</p><p>Of course, more can
        definitely be done, and practical and informed ideas are always very welcome.
        </p><h2 id=\\\"datacite-and-crossref-dois-are-community-governed-infrastructure\\\">DataCite
        and Crossref DOIs Are Community-Governed Infrastructure</h2><p>Responsible
        open infrastructure organizations do not operate as monopolies. Instead, they
        emphasize collective community governance and ownership. Most DOI Registration
        Agencies (RAs) exemplify this approach; they are predominantly not-for-profit
        and offer participatory models that extend beyond membership structures. Organizations
        can join these agencies at no charge, work through consortia or sponsors,
        or opt for annual membership fees, ensuring that financial constraints do
        not hinder accessibility. This community-driven governance model safeguards
        against monopolization and ensures that PIDs remain accessible to diverse
        stakeholders. Further, providing open data and open source code bases further
        safeguards the community investment in open infrastructure services.</p><p>Both
        DataCite and Crossref have tools that allow each member to manage their resources
        in the way that they want. Provenance metadata is a high priority so that
        everyone can see who is asserted as the steward of a research object, whether
        that's changed, who and how much is paid to maintain the record, and any other
        contributors and acknowledgements.</p><p>Every member gets a say in the governance,
        policies, fees, and key decisions that are made. By voting in or standing
        for board elections, joining fee committees or consultations, and approving
        budgets through open governance,<strong> </strong>thousands of institutions
        around the world are effecting change to make the whole system properly broadly
        governed and more and more inclusive as every month and year passes.</p><h2
        id=\\\"datacite-and-crossref-dois-are-persistent-and-openly-available\\\">DataCite
        and Crossref DOIs are Persistent and Openly Available</h2><p>Another advantage
        of Crossref and DataCite DOIs is their persistence. Once registered, the associated
        metadata is openly available without the need for further financial or resource
        commitments. For an initial one-time registration fee, each DOI record is
        maintained for free - forever. No registration fees are levied for the numerous
        updates and additional metadata added, which will continue to add and enrich
        the scholarly record for and with future generations. This is a crucial differentiator
        compared to some local identifiers, which may require ongoing investments
        to maintain accessibility. The stability and longevity of DOIs make them a
        reliable choice for organizations seeking to ensure the enduring accessibility
        of their digital resources.</p><h2 id=\\\"datacite-and-crossref-dois-come-with-infrastructure-support-that-isn%E2%80%99t-free\\\">DataCite
        and Crossref DOIs come with infrastructure support that isn\u2019t free</h2><p>The
        DOI itself is only one component of our activities; what can be done over
        and above a mere PID tells a very different\u2014and fuller\u2014story. A
        DOI is not just an identifier but a link, a locator, but even then, it's almost
        as useless as a URL; the numerous community-led initiatives that have extended
        and built upon this identifier have real community value. It's the metadata,
        the relationships, and the connection with other parts of the digital infrastructure.
        These initiatives take time, resources, volunteers, tools, consultation, and
        sometimes funding. Also, expertise and experience. Examples are:</p><ul><li>Building
        an open data citation corpus</li><li>Co-creating and advising on FAIR projects</li><li>Developing
        infrastructure to connect clinical trials to outputs</li><li>Urgent flagging
        of \u2018free-to-read\u2019 COVID content</li><li>Connecting funding and funders
        with outputs</li><li>Founding and contributing to ORCID and ROR and countless
        other initiatives</li><li>Text-based plagiarism checking and other research
        integrity tools</li><li>Ability to track retracted, withdrawn, or corrected
        outputs</li><li>Co-developing and supporting open-source tool development</li><li>Creating
        a public resource by opening critical data about retracted papers</li></ul><p>It\u2019s
        surprising to occasionally hear the argument that DOIs should be free to create
        when the extensive support systems and initiatives developed by Crossref and
        DataCite are not free to run (nor are <em>distributed</em> systems, for that
        matter). The cost of supporting a functioning global infrastructure should
        not be underestimated. If the community wants a persistent and robust scholarly
        record, and if it wants a say in its governance, then it should be aware of
        the cost not just of technical things like resolver systems, data storage
        and APIs - but also of community engagement, collaboration, and technical
        support. </p><p>In 2024, Crossref will employ ten full-time staff and six
        contractors on membership support alone (that\u2019s not including proactive
        outreach or engagement activities), plus the cost of the tools and systems
        needed to manage their 19,000 members and the 3500+ support requests the team
        receives each month. In 2024, Crossref is projecting an outlay of up to 1
        million USD for data costs with physical and cloud storage and processing.</p><p>Similarly,
        DataCite has three full-time staff dedicated to supporting and partnering
        with emerging regions and three full-time staff members who support technical
        community support and best practice development. Particular emphasis lies
        in the coordination with more than 50 national or regional consortia to cater
        to their unique needs. With over 2,950 repositories globally and hundreds
        of new repositories joining the collective community effort each year, our
        efforts are directed towards fostering an open, global, interconnected ecosystem.</p><p>DataCite
        and Crossref also support and develop other PID services - both ORCID iDs
        and ROR IDs have been or are supported operationally, financially, or both.
        These evolved from community collaborations and continue thriving as essential
        open infrastructures interacting with DOIs.</p><p>We know that collectively,
        we benefit from economies of scale and can reduce costs as a community when
        we cooperate globally. It would be far more costly for the community to replicate
        Crossref and DataCite infrastructures at the national level, let alone at
        the level of each research-performing or research-publishing organisation.
        </p><p>When drawing an analogy to regional electric outlet systems, it becomes
        apparent that the issue of interoperability presents significant challenges.</p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://upstream.force11.org/content/images/2023/10/plugs-1.png\\\"
        class=\\\"kg-image\\\" alt=\\\"Image of multiple plugs used around the world
        with caption &quot;The effect of developing regional infrastructures&quot;\\\"
        loading=\\\"lazy\\\" width=\\\"2000\\\" height=\\\"1169\\\" srcset=\\\"https://upstream.force11.org/content/images/size/w600/2023/10/plugs-1.png
        600w, https://upstream.force11.org/content/images/size/w1000/2023/10/plugs-1.png
        1000w, https://upstream.force11.org/content/images/size/w1600/2023/10/plugs-1.png
        1600w, https://upstream.force11.org/content/images/size/w2400/2023/10/plugs-1.png
        2400w\\\" sizes=\\\"(min-width: 720px) 720px\\\"></figure><p>In the context
        of persistent identifiers in research communications, adhering to universal
        design principles may, on the surface, seem to address interoperability issues.
        However, there is a potential risk of exacerbating fragmentation within the
        ecosystem, thereby disadvantaging emerging regions. Rather, we should advocate
        for collaborative\u2014and global\u2014initiatives aimed at preserving open
        infrastructure under responsible governance that avoids further fragmentation.</p><h2
        id=\\\"collaboration-and-gathering-context\\\">Collaboration and Gathering
        Context</h2><p>In discussing the merits of various identifier systems, gathering
        insights and context from stakeholders with practical experience in implementing
        both is vital. Hypothetical arguments should be avoided, as DataCite and Crossref
        actively collaborate with communities to address real-life challenges by enhancing
        their services. </p><p>We recommend that people explore the facts. Ask questions
        such as \u201C<em>what is the expected resource requirement for us now and
        in the future?</em>\u201D, \u201C<em>who is on the board?</em>\u201D and \u201C<em>how
        can I get on the board?</em>\u201D, \u201C<em>how open is the metadata, i.e.
        is it both available and accessible?</em>\u201D. \u201C<em>What is the likelihood
        of this going away?</em>\u201D and \u201C<em>What safeguards are there for
        long-term preservation?</em>\u201D. Find out what training and support is
        offered. Talk to your local community that speaks your language, and ask the
        members in 151 countries using DataCite and/or Crossref, e.g., an Ambassador,
        Consortia Lead, or Sponsor. </p><p>In conclusion, DOIs through DataCite and
        Crossref represent a cost-effective, community-governed, and persistent solution
        for identifying and referencing digital resources. By embracing DataCite and
        Crossref DOIs, the global scholarly community is empowered, reducing financial
        barriers and fostering broad creation, dissemination, and recognition of research
        outputs and resources.</p><p>DataCite and Crossref are actively investing
        in working on community collaboration, making it essential for stakeholders
        to engage in constructive dialogues that contribute to the ongoing improvement
        of research services. If you tell us more specifics about your needs and experience,
        we\u2019ll tell you more about the reality of running infrastructure, and
        we\u2019ll figure something out to help you get involved. Together, we can
        build a more equitable and accessible scholarly landscape for all communities
        and all countries.</p><p>Visit <a href=\\\"https://www.datacite.org\\\">DataCite</a>
        and <a href=\\\"https://www.crossref.org\\\">Crossref</a> websites for more
        information.</p><p><strong><em>Thanks to the following people for reviewing
        and adding context to this post: Britta Dreyer, Ed Pentz, Helena Cousijn,
        and John Chodacki.</em></strong></p>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.54900/6sz4q-47185\",\"id\":\"24364\",\"image\":\"https://upstream.force11.org/content/images/2023/10/pexels-monstera-production-7412043.jpg\",\"language\":\"en\",\"published_at\":1697038421,\"reference\":[],\"relationships\":[],\"summary\":\"Persistent
        Identifiers, or \u201CPIDs\u201D, have become a popular topic for anyone involved
        in the research communications world. PIDs play a crucial role in the scholarly
        ecosystem by providing long-lasting references to and between all kinds of
        digital resources.\",\"tags\":[\"Thought Pieces\"],\"title\":\"Working for
        Global Equity through Digital Object Identifiers\",\"updated_at\":1697449360,\"url\":\"https://upstream.force11.org/working-for-global-equity-through-digital-object-identifiers\",\"uuid\":\"658f301f-1e22-4b45-8b69-6c5414745001\"},\"highlight\":{\"authors\":[{\"name\":\"Matt
        Buys\",\"url\":\"https://orcid.org/0000-0001-7234-3684\"},{\"name\":\"Ginny
        Hendricks\",\"url\":\"https://orcid.org/0000-0002-0353-2702\"}],\"content_html\":\"<p>Persistent
        Identifiers, or \u201CPIDs\u201D, have become a popular topic for anyone involved
        in the research communications world. PIDs play a crucial role in the scholarly
        ecosystem by providing long-lasting references to and between all kinds of
        digital resources. PIDs usually consist of a unique identifier and a service
        that resolves resource references over time. While various types of PIDs are
        available, the Digital Object Identifier (DOI) is perhaps the most recognized
        in scholarly communities and has a particular focus on the \u2018P\u2019:
        Persistence. As organisations explore the world of PIDs and try to navigate
        what is increasingly becoming a subjective landscape with unintentional obfuscation,
        it becomes evident that the choice of identifier infrastructure can significantly
        impact global inclusivity and accessibility, especially for lower-income countries.</p><p>In
        this post, we present the facts about DataCite and Crossref DOIs and their
        infrastructures, their use, their growth, their governance, their transparency,
        and their cost\u2014so that organisations that care about the rigour of the
        scholarly record can make more informed decisions. By doing so, we dispel
        some misconceptions that may risk damaging a healthy open research ecosystem
        for future generations.</p><h2 id=\\\"datacite-and-crossref-dois-can-lower-financial-barriers\\\">DataCite
        and Crossref DOIs Can Lower Financial Barriers</h2><p>One of the key advantages
        of DataCite and Crossref DOIs is their cost-effectiveness. Unlike other identifier
        systems that require substantial development resources to implement and maintain,
        DOIs offer a more budget-friendly alternative. Hidden costs associated with
        implementing some other PIDs, such as development and maintenance expenses,
        can often outweigh the perceived upfront savings, posing financial barriers
        to their long-term sustainability. These lower costs can make a significant
        difference for many organizations, particularly those in lower-income countries.
        It's crucial to recognize that DataCite and Crossref DOIs are not merely identifiers
        but valuable metadata resources for downstream services, fostering economies
        of scale within the entire research communications ecosystem. This collective
        and reciprocal network ultimately supports broader dissemination, reuse, and
        recognition of outputs and resources on a global scale.</p><p>Together, the
        nine scholarly-related DOI Registration Agencies have enabled the community
        to create and manage 300 million DOI records. These DOIs are used approximately
        1.3 billion times every month - and growing. With an estimated 8.8 million
        researchers worldwide, that\u2019s the equivalent of one article, dataset,
        preprint, report, sample or other output being read or used 150 times a month.
        The users are not just individuals but also the systems and tools that are
        baked into research and academia. These open, free-to-use, mineable DOI records
        are incorporated into platforms that span academia, government, and industry.
        </p><figure class=\\\"kg-card kg-image-card kg-card-hascaption\\\"><img src=\\\"https://lh3.googleusercontent.com/pajGdj7Bw4P0xqEC2spRlw-fRVL2hYBIrtqhBi-9mVSGZq7Ag-Rs0_ZTd9A_JL38PeBd2AsKBQ2sOGRs7ra1OPQs2YeTobdBBXn19ERGPNSFI-rd0Tk_iLWDQmzwFQMQgX4hUdImGo986z_VTNMppNw\\\"
        class=\\\"kg-image\\\" alt=\\\"CHart showing growth in DOI usage (resolutions)
        over time, topping at almost 1.4 billion in a month\\\" loading=\\\"lazy\\\"
        width=\\\"624\\\" height=\\\"420\\\"><figcaption>Source <a href=\\\"https://www.doi.org/the-identifier/what-is-a-doi/#doi-resolutions\\\">doi.org/the-identifier/what-is-a-doi/#doi-resolutions</a>,
        retrieved 2023-Oct-02</figcaption></figure><p>Recognising the disparity in
        trying to run a \u201Cglobal\u201D system and meet the needs of the very wealthy
        as well as the least wealthy, both organisations have long-run programs that
        encourage all low-income countries to participate, offering education, support,
        and outreach alongside fee waivers and support programmes such as collective
        savings like consortia and sponsorship. </p><h2 id=\\\"datacite-and-crossref-work-globally-acknowledging-there-is-more-to-be-done\\\">DataCite
        and Crossref Work Globally, Acknowledging There Is More To Be Done</h2><p>DataCite
        is a worldwide community with members in 52 countries, as well as over 2,950
        repositories in many other countries across the globe. It's important to note
        that repositories are not required to be members themselves; instead, they
        can be affiliated with a DataCite Consortium Organization or Member. Moreover,
        there are 20 national or regional consortia operating in emerging regions.
        DataCite's open infrastructure services are a valuable resource for repositories
        in emerging regions, including the <em>National Academic Digital Repository
        (Ethiopia), International Institute of Tropical Agriculture, IITA (Nigeria),
        Universit\xE9 Gaston Berger (Senegal), Corporacion Ecuatoriana Para El Desarrollo
        De La Investigacion Y La Academia (CEDIA) (Ecuador), NRCT Data Center (Thailand),
        International Centre for Integrated Mountain Development Regional Database
        System (Nepal),</em> and countless others across the globe.</p><p>Crossref
        has members in 151 countries, creating and stewarding DOI records. Crossref\u2019s
        research of ISSN data shows that it already includes over 50% of the journals
        produced in the following low-income countries: <em>Bhutan</em>,<em> Central
        African Republic</em>,<em> Kyrgyzstan</em>,<em> Mali</em>,<em> Sudan</em>,<em>
        </em>and<em> Tajikistan</em>. And it is actively working towards filling in
        the gaps where it covers fewer than 49% of the journals published in the countries
        where research is also increasingly produced: <em>Bangladesh</em>, <em>Cote
        d'Ivoire</em>, <em>Ethiopia</em>, <em>Ghana</em>, <em>Madagascar</em>, <em>Nepal</em>,
        <em>Nicaragua</em>, <em>Senegal</em>, and <em>Tanzania</em> \u2014 among others.</p><p>DataCite\u2019s
        <a href=\\\"https://datacite.org/global-access-program/\\\">GAP Program</a>
        is an ongoing initiative to improve access and enable communities in lesser-represented
        regions to further benefit from DataCite's open infrastructure services. Crossref\u2019s
        <a href=\\\"https://www.crossref.org/gem/\\\">GEM Program</a> has seen a tripling
        of members in some of the lowest-income countries in the world from 100 to
        305 from January to September 2023. The two organisations, with other partners,
        are working on co-creating a guide to consider how academic and government
        bodies might collaborate at a country level to get involved with foundational
        global open infrastructures in a sustainable way.</p><p>Of course, more can
        definitely be done, and practical and informed ideas are always very welcome.
        </p><h2 id=\\\"datacite-and-crossref-dois-are-community-governed-infrastructure\\\">DataCite
        and Crossref DOIs Are Community-Governed Infrastructure</h2><p>Responsible
        open infrastructure organizations do not operate as monopolies. Instead, they
        emphasize collective community governance and ownership. Most DOI Registration
        Agencies (RAs) exemplify this approach; they are predominantly not-for-profit
        and offer participatory models that extend beyond membership structures. Organizations
        can join these agencies at no charge, work through consortia or sponsors,
        or opt for annual membership fees, ensuring that financial constraints do
        not hinder accessibility. This community-driven governance model safeguards
        against monopolization and ensures that PIDs remain accessible to diverse
        stakeholders. Further, providing open data and open source code bases further
        safeguards the community investment in open infrastructure services.</p><p>Both
        DataCite and Crossref have tools that allow each member to manage their resources
        in the way that they want. Provenance metadata is a high priority so that
        everyone can see who is asserted as the steward of a research object, whether
        that's changed, who and how much is paid to maintain the record, and any other
        contributors and acknowledgements.</p><p>Every member gets a say in the governance,
        policies, fees, and key decisions that are made. By voting in or standing
        for board elections, joining fee committees or consultations, and approving
        budgets through open governance,<strong> </strong>thousands of institutions
        around the world are effecting change to make the whole system properly broadly
        governed and more and more inclusive as every month and year passes.</p><h2
        id=\\\"datacite-and-crossref-dois-are-persistent-and-openly-available\\\">DataCite
        and Crossref DOIs are Persistent and Openly Available</h2><p>Another advantage
        of Crossref and DataCite DOIs is their persistence. Once registered, the associated
        metadata is openly available without the need for further financial or resource
        commitments. For an initial one-time registration fee, each DOI record is
        maintained for free - forever. No registration fees are levied for the numerous
        updates and additional metadata added, which will continue to add and enrich
        the scholarly record for and with future generations. This is a crucial differentiator
        compared to some local identifiers, which may require ongoing investments
        to maintain accessibility. The stability and longevity of DOIs make them a
        reliable choice for organizations seeking to ensure the enduring accessibility
        of their digital resources.</p><h2 id=\\\"datacite-and-crossref-dois-come-with-infrastructure-support-that-isn%E2%80%99t-free\\\">DataCite
        and Crossref DOIs come with infrastructure support that isn\u2019t free</h2><p>The
        DOI itself is only one component of our activities; what can be done over
        and above a mere PID tells a very different\u2014and fuller\u2014story. A
        DOI is not just an identifier but a link, a locator, but even then, it's almost
        as useless as a URL; the numerous community-led initiatives that have extended
        and built upon this identifier have real community value. It's the metadata,
        the relationships, and the connection with other parts of the digital infrastructure.
        These initiatives take time, resources, volunteers, tools, consultation, and
        sometimes funding. Also, expertise and experience. Examples are:</p><ul><li>Building
        an open data citation corpus</li><li>Co-creating and advising on FAIR projects</li><li>Developing
        infrastructure to connect clinical trials to outputs</li><li>Urgent flagging
        of \u2018free-to-read\u2019 COVID content</li><li>Connecting funding and funders
        with outputs</li><li>Founding and contributing to ORCID and ROR and countless
        other initiatives</li><li>Text-based plagiarism checking and other research
        integrity tools</li><li>Ability to track retracted, withdrawn, or corrected
        outputs</li><li>Co-developing and supporting open-source tool development</li><li>Creating
        a public resource by opening critical data about retracted papers</li></ul><p>It\u2019s
        surprising to occasionally hear the argument that DOIs should be free to create
        when the extensive support systems and initiatives developed by Crossref and
        DataCite are not free to run (nor are <em>distributed</em> systems, for that
        matter). The cost of supporting a functioning global infrastructure should
        not be underestimated. If the community wants a persistent and robust scholarly
        record, and if it wants a say in its governance, then it should be aware of
        the cost not just of technical things like resolver systems, data storage
        and APIs - but also of community engagement, collaboration, and technical
        support. </p><p>In 2024, Crossref will employ ten full-time staff and six
        contractors on membership support alone (that\u2019s not including proactive
        outreach or engagement activities), plus the cost of the tools and systems
        needed to manage their 19,000 members and the 3500+ support requests the team
        receives each month. In 2024, Crossref is projecting an outlay of up to 1
        million USD for data costs with physical and cloud storage and processing.</p><p>Similarly,
        DataCite has three full-time staff dedicated to supporting and partnering
        with emerging regions and three full-time staff members who support technical
        community support and best practice development. Particular emphasis lies
        in the coordination with more than 50 national or regional consortia to cater
        to their unique needs. With over 2,950 repositories globally and hundreds
        of new repositories joining the collective community effort each year, our
        efforts are directed towards fostering an open, global, interconnected ecosystem.</p><p>DataCite
        and Crossref also support and develop other PID services - both ORCID iDs
        and ROR IDs have been or are supported operationally, financially, or both.
        These evolved from community collaborations and continue thriving as essential
        open infrastructures interacting with DOIs.</p><p>We know that collectively,
        we benefit from economies of scale and can reduce costs as a community when
        we cooperate globally. It would be far more costly for the community to replicate
        Crossref and DataCite infrastructures at the national level, let alone at
        the level of each research-performing or research-publishing organisation.
        </p><p>When drawing an analogy to regional electric outlet systems, it becomes
        apparent that the issue of interoperability presents significant challenges.</p><figure
        class=\\\"kg-card kg-image-card\\\"><img src=\\\"https://upstream.force11.org/content/images/2023/10/plugs-1.png\\\"
        class=\\\"kg-image\\\" alt=\\\"Image of multiple plugs used around the world
        with caption &quot;The effect of developing regional infrastructures&quot;\\\"
        loading=\\\"lazy\\\" width=\\\"2000\\\" height=\\\"1169\\\" srcset=\\\"https://upstream.force11.org/content/images/size/w600/2023/10/plugs-1.png
        600w, https://upstream.force11.org/content/images/size/w1000/2023/10/plugs-1.png
        1000w, https://upstream.force11.org/content/images/size/w1600/2023/10/plugs-1.png
        1600w, https://upstream.force11.org/content/images/size/w2400/2023/10/plugs-1.png
        2400w\\\" sizes=\\\"(min-width: 720px) 720px\\\"></figure><p>In the context
        of persistent identifiers in research communications, adhering to universal
        design principles may, on the surface, seem to address interoperability issues.
        However, there is a potential risk of exacerbating fragmentation within the
        ecosystem, thereby disadvantaging emerging regions. Rather, we should advocate
        for collaborative\u2014and global\u2014initiatives aimed at preserving open
        infrastructure under responsible governance that avoids further fragmentation.</p><h2
        id=\\\"collaboration-and-gathering-context\\\">Collaboration and Gathering
        Context</h2><p>In discussing the merits of various identifier systems, gathering
        insights and context from stakeholders with practical experience in implementing
        both is vital. Hypothetical arguments should be avoided, as DataCite and Crossref
        actively collaborate with communities to address real-life challenges by enhancing
        their services. </p><p>We recommend that people explore the facts. Ask questions
        such as \u201C<em>what is the expected resource requirement for us now and
        in the future?</em>\u201D, \u201C<em>who is on the board?</em>\u201D and \u201C<em>how
        can I get on the board?</em>\u201D, \u201C<em>how open is the metadata, i.e.
        is it both available and accessible?</em>\u201D. \u201C<em>What is the likelihood
        of this going away?</em>\u201D and \u201C<em>What safeguards are there for
        long-term preservation?</em>\u201D. Find out what training and support is
        offered. Talk to your local community that speaks your language, and ask the
        members in 151 countries using DataCite and/or Crossref, e.g., an Ambassador,
        Consortia Lead, or Sponsor. </p><p>In conclusion, DOIs through DataCite and
        Crossref represent a cost-effective, community-governed, and persistent solution
        for identifying and referencing digital resources. By embracing DataCite and
        Crossref DOIs, the global scholarly community is empowered, reducing financial
        barriers and fostering broad creation, dissemination, and recognition of research
        outputs and resources.</p><p>DataCite and Crossref are actively investing
        in working on community collaboration, making it essential for stakeholders
        to engage in constructive dialogues that contribute to the ongoing improvement
        of research services. If you tell us more specifics about your needs and experience,
        we\u2019ll tell you more about the reality of running infrastructure, and
        we\u2019ll figure something out to help you get involved. Together, we can
        build a more equitable and accessible scholarly landscape for all communities
        and all countries.</p><p>Visit <a href=\\\"https://www.datacite.org\\\">DataCite</a>
        and <a href=\\\"https://www.crossref.org\\\">Crossref</a> websites for more
        information.</p><p><strong><em>Thanks to the following people for reviewing
        and adding context to this post: Britta Dreyer, Ed Pentz, Helena Cousijn,
        and John Chodacki.</em></strong></p>\",\"doi\":\"https://doi.org/10.54900/6sz4q-47185\",\"reference\":[],\"summary\":\"Persistent
        Identifiers, or \u201CPIDs\u201D, have become a popular topic for anyone involved
        in the research communications world. PIDs play a crucial role in the scholarly
        ecosystem by providing long-lasting references to and between all kinds of
        digital resources.\",\"tags\":[\"Thought Pieces\"],\"title\":\"Working for
        Global Equity through Digital Object Identifiers\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Chris
        Hartgerink\",\"url\":\"https://orcid.org/0000-0003-1050-6809\"},{\"name\":\"Sarahanne
        Field\",\"url\":\"http://www.sarahannemfield.com\"}],\"blog_id\":\"h49ct36\",\"blog_name\":\"Liberate
        Science\",\"blog_slug\":\"libscie\",\"content_html\":\"<div class=\\\"kg-card
        kg-callout-card kg-callout-card-grey\\\"><div class=\\\"kg-callout-emoji\\\">\U0001F508</div><div
        class=\\\"kg-callout-text\\\">This is a transcript of the Open Update podcast.
        Listen to the original audio on <a href=\\\"https://podcasters.spotify.com/pod/show/open-update/episodes/Onboarding-with-confidence-in-open-research-s03e16-e2acp35?ref=libscie.org\\\">Anchor.fm</a>.</div></div><p>[00:00:00]
        <strong>Chris Hartgerink:</strong> Welcome back to the Open Update.</p><p>We're
        back in the series talking about the decade of open science in the last 10
        years, and then also this week we're gonna be talking a bit about, what do
        we foresee for the next 10 years?</p><p>In the last episode we talked about
        is open science stagnating, how do we look at the developments over the past
        10 years in various areas? Of course, open access, we have very clear numbers
        that it's gotten much, much better. But open science is much more than just
        that. So there's also policy that's in place now that is promising for the
        next 10 years, but we want to take time today to also talk about what is it
        that we receive for the next 10 years? Where are some opportunities, some
        challenges, and what are some of the risks in terms of how we've experienced
        the open research movement in the past 10 years? What are some things to look
        out for?</p><p>[00:00:54] <strong>Sarahanne Field:</strong> Something that
        I'm excited about seeing the development of in the coming years is, is the
        critical branch of meta science where we're not just accepting that these
        forms of openness are beneficial to everyone in every context.</p><p>But considering
        some of the nuanced aspects of that when pre-registration is not valuable
        or beneficial, when registered reports are problematic as a format, when openness
        doesn't always work for the benefit of everyone. So I'm, I'm really interested
        in where those discussions are going to go. That's a branch of meta science
        that I'm starting to see taking shape.</p><p>It's one thing to, to criticize
        other people's research as we have been doing for the last decade with replications,
        with calling people out with retractions. But now we're moving on to, \\\"okay,
        so how does the tenor of that discussion go?\\\" How does that tone, how should
        that tone look? So that's something that I'm excited to see sort of opening
        up in the future.</p><p>[00:01:54] <strong>Chris Hartgerink:</strong> \_For
        me personally, one of the things that I did not think would happen, at some
        point you just notice it's the same old discussions all over again. I'm still
        hearing these conversations about the impact factors, even though we knew
        about this eight years ago, just as well. Well, it's maybe a few different
        people having this conversation, so I understand it's also a bit of onboarding
        probably for people entering this space.</p><p>But why is this even still
        important? We're all susceptible to that feeling of we're having conversations
        all over again. That is something I definitely wanna look out for, is to really
        push that conversation forward about what does it mean to improve open research
        and actually making these things happen instead of just talking about them.</p><p>[00:02:38]
        <strong>Sarahanne Field:</strong> I'm really interested in these unintended
        consequences of the science reform movement.</p><p>That's another sort of
        thing that I'm really getting excited about, because often we are really uprooting
        parts of a larger, very complicated moving system, and so understanding, you
        know, what some of the challenges and the, the consequences of that are, that's
        a sort of a branch of, of critical meta science that I think is really great.</p><p>[00:03:02]
        <strong>Chris Hartgerink:</strong> It's very interesting to see that change
        happening because we talked about solutionism and trying to fix problems,
        and I think from that framework, unintended consequences are unforeseen very
        often, and of course unintended consequences are very often things we didn't
        think about, but it's exactly that point where if we stop thinking about fixing
        one problem and then, you know, clearing ourselves and saying, okay, we're
        done here, then those unintended consequences become part of the process.</p><p>It's
        the, \\\"okay, we address one thing and something else pops up.\\\" It's like
        a continuous whack-a-mole of just trying to move forward. So in that sense,
        I think those two things are very tied to each other 'cause only if we really
        have a way to have those conversations in a productive way, to express criticism,
        to express concerns, to express just thoughts around specific ideas, only
        then can we identify where that next step might be taken.</p><p>In those next
        10 years, if we were to do an episode like this or a series of like this,
        again in 10 years, I would be very, very glad if we can look back and identify,
        okay, we've actually made much more decisions, much more statements, much
        more, joint efforts in terms of how resources get spent, how power gets used,
        how we organize things. Because right now we're very often making those decisions
        individually, or we have open letters where we sign on to a statement, which
        is also a form of organizing.</p><p>But on the scale of organizing, I think
        that's an entry point. It's a relatively low hanging fruit instead of, you
        know, completely the opposite. How do we actually organize to complete a project?
        'cause imagine if we were able to do a large scale project with a hundred
        people, you know, replication projects, they've done this. How would we do
        this on a continuous scale to really, you know, mobilize beyond. Just a research
        project. I think that's something that I would be very keen on being able
        to look back on in 10 years.</p><p>[00:05:30] <strong>Sarahanne Field:</strong>
        \_Just wanted to touch on something you mentioned before about how we can't
        always anticipate the next step because often these consequences are unintended.
        This comes back to sort of something I've been thinking more and more about
        lately is that open is a such an enormous toolbox of practices. When I'm thinking
        about anticipating the, the possible chains of effect of certain actions,
        what comes along with that is also accountability.</p><p>For me, looking forward,
        you know, it, I would like to see people engaging with the challenges of open
        research with this kind of multifaceted approach. Having this anticipation,
        having this accountability alongside the actions that they are taking. I think
        that that rounded approach is, is gonna be beneficial and that's gonna really
        be valuable. That's, that's my perspective.</p><p>You mentioned that part
        of what causes a stagnation, that we're seeing is partly onboarding. So by
        that I'm assuming you mean when new researchers, for example, are, are coming
        on board with, with open research, they're learning the discourse, they're,
        they're learning to contribute their own ideas.</p><p>'cause this is gonna
        be constantly happening. I know people literally every day, every minute are
        coming to open research. How do we allow that onboarding and encourage that
        onboarding? 'cause of course we want this without getting redundancy or getting
        stagnation. Do you have any thoughts on that?</p><p>[00:07:07] <strong>Chris
        Hartgerink:</strong> I always like when people say, let's create onboarding
        ramps, because I think that's a very natural way of thinking about it. You
        can't just immediately hit the highway without an onboarding ramp. Right.
        I didn't necessarily want to criticize that these conversations are being
        had. 'cause I think it's more the, that there's still a need to have these
        conversations.</p><p>So in terms of onboarding, there's \\\"one\\\" too many
        places where there's literally zero onboarding. Here's your key to the office,
        here's your laptop or your computer, your logging info, and you know, figure
        it out on your own. Especially with early career researchers who are entering
        a PhD.</p><p>You might have situations where it's really nice and there's
        actually a group who cares about that, and they also go beyond the onboarding
        they might have, for example, you know, building trust within a team, which
        is incredibly important.</p><p>And then also offboarding to understand where
        our improvement factors for that specific group. Having both the conversations
        at the very beginning, at the very end, but also intermittently in between
        to understand what's happening. And I think there's just too many places within
        research where very normal management practices aren't implemented at all.</p><p>So
        for example, if I were to enter a research group today, I would very much
        like to know, what is the culture? What is the expectation with respect to
        how do we communicate? What is the expectation with respect to how do we work
        within a team?</p><p>You know, when something gets submitted, is it enough
        to have people's implicit approval or explicit approval? And just setting
        up that cultural norm. Then also vice versa. Literally saying, okay, this
        is what we don't do. So if there is a group where you come into and you know,
        journal impact factors are not important at all, but everybody else at the
        department talks about them, then you're gonna be the odd one out and it's
        gonna influence how you see it.</p><p>So I think just in that sense, onboarding
        is incredibly important to not necessarily preempt these conversations, but
        to create clarity. Because I think I've mentioned this in another episode
        like way back, but in ambiguity there resides a lot of power. And I think
        with a lot of research groups, there's just incredible amounts of ambiguity.</p><p>There's
        a meeting and then there's a decision, and I feel like that also creates inconsistencies
        between person 1, 2, 3. In the same research group, and if there's one thing
        about good management is that you apply it the same way to everybody who is
        affected by that, regardless of whether they're an, an incredible say, PhD
        candidate or an average one, or, underperforming one.</p><p>I think in that
        sense, there's just so much we can do to make sure that management. Creates
        good conditions. The compost for these practices to flourish.</p><p>[00:10:16]
        <strong>Sarahanne Field:</strong> That's what I really like about what FORTT
        are doing. They are basically a landing page for people to go as sort of a
        first step in their open research journey. I love that because it does help
        people, in the sense that it provides a uniform basis for everyone to sort
        of start on. It provides information, templates, that kind of thing. And it
        really does help with onboarding so that people aren't sort of reinventing
        the wheel for themselves. That really helps with, especially with, with ECRs
        who are still finding their footing in research in general.</p><p>Learning,
        you know, to conduct research. And then on top of that, this extra level of
        methodology and, and philosophy to sort of also learn that alongside.</p><p>[00:11:00]
        <strong>Chris Hartgerink:</strong> FORTT is doing tremendous work. Just the
        size of that community and how fast they've grown in a decentralized system.
        Flip side of FORTT, but also a lot of other onboarding or in general academic
        approaches to communicating is, it tends to be incredibly precise. And incredibly
        lengthy and incredibly difficult to understand.</p><p>It's not an onboarding
        language very often, so I think that there's still room for improvement in
        terms of how do we write, with what clarity do we enter that situation and
        with the purpose to communicate something effectively in different forms,
        but also in a succinct way.</p><p>I often stumble upon resources that are
        pages and pages and pages long, which creates a lot of attrition, which creates
        a lot of, lack of retaining, even though maybe if you can summarize it in,
        say, a few paragraphs that are very well copywritten, that aren't perfect.
        If retaining the information is higher, I think that trade off is worth it.</p><p>So
        this aspect of getting out of our academic mindset sometimes is very difficult.
        But it's important because I see that there we could really, really gain a
        lot of ground. But that is my personal opinion on that front because I only
        started noticing how much that influenced how I personally wrote after I stopped
        needing to do it. Because clear communication really removes the need for
        so much extra work and clear communication is incredibly difficult. Let's
        be, let's be honest.</p><p>[00:12:49] <strong>Sarahanne Field:</strong> I
        mean, when you come into the open space, there is so much to learn.</p><p>So
        say you wanna conduct a replication, you know, there are lots of different
        templates for how to conduct that. There are a lot of different ideas on how
        you might wanna go about choosing a study for replication. There are different
        ways to pre-register your plans for a, a replication, for example.</p><p>Having
        piles and piles of resources to go through is a huge amount of labor. But
        I think it's, it's difficult because there is so much information to be consumed,
        but that's why I really like Christina Bergman's buffet approach to open research.
        For the listeners who aren't familiar with that, it's this idea that when
        you approach open research, you pick and choose the approaches or the resources
        or the tools you use, which fit best into your way of doing research. Into
        your particular research paradigm, into your particular epistemology.</p><p>And
        I think that's really important to help educate people on that way of thinking
        about open research. It's not about you have to adopt all of the things, do
        all the stuff or you're not an open researcher. It applies to you in whatever
        phase you're at, in whatever way you do research. It's a matter of helping,
        you know, guide people to what's most useful for them and how they can contribute
        to the open movement, if you can call it.</p><p>[00:14:12] <strong>Chris Hartgerink:</strong>
        Yeah. And in that sense, also, what, what helps reduce the complexity is clear
        language, of course. But there's also a, you know, recognizing indeed that
        you don't have to eat everything from the buffet. You don't need to finish
        all the plates. Um, and there's something for everyone. At the same time also
        reducing all of this complexity to fundamental principles.</p><p>I think we
        too often get sucked into talking about reproducibility, replicability, open
        access, APCs, all of these specific things, when in essence all of it can
        be really drilled down to working openly. And that doesn't mean everything
        needs to be open, but this idea of can others access your results or whatever
        it is that you're doing, and understanding where in that process it might
        not be possible.</p><p>And just reflecting on that and saying, okay, what
        could I do to improve that bit? For me, that that has been really the, the
        red line throughout my own work. It's the. I'm reviewing something. Can I
        understand what they, what they did in the paper, where those results came
        from? Oh, I'm submitting something. Oh, you know, can people actually access
        the data? I might have a link in there, but let's double check. And so all
        of these things about working openly, it's also this question of just simply
        what is the, what is the narrative around your own work, and is that publicly
        available?</p><p>Sometimes you can actually decide. I don't want this to be
        available, but to me that has really been the core of my own philosophy. And
        then it's no longer about applying that in research, but it, it could be applied
        anywhere. Whether, whether I, you know, if I go to software engineering, of
        course we have open source, but that same principle applies if we go to finances.
        You could ask the same question. If you go to legal scholarship, you could
        ask that question as well. Of course, it gets different answers. But in that
        sense, reducing that complexity is, I hope that that happens in the next 10
        years and that, uh, that that helps make it easier to navigate and that people
        also get the encouragement to discover what that means for them, because in
        that sense, Anybody's working process, regardless of whether it's open or
        closed or anything in between is very personal and changes all the time.</p><p>[00:16:49]
        <strong>Sarahanne Field:</strong> What I like about some of the maturity of,
        of the movement that we're we're coming into, is that we are interrogating
        that assumption of open always being best. Where some of us at least are getting
        the confidence to sort of decipher ourselves rather than just drinking the
        open Kool-Aid. We're starting to think about this more actively engaging with
        that question of \\\"when is open most beneficial and is, is it always beneficial
        and for every case?\\\"</p><p>That's something that I try and instill in,
        in people who come to me for, for advice, students I have, for example. I
        try and just instill a sense of confidence in them and give them the idea
        that they should be conducting research in the spirit of openness.</p><p>So
        they may not have all the skills and the knowledge to engage in all the practices
        they want to. But I think what has to come first is acting in that spirit
        of honesty and transparency and integrity. That's a really important place
        to start and have confidence in acting in that way, and in, in my opinion,
        so much falls into place after that.</p><p>Then you can learn, for example,
        as just an undergraduate student to write your research reports with transparency,
        be aware of things like supplemental material and that they can be published
        alongside the paper. Encouraging detail when it comes to methodological reporting,
        for example, and giving them confidence that they can just start at that level.</p><p>So
        I think that's something that I really like, is that we're just starting to
        sort of see how we can act in the spirit of openness and not always have to
        be doing all the things. That's sort of a natural progression from Bergman's
        idea of, of the open buffet is just starting off, you know, as you're onboarding
        in general or as an ECR coming into open research, that you begin with acting
        in the spirit of that.</p><p>It takes time, right? To build the skill to perform
        these practices properly. It's difficult to get all of the steps right. And
        there is a fear in a lot of, especially ECRs, that if they do it wrong, the
        open science bullies are gonna come for them. People have said this to me
        that they're afraid of that. So I think instilling that sense of confidence
        and, and getting them to, to learn to think in the spirit of openness is a
        great place to start.</p><p>So, confidence building. Capacity building. Giving
        them the, the self-efficacy and the confidence to, to think I can build these
        skills up. I have the space and the time to learn these things, rather than
        saying, yeah, all the practices have to happen like that, you know?</p><p>That
        brings me to a, a new thread of conversation we might want to follow in the
        next episode, which is the question of how the burden of responsible conduct,
        open practices, who that burden should fall on, and how should we distribute
        that burden or that responsibility across the members in a team.</p><p>[00:19:37]
        <strong>Chris Hartgerink:</strong> So join us again in a few weeks and we'll
        be chatting about this.</p><p>Thank you again for listening. Don't forget
        to join our signal group. If you have ideas around how research or open research
        or just research in general should develop in the next 10 years, what you
        would like to see. I think we've had some very concrete points, more confidence
        building, more clarity and communication and reducing the complexity.</p><p>And
        so we would love to hear from you as well.</p><p>[00:20:05] <strong>Sarahanne
        Field:</strong> Join the signal team. Give us your feedback.</p>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/wk19c-zqs88\",\"id\":\"24723\",\"image\":\"https://images.unsplash.com/photo-1593435221376-379aaae1a26c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDMzfHxyYW1wfGVufDB8fHx8MTY5NjkyMzk0OHww&ixlib=rb-4.0.3&q=80&w=2000\",\"language\":\"en\",\"published_at\":1696929637,\"reference\":[],\"relationships\":[],\"summary\":\"\U0001F508This
        is a transcript of the Open Update podcast. Listen to the original audio on
        Anchor.fm. [00:00:00] <strong>Chris Hartgerink:</strong> Welcome back to the
        Open Update.\",\"tags\":[\"Open Update\",\"Labs\"],\"title\":\"Onboarding
        with confidence in open research (s03e16)\",\"updated_at\":1696929637,\"url\":\"https://libscie.org/onboarding-with-confidence-in-open-science-s03e16\",\"uuid\":\"cc25062b-cf1c-4077-9559-a703acc19a00\"},\"highlight\":{\"authors\":[{\"name\":\"Chris
        Hartgerink\",\"url\":\"https://orcid.org/0000-0003-1050-6809\"},{\"name\":\"Sarahanne
        Field\",\"url\":\"http://www.sarahannemfield.com\"}],\"content_html\":\"<div
        class=\\\"kg-card kg-callout-card kg-callout-card-grey\\\"><div class=\\\"kg-callout-emoji\\\">\U0001F508</div><div
        class=\\\"kg-callout-text\\\">This is a transcript of the Open Update podcast.
        Listen to the original audio on <a href=\\\"https://podcasters.spotify.com/pod/show/open-update/episodes/Onboarding-with-confidence-in-open-research-s03e16-e2acp35?ref=libscie.org\\\">Anchor.fm</a>.</div></div><p>[00:00:00]
        <strong>Chris Hartgerink:</strong> Welcome back to the Open Update.</p><p>We're
        back in the series talking about the decade of open science in the last 10
        years, and then also this week we're gonna be talking a bit about, what do
        we foresee for the next 10 years?</p><p>In the last episode we talked about
        is open science stagnating, how do we look at the developments over the past
        10 years in various areas? Of course, open access, we have very clear numbers
        that it's gotten much, much better. But open science is much more than just
        that. So there's also policy that's in place now that is promising for the
        next 10 years, but we want to take time today to also talk about what is it
        that we receive for the next 10 years? Where are some opportunities, some
        challenges, and what are some of the risks in terms of how we've experienced
        the open research movement in the past 10 years? What are some things to look
        out for?</p><p>[00:00:54] <strong>Sarahanne Field:</strong> Something that
        I'm excited about seeing the development of in the coming years is, is the
        critical branch of meta science where we're not just accepting that these
        forms of openness are beneficial to everyone in every context.</p><p>But considering
        some of the nuanced aspects of that when pre-registration is not valuable
        or beneficial, when registered reports are problematic as a format, when openness
        doesn't always work for the benefit of everyone. So I'm, I'm really interested
        in where those discussions are going to go. That's a branch of meta science
        that I'm starting to see taking shape.</p><p>It's one thing to, to criticize
        other people's research as we have been doing for the last decade with replications,
        with calling people out with retractions. But now we're moving on to, \\\"okay,
        so how does the tenor of that discussion go?\\\" How does that tone, how should
        that tone look? So that's something that I'm excited to see sort of opening
        up in the future.</p><p>[00:01:54] <strong>Chris Hartgerink:</strong> \_For
        me personally, one of the things that I did not think would happen, at some
        point you just notice it's the same old discussions all over again. I'm still
        hearing these conversations about the impact factors, even though we knew
        about this eight years ago, just as well. Well, it's maybe a few different
        people having this conversation, so I understand it's also a bit of onboarding
        probably for people entering this space.</p><p>But why is this even still
        important? We're all susceptible to that feeling of we're having conversations
        all over again. That is something I definitely wanna look out for, is to really
        push that conversation forward about what does it mean to improve open research
        and actually making these things happen instead of just talking about them.</p><p>[00:02:38]
        <strong>Sarahanne Field:</strong> I'm really interested in these unintended
        consequences of the science reform movement.</p><p>That's another sort of
        thing that I'm really getting excited about, because often we are really uprooting
        parts of a larger, very complicated moving system, and so understanding, you
        know, what some of the challenges and the, the consequences of that are, that's
        a sort of a branch of, of critical meta science that I think is really great.</p><p>[00:03:02]
        <strong>Chris Hartgerink:</strong> It's very interesting to see that change
        happening because we talked about solutionism and trying to fix problems,
        and I think from that framework, unintended consequences are unforeseen very
        often, and of course unintended consequences are very often things we didn't
        think about, but it's exactly that point where if we stop thinking about fixing
        one problem and then, you know, clearing ourselves and saying, okay, we're
        done here, then those unintended consequences become part of the process.</p><p>It's
        the, \\\"okay, we address one thing and something else pops up.\\\" It's like
        a continuous whack-a-mole of just trying to move forward. So in that sense,
        I think those two things are very tied to each other 'cause only if we really
        have a way to have those conversations in a productive way, to express criticism,
        to express concerns, to express just thoughts around specific ideas, only
        then can we identify where that next step might be taken.</p><p>In those next
        10 years, if we were to do an episode like this or a series of like this,
        again in 10 years, I would be very, very glad if we can look back and identify,
        okay, we've actually made much more decisions, much more statements, much
        more, joint efforts in terms of how resources get spent, how power gets used,
        how we organize things. Because right now we're very often making those decisions
        individually, or we have open letters where we sign on to a statement, which
        is also a form of organizing.</p><p>But on the scale of organizing, I think
        that's an entry point. It's a relatively low hanging fruit instead of, you
        know, completely the opposite. How do we actually organize to complete a project?
        'cause imagine if we were able to do a large scale project with a hundred
        people, you know, replication projects, they've done this. How would we do
        this on a continuous scale to really, you know, mobilize beyond. Just a research
        project. I think that's something that I would be very keen on being able
        to look back on in 10 years.</p><p>[00:05:30] <strong>Sarahanne Field:</strong>
        \_Just wanted to touch on something you mentioned before about how we can't
        always anticipate the next step because often these consequences are unintended.
        This comes back to sort of something I've been thinking more and more about
        lately is that open is a such an enormous toolbox of practices. When I'm thinking
        about anticipating the, the possible chains of effect of certain actions,
        what comes along with that is also accountability.</p><p>For me, looking forward,
        you know, it, I would like to see people engaging with the challenges of open
        research with this kind of multifaceted approach. Having this anticipation,
        having this accountability alongside the actions that they are taking. I think
        that that rounded approach is, is gonna be beneficial and that's gonna really
        be valuable. That's, that's my perspective.</p><p>You mentioned that part
        of what causes a stagnation, that we're seeing is partly onboarding. So by
        that I'm assuming you mean when new researchers, for example, are, are coming
        on board with, with open research, they're learning the discourse, they're,
        they're learning to contribute their own ideas.</p><p>'cause this is gonna
        be constantly happening. I know people literally every day, every minute are
        coming to open research. How do we allow that onboarding and encourage that
        onboarding? 'cause of course we want this without getting redundancy or getting
        stagnation. Do you have any thoughts on that?</p><p>[00:07:07] <strong>Chris
        Hartgerink:</strong> I always like when people say, let's create onboarding
        ramps, because I think that's a very natural way of thinking about it. You
        can't just immediately hit the highway without an onboarding ramp. Right.
        I didn't necessarily want to criticize that these conversations are being
        had. 'cause I think it's more the, that there's still a need to have these
        conversations.</p><p>So in terms of onboarding, there's \\\"one\\\" too many
        places where there's literally zero onboarding. Here's your key to the office,
        here's your laptop or your computer, your logging info, and you know, figure
        it out on your own. Especially with early career researchers who are entering
        a PhD.</p><p>You might have situations where it's really nice and there's
        actually a group who cares about that, and they also go beyond the onboarding
        they might have, for example, you know, building trust within a team, which
        is incredibly important.</p><p>And then also offboarding to understand where
        our improvement factors for that specific group. Having both the conversations
        at the very beginning, at the very end, but also intermittently in between
        to understand what's happening. And I think there's just too many places within
        research where very normal management practices aren't implemented at all.</p><p>So
        for example, if I were to enter a research group today, I would very much
        like to know, what is the culture? What is the expectation with respect to
        how do we communicate? What is the expectation with respect to how do we work
        within a team?</p><p>You know, when something gets submitted, is it enough
        to have people's implicit approval or explicit approval? And just setting
        up that cultural norm. Then also vice versa. Literally saying, okay, this
        is what we don't do. So if there is a group where you come into and you know,
        journal impact factors are not important at all, but everybody else at the
        department talks about them, then you're gonna be the odd one out and it's
        gonna influence how you see it.</p><p>So I think just in that sense, onboarding
        is incredibly important to not necessarily preempt these conversations, but
        to create clarity. Because I think I've mentioned this in another episode
        like way back, but in ambiguity there resides a lot of power. And I think
        with a lot of research groups, there's just incredible amounts of ambiguity.</p><p>There's
        a meeting and then there's a decision, and I feel like that also creates inconsistencies
        between person 1, 2, 3. In the same research group, and if there's one thing
        about good management is that you apply it the same way to everybody who is
        affected by that, regardless of whether they're an, an incredible say, PhD
        candidate or an average one, or, underperforming one.</p><p>I think in that
        sense, there's just so much we can do to make sure that management. Creates
        good conditions. The compost for these practices to flourish.</p><p>[00:10:16]
        <strong>Sarahanne Field:</strong> That's what I really like about what FORTT
        are doing. They are basically a landing page for people to go as sort of a
        first step in their open research journey. I love that because it does help
        people, in the sense that it provides a uniform basis for everyone to sort
        of start on. It provides information, templates, that kind of thing. And it
        really does help with onboarding so that people aren't sort of reinventing
        the wheel for themselves. That really helps with, especially with, with ECRs
        who are still finding their footing in research in general.</p><p>Learning,
        you know, to conduct research. And then on top of that, this extra level of
        methodology and, and philosophy to sort of also learn that alongside.</p><p>[00:11:00]
        <strong>Chris Hartgerink:</strong> FORTT is doing tremendous work. Just the
        size of that community and how fast they've grown in a decentralized system.
        Flip side of FORTT, but also a lot of other onboarding or in general academic
        approaches to communicating is, it tends to be incredibly precise. And incredibly
        lengthy and incredibly difficult to understand.</p><p>It's not an onboarding
        language very often, so I think that there's still room for improvement in
        terms of how do we write, with what clarity do we enter that situation and
        with the purpose to communicate something effectively in different forms,
        but also in a succinct way.</p><p>I often stumble upon resources that are
        pages and pages and pages long, which creates a lot of attrition, which creates
        a lot of, lack of retaining, even though maybe if you can summarize it in,
        say, a few paragraphs that are very well copywritten, that aren't perfect.
        If retaining the information is higher, I think that trade off is worth it.</p><p>So
        this aspect of getting out of our academic mindset sometimes is very difficult.
        But it's important because I see that there we could really, really gain a
        lot of ground. But that is my personal opinion on that front because I only
        started noticing how much that influenced how I personally wrote after I stopped
        needing to do it. Because clear communication really removes the need for
        so much extra work and clear communication is incredibly difficult. Let's
        be, let's be honest.</p><p>[00:12:49] <strong>Sarahanne Field:</strong> I
        mean, when you come into the open space, there is so much to learn.</p><p>So
        say you wanna conduct a replication, you know, there are lots of different
        templates for how to conduct that. There are a lot of different ideas on how
        you might wanna go about choosing a study for replication. There are different
        ways to pre-register your plans for a, a replication, for example.</p><p>Having
        piles and piles of resources to go through is a huge amount of labor. But
        I think it's, it's difficult because there is so much information to be consumed,
        but that's why I really like Christina Bergman's buffet approach to open research.
        For the listeners who aren't familiar with that, it's this idea that when
        you approach open research, you pick and choose the approaches or the resources
        or the tools you use, which fit best into your way of doing research. Into
        your particular research paradigm, into your particular epistemology.</p><p>And
        I think that's really important to help educate people on that way of thinking
        about open research. It's not about you have to adopt all of the things, do
        all the stuff or you're not an open researcher. It applies to you in whatever
        phase you're at, in whatever way you do research. It's a matter of helping,
        you know, guide people to what's most useful for them and how they can contribute
        to the open movement, if you can call it.</p><p>[00:14:12] <strong>Chris Hartgerink:</strong>
        Yeah. And in that sense, also, what, what helps reduce the complexity is clear
        language, of course. But there's also a, you know, recognizing indeed that
        you don't have to eat everything from the buffet. You don't need to finish
        all the plates. Um, and there's something for everyone. At the same time also
        reducing all of this complexity to fundamental principles.</p><p>I think we
        too often get sucked into talking about reproducibility, replicability, open
        access, APCs, all of these specific things, when in essence all of it can
        be really drilled down to working openly. And that doesn't mean everything
        needs to be open, but this idea of can others access your results or whatever
        it is that you're doing, and understanding where in that process it might
        not be possible.</p><p>And just reflecting on that and saying, okay, what
        could I do to improve that bit? For me, that that has been really the, the
        red line throughout my own work. It's the. I'm reviewing something. Can I
        understand what they, what they did in the paper, where those results came
        from? Oh, I'm submitting something. Oh, you know, can people actually access
        the data? I might have a link in there, but let's double check. And so all
        of these things about working openly, it's also this question of just simply
        what is the, what is the narrative around your own work, and is that publicly
        available?</p><p>Sometimes you can actually decide. I don't want this to be
        available, but to me that has really been the core of my own philosophy. And
        then it's no longer about applying that in research, but it, it could be applied
        anywhere. Whether, whether I, you know, if I go to software engineering, of
        course we have open source, but that same principle applies if we go to finances.
        You could ask the same question. If you go to legal scholarship, you could
        ask that question as well. Of course, it gets different answers. But in that
        sense, reducing that complexity is, I hope that that happens in the next 10
        years and that, uh, that that helps make it easier to navigate and that people
        also get the encouragement to discover what that means for them, because in
        that sense, Anybody's working process, regardless of whether it's open or
        closed or anything in between is very personal and changes all the time.</p><p>[00:16:49]
        <strong>Sarahanne Field:</strong> What I like about some of the maturity of,
        of the movement that we're we're coming into, is that we are interrogating
        that assumption of open always being best. Where some of us at least are getting
        the confidence to sort of decipher ourselves rather than just drinking the
        open Kool-Aid. We're starting to think about this more actively engaging with
        that question of \\\"when is open most beneficial and is, is it always beneficial
        and for every case?\\\"</p><p>That's something that I try and instill in,
        in people who come to me for, for advice, students I have, for example. I
        try and just instill a sense of confidence in them and give them the idea
        that they should be conducting research in the spirit of openness.</p><p>So
        they may not have all the skills and the knowledge to engage in all the practices
        they want to. But I think what has to come first is acting in that spirit
        of honesty and transparency and integrity. That's a really important place
        to start and have confidence in acting in that way, and in, in my opinion,
        so much falls into place after that.</p><p>Then you can learn, for example,
        as just an undergraduate student to write your research reports with transparency,
        be aware of things like supplemental material and that they can be published
        alongside the paper. Encouraging detail when it comes to methodological reporting,
        for example, and giving them confidence that they can just start at that level.</p><p>So
        I think that's something that I really like, is that we're just starting to
        sort of see how we can act in the spirit of openness and not always have to
        be doing all the things. That's sort of a natural progression from Bergman's
        idea of, of the open buffet is just starting off, you know, as you're onboarding
        in general or as an ECR coming into open research, that you begin with acting
        in the spirit of that.</p><p>It takes time, right? To build the skill to perform
        these practices properly. It's difficult to get all of the steps right. And
        there is a fear in a lot of, especially ECRs, that if they do it wrong, the
        open science bullies are gonna come for them. People have said this to me
        that they're afraid of that. So I think instilling that sense of confidence
        and, and getting them to, to learn to think in the spirit of openness is a
        great place to start.</p><p>So, confidence building. Capacity building. Giving
        them the, the self-efficacy and the confidence to, to think I can build these
        skills up. I have the space and the time to learn these things, rather than
        saying, yeah, all the practices have to happen like that, you know?</p><p>That
        brings me to a, a new thread of conversation we might want to follow in the
        next episode, which is the question of how the burden of responsible conduct,
        open practices, who that burden should fall on, and how should we distribute
        that burden or that responsibility across the members in a team.</p><p>[00:19:37]
        <strong>Chris Hartgerink:</strong> So join us again in a few weeks and we'll
        be chatting about this.</p><p>Thank you again for listening. Don't forget
        to join our signal group. If you have ideas around how research or open research
        or just research in general should develop in the next 10 years, what you
        would like to see. I think we've had some very concrete points, more confidence
        building, more clarity and communication and reducing the complexity.</p><p>And
        so we would love to hear from you as well.</p><p>[00:20:05] <strong>Sarahanne
        Field:</strong> Join the signal team. Give us your feedback.</p>\",\"doi\":\"https://doi.org/10.59350/wk19c-zqs88\",\"reference\":[],\"summary\":\"\U0001F508This
        is a transcript of the Open Update podcast. Listen to the original audio on
        Anchor.fm. [00:00:00] <strong>Chris Hartgerink:</strong> Welcome back to the
        Open Update.\",\"tags\":[\"Open Update\",\"Labs\"],\"title\":\"Onboarding
        with confidence in open research (s03e16)\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Carsten
        Hjort Lange\",\"url\":\"https://vbn.aau.dk/en/persons/131743\"}],\"blog_id\":\"5knte41\",\"blog_name\":\"Stasis\",\"blog_slug\":\"stasis\",\"content_html\":\"\\n<p
        class=\\\"has-drop-cap\\\">The words and concepts we use to describe a particular
        war speak volumes about who we are as individuals and as a community, and
        where we stand in an unfolding conflict. Concepts have always been used to
        occupy the moral high ground. Rebellion, for example, is used to delegitimise
        those who rebel. In Rome, the allied communities were not legally Roman citizens
        and therefore their rebellion was not a \u2018civil war\u2019. However, these
        non-citizens were considered part of the same polity as Roman citizens; the
        idea of an internal war was born, in debates in the Senate and beyond, debates
        between friends and foes. We debate! And sometimes we use violence, or even
        go to war against each other. It happens now, and it has happened many times
        in the past. The writings of contemporaries (and later sources) reflect these
        debates. From the outbreak of the Second Punic War in 218 to the Social War
        in 91, the Romans of the long second century had protracted debates about
        the changing nature of warfare \u2013 including the rebellion of their allies
        in the midst of the Second Punic War \u2013 conceptual and otherwise \u2013
        culminating in Rome\u2019s first civil war in 88 BCE.</p>\\n\\n\\n\\n<p>At
        its core, my forthcoming book, <em>From Hannibal to Sulla: The Birth of Civil
        War in Republican Rome</em> (<em>Studies in Ancient Civil War</em>, De Gruyter),
        attempts to trace these debates and to show how, over the course of the second
        century, the language of external war was slowly adapted to a new language
        of internal and, ultimately, civil war. It combines two ideas: 1) the formative
        role of antebellum on the one hand, and 2) the importance of the \u2018great\u2019
        war before the civil war on the other hand; in preparing for the next conflict,
        Rome looked back at the last great one. It suggests that the period from the
        Second Punic War constituted, conceptually speaking, an \u2018antebellum\u2019
        period to Rome\u2019s later civil wars. It traces the origins not only of
        the concept and terminology of the <em>bellum civile</em>, but also of related
        terms.</p>\\n\\n\\n\\n<figure class=\\\"wp-block-image size-full\\\"><a href=\\\"https://www.degruyter.com/document/isbn/9783111335216/html\\\"><img
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"827\\\" height=\\\"1155\\\"
        src=\\\"https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090.png\\\"
        alt=\\\"\\\" class=\\\"wp-image-1253\\\" srcset=\\\"https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090.png
        827w, https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090-215x300.png
        215w, https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090-358x500.png
        358w, https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090-768x1073.png
        768w\\\" sizes=\\\"(max-width: 827px) 100vw, 827px\\\" /></a></figure>\\n\\n\\n\\n<p>However,
        the book does not really propose a \u2018conceptual history\u2019 (<em>Begriffsgeschichte</em>)
        in the manner of Reinhart Koselleck. The basic idea that, at some point in
        modernity, old words have acquired new meanings so that they no longer need
        to be translated seems overly simplistic, partly because it gives undue weight
        to modernity (it is close(r) to our contemporary world = it is modern = it
        is different from the non-modern world, or so the argument often seems to
        go. I work mostly on old stuff (= a long time ago), but it is just not pre-modern,
        it is very modern, conceptually speaking certainly so). It is true that some
        concepts recur throughout history, but even so we must accept, with Quentin
        Skinner, that they were never immutable and never had a (pre-)determined meaning.
        We can accept that the phenomenon of civil war recurs (with Thucydides\u2019
        description of the <em>stasis</em> in Corcyra during the Peloponnesian War,
        3.81-85) in the understanding that civil wars occur throughout history: Corcyra,
        the late Roman Republic, the late Roman Empire, the English Civil War, the
        American Civil War, and so on. More than anything, my book is about the history
        of the use of concepts rather than a history of concepts.</p>\\n\\n\\n\\n<p>What
        is more, however coherent definitions may be in theory, in practice they were
        as contestable in ancient times as they are today. Can we still believe \u2013
        in the spirit of positivism \u2013 that deciphering the examples from our
        evidence will provide us with a correct basic definition of the concepts we
        are considering? Hardly! We should never accept that civil war is a concept
        that everyone agreed on in ancient times. Language has an unfortunate tendency
        to obscure the extent to which sources disagree, both in ancient times and
        today. Is <em>stasis</em> the same as civil war? <em>Stasis</em> can be a
        <em>polemos</em>, a (civil) war, but it can also be a sedition. The ancients
        simply never agreed on a definition of such slippery concepts as <em>stasis</em>
        and civil war. Why would they? Why would we? Concepts naturally change over
        time as they are used in a particular context.</p>\\n\\n\\n\\n<p>In 43 BCE
        the warmonger Cicero was trying hard to get the proconsul M. Antonius (<em>cos</em>.
        44) declared a <em>hostis publicus</em>. He was strenuously opposed in this
        endeavour by L. Iulius Caesar (<em>cos</em>. 64), who insisted that the term
        <em>bellum</em> be replaced with <em>tumultus</em> (<em>Phil</em>. 12.17):</p>\\n\\n\\n\\n<figure
        class=\\\"wp-block-pullquote\\\"><blockquote><p>I consistently called Antonius
        a public enemy [<em>hostis</em>], while others [L. Iulius Caesar] called him
        an adversary [<em>adversarius</em>]; I consistently called this a war [<em>bellum</em>],
        while others called it a public emergency [<em>tumultus</em>].</p><cite>Cic.
        <em>Phil</em>. 12.17</cite></blockquote></figure>\\n\\n\\n\\n<p>There has
        never been, and never will be, a single narrative. Reading, for example, David
        Armitage\u2019s fine 2017 book <em>Civil Wars. A History In Ideas</em>, it
        quickly becomes clear that the concept of civil war has always been, as I
        said, a slippery one. Importantly, I firmly believe that scholars should ultimately
        accept that we can safely assume that some features of ancient civil wars
        are indeed regular features of any civil war, and that we should therefore
        accept the civil war in ancient Rome as a valid and instructive example to
        consider in modern debates about civil war. This should come as no surprise!
        And it works just fine without history repeating itself.</p>\\n\\n\\n\\n<p>Accordingly,
        all scholars working on civil war\u2014including social and political scientists\u2014need
        to learn their history as well as the traditions of the concepts they apply.
        After all, history is the only laboratory we have. The next question must
        be about the consequences of such an approach.</p>\\n\\n\\n\\n<p>In the book
        I write:</p>\\n\\n\\n\\n<blockquote class=\\\"wp-block-quote\\\">\\n<p>Through
        careful historicization, it is possible to highlight the ahistorical nature
        of modern definitions of civil war. All modern debates about civil war should
        ideally take the Late Republic as their conceptual point of departure: its
        genesis after lies there (a \u201Cwar\u201D between \u201Ccitizens\u201D).
        Alternatively, the concept needs to be abandoned by moderns altogether and
        the concept of \u201Cinternal war\u201D used instead (as famously proposed
        by Eckstein in 1965 [<em>On the Etiology of Internal Wars</em>]). The problem
        with this alternative new concept of \u201Cinternal war\u201D is that this,
        too, was already theorised by the Romans in Latin and is not new at all: <em>bellum
        intestinum</em>, mainly but not exclusively a concept used in connection with
        wars between Rome and its allies or within the polity of allies. The semantic
        range of civil wars is today being stretched to encompass ideological differences
        within for example political parties, so that there is a risk of it losing
        any conceptual specificity</p>\\n<cite>Lange, <em><em>From Hannibal to Sulla:
        The Birth of Civil War in Republican Rome</em></em> (forthcoming)</cite></blockquote>\\n\\n\\n\\n<p>The
        problem today is that the concept of civil war is too often stripped of its
        basic meaning (citizenship and war). Not surprisingly, this is further complicated
        by issues such as the definition of war. When is a war a war? It is safe to
        say that a true civil war requires at least something resembling armies and
        battles, with actual fighting between citizens. In his famous book <em>The
        Logic of Violence in Civil War</em>, Stathis Kalyvas gives us a definition:
        \u201Carmed combat within the borders of a recognised sovereign entity between
        parties subject to a common authority at the beginning of hostilities\u201D
        (2006, 17). His definition not only lacks references to and understanding
        of both <em>war</em> and <em>citizenship</em> (<em>bellum civile</em>), the
        words that make up the concept it defines, but also lacks the exclusivity
        necessary for it to function; it is vague. As I wrote in 2017 in a <a rel=\\\"noreferrer
        noopener\\\" href=\\\"https://cal.library.utoronto.ca/index.php/cal/article/view/28855\\\"
        target=\\\"_blank\\\">review </a>of David Armitage\u2019s 2017 book:</p>\\n\\n\\n\\n<blockquote
        class=\\\"wp-block-quote\\\">\\n<p>The terms \u201Cinclusive\u201D [Kalyvas\u2019
        vague definition] or \u201Cexclusive\u201D [something a kind of conventional
        war and battles between citizens] are often connected to definitions, as if
        we will somehow finally understand a concept by having agreed on its definition.
        Of course we all need definitions, mainly in order to agree that we are talking
        about the same thing (15-18, 219 n.50).</p>\\n<cite>Lange, Review of David
        Armitage, <em>Civil Wars: A History In Ideas</em> (2017)</cite></blockquote>\\n\\n\\n\\n<p>Those
        of us who study civil war must accept that we are studying a spectrum of violence,
        from <em>stasis</em> and <em>seditio</em> to <em>bellum civile</em>: from
        <em>antebellum</em> to <em>bellum</em>. In terms of contemporary relevance,
        what we see today are debates about the naming of conflicts, similar to those
        in ancient times. I write about this in the book:</p>\\n\\n\\n\\n<blockquote
        class=\\\"wp-block-quote\\\">\\n<p>Unsurprisingly, similar debates are also
        part of political landscapes today. On the night of the so-called <em>Capitol
        Riots</em> in 2021, NBC broadcast journalist Savannah Guthrie described the
        situation on live television as a \u201C<em>stasis</em>, for lack of a better
        word\u201D: that is, a period of debilitating and incapacitating civil strife
        or upheaval, taken directly from the Greek expression for \u201Cstandstill\u201D
        (\u03C3\u03C4\u03AC\u03C3\u03B9\u03C2), used in this book in its meaning of
        sedition. We will see in chapter 1 that these so-called riots have themselves
        already been described using numerous different concepts, all related in some
        way to <em>stasis</em>; Guthrie\u2019s comment is the ideal point of departure
        for any discussion about the enduring ambiguity of labels and definitions
        for internal conflict, in the modern world just as in the ancient. <em>Stasis</em>
        and civil war exert so profound an influence that societies must confront
        it in all periods.</p>\\n<cite>Lange, <em><em>From Hannibal to Sulla: The
        Birth of Civil War in Republican Rome</em></em> (forthcoming)</cite></blockquote>\\n\\n\\n\\n<p>Let\u2019s
        play around with a hypothetical scenario: what if the United States were to
        descend into civil war again? Barbara Walter, in her fine book <em>How Civil
        Wars Start. And How to Stop Them</em> (2022), is already ahead of us. According
        to her analysis, incidents such as the plot to kidnap Michigan Governor Gretchen
        Whitmer in 2020, the so-called <em>Capitol Riots</em> in 2021, and the recent
        attack on Representative Nancy Pelosi and her husband in 2022 are indeed examples
        of a potentially unfolding civil war. As such, these events may one day be
        easily described by future historians as part of the \u2018antebellum\u2019
        period leading up to the <em>Second American Civil War</em>, to explain its
        genesis.</p>\\n\\n\\n\\n<p>I am an ancient historian specialising in Republican
        Rome, not a specialist in contemporary American politics, but I will say this
        as a scholar of civil war: Joanne Freeman, in her wonderful book <em>The Field
        of Blood: Violence in Congress and the Road to Civil War</em>, focuses on
        physical violence in the US Congress from 1830 to the outbreak of the Civil
        War (2018). It sheds new light on the systemic breakdown in the decades leading
        up to the Civil War. It allows us to ask whether we can have a relatively
        well-functioning political system and a systemic breakdown at the same time.
        The prelude to the American Civil War, and indeed Late Republican Rome, suggest
        just that; suggest that it is possible. There may be a growing fear \u2013
        with Barbara Walter \u2013 that something similar is afoot in contemporary
        US politics and society. The concept of polarisation is crucial to our understanding
        of current events. Another great book can help us decipher modern US politics.
        Ezra Klein says as much in <em>Why We\u2019re Polarised</em>:</p>\\n\\n\\n\\n<figure
        class=\\\"wp-block-pullquote\\\"><blockquote><p>To appeal to a more polarized
        public, political institutions and political actors behave in more polarized
        ways. As political institutions and actors become more polarized, they further
        polarize the public.</p><cite>Ezra Klein, <em>Why We\u2019re Polarized</em>
        (2020), xix</cite></blockquote></figure>\\n\\n\\n\\n<p>Citing once again from
        my book, \u201CThis could easily be a description of antebellum politics in
        ancient times, of ancient civil strife and civil war language.\u201D</p>\\n\\n\\n\\n<p>Returning
        to the above debate about definitions of civil war, Barbara Walter, similar
        to Stathis Kalyvas, claims that (2022, xiv-xv): \u201CGone are the large battlefields,
        the armies, and the conventional tactics. Today, civil wars are waged by different
        ethnic and religious groups, by guerrilla soldiers and militias, who often
        target civilians.\u201D</p>\\n\\n\\n\\n<p>My answer I give in the book is
        as follows: \u201CThe issue here is, as mentioned, historicity\u2014an awareness
        of the repetitive nature of civil war and its features in earlier periods\u2014which
        is vital for us to compare conflicts over time. Problematically, this historicity
        is lost in her approach. In any case, what Walter discusses and defines as
        civil war would seem to any reader of Thucydides to clearly be not civil war
        but rather <em>stasis</em>. Walter\u2019s basic point regarding an (potentially)
        unfolding civil war can survive this, as this would, if a civil war happens,
        be its antebellum period (as mentioned in the introduction). There is however
        a distinct lack of language and knowledge of history.\u201D I wrote above
        that concepts change over time as they are used in a specific context. But
        this is simply too much and makes any comparison over time difficult or even
        impossible.</p>\\n\\n\\n\\n<p>More than anything else, I fear that the legacy
        of Rome will be lost. Now, I understand that this may be difficult for non-historians
        to accept as a major problem, but in doing so we will ultimately lose the
        ability to understand the legacy of Rome, and we may lose the ability to understand
        the fundamentally repetitive nature of civil war (understanding historical
        similarities as opposed to history repeating itself).</p>\\n\\n\\n\\n<p>Taking
        these issues further, the contemporary unfolding US debate about the 2020
        election and its aftermath, including the so-called <em>Capitol Riots</em>,
        is of great interest to an ancient historian working on civil war and related
        concepts. A debate about which concepts to use to describe what happened and
        what is happening, which happens to be very similar to debates about the changing
        nature of warfare and conflict in ancient times.</p>\\n\\n\\n\\n<p>Former
        President Trump\u2019s incitement to violence (and arguably to a coup) may
        not be a straightforward repetition of past events in every respect, but there
        are certainly patterns. These patterns illustrate the fundamental relevance
        of history to our understanding of the world around us.</p>\\n\\n\\n\\n<p>In
        the end, <em>Capitol Riots</em> \u2013 a violent disturbance of the peace
        by a crowd of people \u2013 seems a poor choice of words; it hardly conveys
        the seriousness of what happened. In the book I write:</p>\\n\\n\\n\\n<blockquote
        class=\\\"wp-block-quote\\\">\\n<p>The period from the 2020 US election onwards
        has more than anything shown that we today lack a nuanced language of internal
        political violence and conflict. Mainstream political discourse channelled
        through mass media lacks a spectrum of concepts related to organised violence,
        or perhaps better, we do not use them. Where attempts are made, they often
        stretch the semantic range of \u2018civil war\u2019 beyond recognition.&nbsp;
        &#8230; When the so-called Capitol Riots of 2021 are included, the term that
        springs to mind more readily than civil war is in fact political violence
        through insurrection\u2014as we shall see further below\u2014and <em>stasis</em>
        or <em>tumultus</em> or similar. But to claim that we might ultimately have
        a Second American Civil War is to invoke the legacy of one million military
        casualties of the American Civil War (1861\u20131865), of which as many as
        three-quarters died (&#8230;). This is about creating \u2013 perhaps even
        subconsciously \u2013 a mental picture of the horrors of not just civil war
        in principle, but the American Civil War in particular. All labels are of
        course political at the outset. We as scholars may agree that a conflict is
        either a civil war or not, but that may not reflect the language used to describe
        the war at the time.</p>\\n<cite>Lange, <em>From Hannibal to Sulla: The Birth
        of Civil War in Republican Rome</em> (forthcoming)</cite></blockquote>\\n\\n\\n\\n<p>But
        this post is not about Trump, not about his norm-breaking presidency, not
        about the US Constitution which presupposes a peaceful transition of power,
        not about \u201Cthe president\u2019s infamous walk to St John\u2019s with
        his entourage, including Secretary of Defense Esper and Chairman of the Joint
        Chiefs of Staff General Mark Milley in camouflage uniform, as well as Attorney
        General William Barr\u201D (from my book), and not about Trump wanting to
        send in the armed forces against protesters.</p>\\n\\n\\n\\n<p>But whatever
        we think of contemporary US politics, the storming of the Capitol on 6 January
        2021 has already changed the USA forever. The <em>House Select Committee to
        Investigate the January 6th Attack on the United States Capitol</em> is a
        testament to an ongoing and still unfolding debate about the so-called <em>Capitol
        Riots</em>, but it is also a testament to a committee that seeks to inform
        and shape future debates. The report suggests that Trump took advantage of
        an unfolding situation of his own making. The evidence seems overwhelming
        to support this conclusion. Trump was plotting to overturn the outcome of
        the 2020 election.</p>\\n\\n\\n\\n<p>This is hardly the place to go into detail,
        but Chapter One is called <em>THE BIG LIE</em>! Suffice it to say that the
        phrase was used by Adolf Hitler in <em>Mein Kampf</em> (1925 Vol. 1, 244)
        to explain how people could be made to believe a colossal lie. Whether the
        comparison is fair or not is not the point. Polarisation is!</p>\\n\\n\\n\\n<p>In
        terms of the language of crisis, the report uses many terms to describe what
        has happened: Terrorist attack (3 times), incite/incitement (20 times, but
        intent 44 times), civil war (20 times), riot(s) (25 times), coup (89 times),
        sedition (87 times; seditious conspiracy (65 times)), insurrection (100 times),
        mob (126 times), conspiracy (200 times). One might have wished for some prioritisation,
        but more than anything this reflects an unfolding debate about what future
        generations might end up calling the so-called <em>Capitol Riots</em>. Suffice
        it to say that <em>stasis</em> is perhaps the best concept to use, at least
        seen from the perspective and through the lens of ancient history.</p>\\n\\n\\n\\n<p>Writing
        about the American Civil War, Aaron Sheehan-Dean (2018, <em>The Calculus of
        Violence. How Americans Fought the Civil War</em>, 44) emphasises, \u201C[b]eyond
        their claims to a defensive and hence just position, the language each side
        used to frame its recourse to war in 1861 shaped how they behaved in the years
        to come\u201D. Translated into relevance today this is certainly food for
        thought. We can learn from history, and we need to accept its relevance today.
        But this can only happen if we accept the historicity of the concepts we use
        to describe contemporary (and past) conflicts. Not only do scholars working
        on the ancient use new words to describe old societies. We also use old words
        to describe modern society.</p>\\n\\n\\n\\n<h2 class=\\\"wp-block-heading\\\">References</h2>\\n\\n\\n\\n<ul>\\n<li>David
        Armitage, <em>Civil Wars: A History In Ideas</em>, New Haven: Yale University
        Press 2017.</li>\\n\\n\\n\\n<li>Ezra Klein, <em>Why We\u2019re Polarized</em>,
        New York: Avid Reader Press 2020.</li>\\n\\n\\n\\n<li>Carsten H. Lange: <em>From
        Hannibal to Sulla: The Birth of Civil War in Republican Rome</em> (<em>Studies
        in Ancient Civil War</em>; vol. 1), Berlin: De Gruyter 2024. ISBN 978-3-111-33309-0
        (Print), ISBN 978-3-111-33521-6 (eBook) [<a rel=\\\"noreferrer noopener\\\"
        href=\\\"https://www.degruyter.com/document/isbn/9783111335216/html\\\" target=\\\"_blank\\\">website</a>]</li>\\n\\n\\n\\n<li>Aaron
        Sheehan-Dean, <em>The Calculus of Violence: How Americans Fought the Civil
        War</em>, Cambridge Mass.: Harvard University Press 2018.</li>\\n</ul>\\n\\n\\n\\n<p>Featured
        image (c) Tyler Merbler (<a rel=\\\"noreferrer noopener\\\" href=\\\"https://www.flickr.com/photos/37527185@N05/50820534063\\\"
        target=\\\"_blank\\\">flickr</a>; <a rel=\\\"noreferrer noopener\\\" href=\\\"https://creativecommons.org/licenses/by/2.0\\\"
        target=\\\"_blank\\\">CC BY-ND 2.0</a>)</p>\\n\\n\\n\\n<p class=\\\"has-text-align-left\\\">DOI:
        <a href=\\\"https://doi.org/10.59350/5b9hv-1xz57\\\">10.59350/5b9hv-1xz57</a></p>\\n\\n\\n\\n<div
        class=\\\"wpcp\\\">Cite this article as: Carsten Hjort Lange, \u201CCivil
        War Seen Through the Lens of Ancient History,\u201D in <em>Stasis \u2013 Avenues
        to Ancient Civil War</em>, October 10, 2023, <a href=\\\"https://stasis.hypotheses.org/896\\\">https://stasis.hypotheses.org/896</a>.</div>\\n\\n\\n\\n<div
        class=\\\"wp-block-group alignwide has-background is-layout-flow\\\" style=\\\"background-color:#ffffff\\\"><div
        class=\\\"wp-block-group__inner-container\\\">\\n<div class=\\\"wp-block-group
        is-layout-flow\\\"><div class=\\\"wp-block-group__inner-container\\\">\\n<h2
        class=\\\"wp-block-heading has-text-align-center\\\">From Hannibal to Sulla:
        The Birth of Civil War in Republican Rome</h2>\\n\\n\\n\\n<h3 class=\\\"wp-block-heading
        has-text-align-center\\\">Carsten Hjort Lange</h3>\\n\\n\\n\\n<p class=\\\"has-text-align-center\\\"><em>Studies
        in Ancient Civil War</em> 1, 2024</p>\\n\\n\\n\\n<div class=\\\"wp-block-button
        is-style-outline aligncenter\\\"><a class=\\\"wp-block-button__link wp-element-button\\\"
        href=\\\"https://www.degruyter.com/document/isbn/9783111335216/html\\\" target=\\\"_blank\\\"
        rel=\\\"noreferrer noopener\\\">publisher\u2019s site</a></div>\\n</div></div>\\n</div></div>\\n\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/5b9hv-1xz57\",\"id\":\"24809\",\"image\":\"https://stasis.hypotheses.org/files/2023/09/feature-image-lange_2023.png\",\"language\":\"en\",\"published_at\":1696927158,\"reference\":[{\"key\":\"ref1\",\"url\":\"https://www.degruyter.com/document/isbn/9783111335216/html\"},{\"key\":\"ref2\",\"url\":\"https://www.flickr.com/photos/37527185@n05/50820534063\"},{\"key\":\"ref3\",\"url\":\"https://creativecommons.org/licenses/by/2.0\"},{\"doi\":\"https://doi.org/10.59350/5b9hv-1xz57\",\"key\":\"ref4\"},{\"key\":\"ref5\",\"url\":\"https://stasis.hypotheses.org/896\"}],\"relationships\":[],\"summary\":\"The
        words and concepts we use to describe a particular war speak volumes about
        who we are as individuals and as a community, and where we stand in an unfolding
        conflict. Concepts have always been used to occupy the moral high ground.\",\"tags\":[\"Book
        Launch\",\"Adolf Hitler\",\"Ancient History\",\"Antebellum\",\"Bellum Civile\"],\"title\":\"Civil
        War Seen Through the Lens of Ancient History\",\"updated_at\":1696930192,\"url\":\"https://stasis.hypotheses.org/896\",\"uuid\":\"54eb2ba9-ddc4-4132-9114-439ad52a04ad\"},\"highlight\":{\"authors\":[{\"name\":\"Carsten
        Hjort Lange\",\"url\":\"https://vbn.aau.dk/en/persons/131743\"}],\"content_html\":\"\\n<p
        class=\\\"has-drop-cap\\\">The words and concepts we use to describe a particular
        war speak volumes about who we are as individuals and as a community, and
        where we stand in an unfolding conflict. Concepts have always been used to
        occupy the moral high ground. Rebellion, for example, is used to delegitimise
        those who rebel. In Rome, the allied communities were not legally Roman citizens
        and therefore their rebellion was not a \u2018civil war\u2019. However, these
        non-citizens were considered part of the same polity as Roman citizens; the
        idea of an internal war was born, in debates in the Senate and beyond, debates
        between friends and foes. We debate! And sometimes we use violence, or even
        go to war against each other. It happens now, and it has happened many times
        in the past. The writings of contemporaries (and later sources) reflect these
        debates. From the outbreak of the Second Punic War in 218 to the Social War
        in 91, the Romans of the long second century had protracted debates about
        the changing nature of warfare \u2013 including the rebellion of their allies
        in the midst of the Second Punic War \u2013 conceptual and otherwise \u2013
        culminating in Rome\u2019s first civil war in 88 BCE.</p>\\n\\n\\n\\n<p>At
        its core, my forthcoming book, <em>From Hannibal to Sulla: The Birth of Civil
        War in Republican Rome</em> (<em>Studies in Ancient Civil War</em>, De Gruyter),
        attempts to trace these debates and to show how, over the course of the second
        century, the language of external war was slowly adapted to a new language
        of internal and, ultimately, civil war. It combines two ideas: 1) the formative
        role of antebellum on the one hand, and 2) the importance of the \u2018great\u2019
        war before the civil war on the other hand; in preparing for the next conflict,
        Rome looked back at the last great one. It suggests that the period from the
        Second Punic War constituted, conceptually speaking, an \u2018antebellum\u2019
        period to Rome\u2019s later civil wars. It traces the origins not only of
        the concept and terminology of the <em>bellum civile</em>, but also of related
        terms.</p>\\n\\n\\n\\n<figure class=\\\"wp-block-image size-full\\\"><a href=\\\"https://www.degruyter.com/document/isbn/9783111335216/html\\\"><img
        decoding=\\\"async\\\" loading=\\\"lazy\\\" width=\\\"827\\\" height=\\\"1155\\\"
        src=\\\"https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090.png\\\"
        alt=\\\"\\\" class=\\\"wp-image-1253\\\" srcset=\\\"https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090.png
        827w, https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090-215x300.png
        215w, https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090-358x500.png
        358w, https://stasis.hypotheses.org/files/2023/10/2023-Lange_mark_cov_9783111333090-768x1073.png
        768w\\\" sizes=\\\"(max-width: 827px) 100vw, 827px\\\" /></a></figure>\\n\\n\\n\\n<p>However,
        the book does not really propose a \u2018conceptual history\u2019 (<em>Begriffsgeschichte</em>)
        in the manner of Reinhart Koselleck. The basic idea that, at some point in
        modernity, old words have acquired new meanings so that they no longer need
        to be translated seems overly simplistic, partly because it gives undue weight
        to modernity (it is close(r) to our contemporary world = it is modern = it
        is different from the non-modern world, or so the argument often seems to
        go. I work mostly on old stuff (= a long time ago), but it is just not pre-modern,
        it is very modern, conceptually speaking certainly so). It is true that some
        concepts recur throughout history, but even so we must accept, with Quentin
        Skinner, that they were never immutable and never had a (pre-)determined meaning.
        We can accept that the phenomenon of civil war recurs (with Thucydides\u2019
        description of the <em>stasis</em> in Corcyra during the Peloponnesian War,
        3.81-85) in the understanding that civil wars occur throughout history: Corcyra,
        the late Roman Republic, the late Roman Empire, the English Civil War, the
        American Civil War, and so on. More than anything, my book is about the history
        of the use of concepts rather than a history of concepts.</p>\\n\\n\\n\\n<p>What
        is more, however coherent definitions may be in theory, in practice they were
        as contestable in ancient times as they are today. Can we still believe \u2013
        in the spirit of positivism \u2013 that deciphering the examples from our
        evidence will provide us with a correct basic definition of the concepts we
        are considering? Hardly! We should never accept that civil war is a concept
        that everyone agreed on in ancient times. Language has an unfortunate tendency
        to obscure the extent to which sources disagree, both in ancient times and
        today. Is <em>stasis</em> the same as civil war? <em>Stasis</em> can be a
        <em>polemos</em>, a (civil) war, but it can also be a sedition. The ancients
        simply never agreed on a definition of such slippery concepts as <em>stasis</em>
        and civil war. Why would they? Why would we? Concepts naturally change over
        time as they are used in a particular context.</p>\\n\\n\\n\\n<p>In 43 BCE
        the warmonger Cicero was trying hard to get the proconsul M. Antonius (<em>cos</em>.
        44) declared a <em>hostis publicus</em>. He was strenuously opposed in this
        endeavour by L. Iulius Caesar (<em>cos</em>. 64), who insisted that the term
        <em>bellum</em> be replaced with <em>tumultus</em> (<em>Phil</em>. 12.17):</p>\\n\\n\\n\\n<figure
        class=\\\"wp-block-pullquote\\\"><blockquote><p>I consistently called Antonius
        a public enemy [<em>hostis</em>], while others [L. Iulius Caesar] called him
        an adversary [<em>adversarius</em>]; I consistently called this a war [<em>bellum</em>],
        while others called it a public emergency [<em>tumultus</em>].</p><cite>Cic.
        <em>Phil</em>. 12.17</cite></blockquote></figure>\\n\\n\\n\\n<p>There has
        never been, and never will be, a single narrative. Reading, for example, David
        Armitage\u2019s fine 2017 book <em>Civil Wars. A History In Ideas</em>, it
        quickly becomes clear that the concept of civil war has always been, as I
        said, a slippery one. Importantly, I firmly believe that scholars should ultimately
        accept that we can safely assume that some features of ancient civil wars
        are indeed regular features of any civil war, and that we should therefore
        accept the civil war in ancient Rome as a valid and instructive example to
        consider in modern debates about civil war. This should come as no surprise!
        And it works just fine without history repeating itself.</p>\\n\\n\\n\\n<p>Accordingly,
        all scholars working on civil war\u2014including social and political scientists\u2014need
        to learn their history as well as the traditions of the concepts they apply.
        After all, history is the only laboratory we have. The next question must
        be about the consequences of such an approach.</p>\\n\\n\\n\\n<p>In the book
        I write:</p>\\n\\n\\n\\n<blockquote class=\\\"wp-block-quote\\\">\\n<p>Through
        careful historicization, it is possible to highlight the ahistorical nature
        of modern definitions of civil war. All modern debates about civil war should
        ideally take the Late Republic as their conceptual point of departure: its
        genesis after lies there (a \u201Cwar\u201D between \u201Ccitizens\u201D).
        Alternatively, the concept needs to be abandoned by moderns altogether and
        the concept of \u201Cinternal war\u201D used instead (as famously proposed
        by Eckstein in 1965 [<em>On the Etiology of Internal Wars</em>]). The problem
        with this alternative new concept of \u201Cinternal war\u201D is that this,
        too, was already theorised by the Romans in Latin and is not new at all: <em>bellum
        intestinum</em>, mainly but not exclusively a concept used in connection with
        wars between Rome and its allies or within the polity of allies. The semantic
        range of civil wars is today being stretched to encompass ideological differences
        within for example political parties, so that there is a risk of it losing
        any conceptual specificity</p>\\n<cite>Lange, <em><em>From Hannibal to Sulla:
        The Birth of Civil War in Republican Rome</em></em> (forthcoming)</cite></blockquote>\\n\\n\\n\\n<p>The
        problem today is that the concept of civil war is too often stripped of its
        basic meaning (citizenship and war). Not surprisingly, this is further complicated
        by issues such as the definition of war. When is a war a war? It is safe to
        say that a true civil war requires at least something resembling armies and
        battles, with actual fighting between citizens. In his famous book <em>The
        Logic of Violence in Civil War</em>, Stathis Kalyvas gives us a definition:
        \u201Carmed combat within the borders of a recognised sovereign entity between
        parties subject to a common authority at the beginning of hostilities\u201D
        (2006, 17). His definition not only lacks references to and understanding
        of both <em>war</em> and <em>citizenship</em> (<em>bellum civile</em>), the
        words that make up the concept it defines, but also lacks the exclusivity
        necessary for it to function; it is vague. As I wrote in 2017 in a <a rel=\\\"noreferrer
        noopener\\\" href=\\\"https://cal.library.utoronto.ca/index.php/cal/article/view/28855\\\"
        target=\\\"_blank\\\">review </a>of David Armitage\u2019s 2017 book:</p>\\n\\n\\n\\n<blockquote
        class=\\\"wp-block-quote\\\">\\n<p>The terms \u201Cinclusive\u201D [Kalyvas\u2019
        vague definition] or \u201Cexclusive\u201D [something a kind of conventional
        war and battles between citizens] are often connected to definitions, as if
        we will somehow finally understand a concept by having agreed on its definition.
        Of course we all need definitions, mainly in order to agree that we are talking
        about the same thing (15-18, 219 n.50).</p>\\n<cite>Lange, Review of David
        Armitage, <em>Civil Wars: A History In Ideas</em> (2017)</cite></blockquote>\\n\\n\\n\\n<p>Those
        of us who study civil war must accept that we are studying a spectrum of violence,
        from <em>stasis</em> and <em>seditio</em> to <em>bellum civile</em>: from
        <em>antebellum</em> to <em>bellum</em>. In terms of contemporary relevance,
        what we see today are debates about the naming of conflicts, similar to those
        in ancient times. I write about this in the book:</p>\\n\\n\\n\\n<blockquote
        class=\\\"wp-block-quote\\\">\\n<p>Unsurprisingly, similar debates are also
        part of political landscapes today. On the night of the so-called <em>Capitol
        Riots</em> in 2021, NBC broadcast journalist Savannah Guthrie described the
        situation on live television as a \u201C<em>stasis</em>, for lack of a better
        word\u201D: that is, a period of debilitating and incapacitating civil strife
        or upheaval, taken directly from the Greek expression for \u201Cstandstill\u201D
        (\u03C3\u03C4\u03AC\u03C3\u03B9\u03C2), used in this book in its meaning of
        sedition. We will see in chapter 1 that these so-called riots have themselves
        already been described using numerous different concepts, all related in some
        way to <em>stasis</em>; Guthrie\u2019s comment is the ideal point of departure
        for any discussion about the enduring ambiguity of labels and definitions
        for internal conflict, in the modern world just as in the ancient. <em>Stasis</em>
        and civil war exert so profound an influence that societies must confront
        it in all periods.</p>\\n<cite>Lange, <em><em>From Hannibal to Sulla: The
        Birth of Civil War in Republican Rome</em></em> (forthcoming)</cite></blockquote>\\n\\n\\n\\n<p>Let\u2019s
        play around with a hypothetical scenario: what if the United States were to
        descend into civil war again? Barbara Walter, in her fine book <em>How Civil
        Wars Start. And How to Stop Them</em> (2022), is already ahead of us. According
        to her analysis, incidents such as the plot to kidnap Michigan Governor Gretchen
        Whitmer in 2020, the so-called <em>Capitol Riots</em> in 2021, and the recent
        attack on Representative Nancy Pelosi and her husband in 2022 are indeed examples
        of a potentially unfolding civil war. As such, these events may one day be
        easily described by future historians as part of the \u2018antebellum\u2019
        period leading up to the <em>Second American Civil War</em>, to explain its
        genesis.</p>\\n\\n\\n\\n<p>I am an ancient historian specialising in Republican
        Rome, not a specialist in contemporary American politics, but I will say this
        as a scholar of civil war: Joanne Freeman, in her wonderful book <em>The Field
        of Blood: Violence in Congress and the Road to Civil War</em>, focuses on
        physical violence in the US Congress from 1830 to the outbreak of the Civil
        War (2018). It sheds new light on the systemic breakdown in the decades leading
        up to the Civil War. It allows us to ask whether we can have a relatively
        well-functioning political system and a systemic breakdown at the same time.
        The prelude to the American Civil War, and indeed Late Republican Rome, suggest
        just that; suggest that it is possible. There may be a growing fear \u2013
        with Barbara Walter \u2013 that something similar is afoot in contemporary
        US politics and society. The concept of polarisation is crucial to our understanding
        of current events. Another great book can help us decipher modern US politics.
        Ezra Klein says as much in <em>Why We\u2019re Polarised</em>:</p>\\n\\n\\n\\n<figure
        class=\\\"wp-block-pullquote\\\"><blockquote><p>To appeal to a more polarized
        public, political institutions and political actors behave in more polarized
        ways. As political institutions and actors become more polarized, they further
        polarize the public.</p><cite>Ezra Klein, <em>Why We\u2019re Polarized</em>
        (2020), xix</cite></blockquote></figure>\\n\\n\\n\\n<p>Citing once again from
        my book, \u201CThis could easily be a description of antebellum politics in
        ancient times, of ancient civil strife and civil war language.\u201D</p>\\n\\n\\n\\n<p>Returning
        to the above debate about definitions of civil war, Barbara Walter, similar
        to Stathis Kalyvas, claims that (2022, xiv-xv): \u201CGone are the large battlefields,
        the armies, and the conventional tactics. Today, civil wars are waged by different
        ethnic and religious groups, by guerrilla soldiers and militias, who often
        target civilians.\u201D</p>\\n\\n\\n\\n<p>My answer I give in the book is
        as follows: \u201CThe issue here is, as mentioned, historicity\u2014an awareness
        of the repetitive nature of civil war and its features in earlier periods\u2014which
        is vital for us to compare conflicts over time. Problematically, this historicity
        is lost in her approach. In any case, what Walter discusses and defines as
        civil war would seem to any reader of Thucydides to clearly be not civil war
        but rather <em>stasis</em>. Walter\u2019s basic point regarding an (potentially)
        unfolding civil war can survive this, as this would, if a civil war happens,
        be its antebellum period (as mentioned in the introduction). There is however
        a distinct lack of language and knowledge of history.\u201D I wrote above
        that concepts change over time as they are used in a specific context. But
        this is simply too much and makes any comparison over time difficult or even
        impossible.</p>\\n\\n\\n\\n<p>More than anything else, I fear that the legacy
        of Rome will be lost. Now, I understand that this may be difficult for non-historians
        to accept as a major problem, but in doing so we will ultimately lose the
        ability to understand the legacy of Rome, and we may lose the ability to understand
        the fundamentally repetitive nature of civil war (understanding historical
        similarities as opposed to history repeating itself).</p>\\n\\n\\n\\n<p>Taking
        these issues further, the contemporary unfolding US debate about the 2020
        election and its aftermath, including the so-called <em>Capitol Riots</em>,
        is of great interest to an ancient historian working on civil war and related
        concepts. A debate about which concepts to use to describe what happened and
        what is happening, which happens to be very similar to debates about the changing
        nature of warfare and conflict in ancient times.</p>\\n\\n\\n\\n<p>Former
        President Trump\u2019s incitement to violence (and arguably to a coup) may
        not be a straightforward repetition of past events in every respect, but there
        are certainly patterns. These patterns illustrate the fundamental relevance
        of history to our understanding of the world around us.</p>\\n\\n\\n\\n<p>In
        the end, <em>Capitol Riots</em> \u2013 a violent disturbance of the peace
        by a crowd of people \u2013 seems a poor choice of words; it hardly conveys
        the seriousness of what happened. In the book I write:</p>\\n\\n\\n\\n<blockquote
        class=\\\"wp-block-quote\\\">\\n<p>The period from the 2020 US election onwards
        has more than anything shown that we today lack a nuanced language of internal
        political violence and conflict. Mainstream political discourse channelled
        through mass media lacks a spectrum of concepts related to organised violence,
        or perhaps better, we do not use them. Where attempts are made, they often
        stretch the semantic range of \u2018civil war\u2019 beyond recognition.&nbsp;
        &#8230; When the so-called Capitol Riots of 2021 are included, the term that
        springs to mind more readily than civil war is in fact political violence
        through insurrection\u2014as we shall see further below\u2014and <em>stasis</em>
        or <em>tumultus</em> or similar. But to claim that we might ultimately have
        a Second American Civil War is to invoke the legacy of one million military
        casualties of the American Civil War (1861\u20131865), of which as many as
        three-quarters died (&#8230;). This is about creating \u2013 perhaps even
        subconsciously \u2013 a mental picture of the horrors of not just civil war
        in principle, but the American Civil War in particular. All labels are of
        course political at the outset. We as scholars may agree that a conflict is
        either a civil war or not, but that may not reflect the language used to describe
        the war at the time.</p>\\n<cite>Lange, <em>From Hannibal to Sulla: The Birth
        of Civil War in Republican Rome</em> (forthcoming)</cite></blockquote>\\n\\n\\n\\n<p>But
        this post is not about Trump, not about his norm-breaking presidency, not
        about the US Constitution which presupposes a peaceful transition of power,
        not about \u201Cthe president\u2019s infamous walk to St John\u2019s with
        his entourage, including Secretary of Defense Esper and Chairman of the Joint
        Chiefs of Staff General Mark Milley in camouflage uniform, as well as Attorney
        General William Barr\u201D (from my book), and not about Trump wanting to
        send in the armed forces against protesters.</p>\\n\\n\\n\\n<p>But whatever
        we think of contemporary US politics, the storming of the Capitol on 6 January
        2021 has already changed the USA forever. The <em>House Select Committee to
        Investigate the January 6th Attack on the United States Capitol</em> is a
        testament to an ongoing and still unfolding debate about the so-called <em>Capitol
        Riots</em>, but it is also a testament to a committee that seeks to inform
        and shape future debates. The report suggests that Trump took advantage of
        an unfolding situation of his own making. The evidence seems overwhelming
        to support this conclusion. Trump was plotting to overturn the outcome of
        the 2020 election.</p>\\n\\n\\n\\n<p>This is hardly the place to go into detail,
        but Chapter One is called <em>THE BIG LIE</em>! Suffice it to say that the
        phrase was used by Adolf Hitler in <em>Mein Kampf</em> (1925 Vol. 1, 244)
        to explain how people could be made to believe a colossal lie. Whether the
        comparison is fair or not is not the point. Polarisation is!</p>\\n\\n\\n\\n<p>In
        terms of the language of crisis, the report uses many terms to describe what
        has happened: Terrorist attack (3 times), incite/incitement (20 times, but
        intent 44 times), civil war (20 times), riot(s) (25 times), coup (89 times),
        sedition (87 times; seditious conspiracy (65 times)), insurrection (100 times),
        mob (126 times), conspiracy (200 times). One might have wished for some prioritisation,
        but more than anything this reflects an unfolding debate about what future
        generations might end up calling the so-called <em>Capitol Riots</em>. Suffice
        it to say that <em>stasis</em> is perhaps the best concept to use, at least
        seen from the perspective and through the lens of ancient history.</p>\\n\\n\\n\\n<p>Writing
        about the American Civil War, Aaron Sheehan-Dean (2018, <em>The Calculus of
        Violence. How Americans Fought the Civil War</em>, 44) emphasises, \u201C[b]eyond
        their claims to a defensive and hence just position, the language each side
        used to frame its recourse to war in 1861 shaped how they behaved in the years
        to come\u201D. Translated into relevance today this is certainly food for
        thought. We can learn from history, and we need to accept its relevance today.
        But this can only happen if we accept the historicity of the concepts we use
        to describe contemporary (and past) conflicts. Not only do scholars working
        on the ancient use new words to describe old societies. We also use old words
        to describe modern society.</p>\\n\\n\\n\\n<h2 class=\\\"wp-block-heading\\\">References</h2>\\n\\n\\n\\n<ul>\\n<li>David
        Armitage, <em>Civil Wars: A History In Ideas</em>, New Haven: Yale University
        Press 2017.</li>\\n\\n\\n\\n<li>Ezra Klein, <em>Why We\u2019re Polarized</em>,
        New York: Avid Reader Press 2020.</li>\\n\\n\\n\\n<li>Carsten H. Lange: <em>From
        Hannibal to Sulla: The Birth of Civil War in Republican Rome</em> (<em>Studies
        in Ancient Civil War</em>; vol. 1), Berlin: De Gruyter 2024. ISBN 978-3-111-33309-0
        (Print), ISBN 978-3-111-33521-6 (eBook) [<a rel=\\\"noreferrer noopener\\\"
        href=\\\"https://www.degruyter.com/document/isbn/9783111335216/html\\\" target=\\\"_blank\\\">website</a>]</li>\\n\\n\\n\\n<li>Aaron
        Sheehan-Dean, <em>The Calculus of Violence: How Americans Fought the Civil
        War</em>, Cambridge Mass.: Harvard University Press 2018.</li>\\n</ul>\\n\\n\\n\\n<p>Featured
        image (c) Tyler Merbler (<a rel=\\\"noreferrer noopener\\\" href=\\\"https://www.flickr.com/photos/37527185@N05/50820534063\\\"
        target=\\\"_blank\\\">flickr</a>; <a rel=\\\"noreferrer noopener\\\" href=\\\"https://creativecommons.org/licenses/by/2.0\\\"
        target=\\\"_blank\\\">CC BY-ND 2.0</a>)</p>\\n\\n\\n\\n<p class=\\\"has-text-align-left\\\">DOI:
        <a href=\\\"https://doi.org/10.59350/5b9hv-1xz57\\\">10.59350/5b9hv-1xz57</a></p>\\n\\n\\n\\n<div
        class=\\\"wpcp\\\">Cite this article as: Carsten Hjort Lange, \u201CCivil
        War Seen Through the Lens of Ancient History,\u201D in <em>Stasis \u2013 Avenues
        to Ancient Civil War</em>, October 10, 2023, <a href=\\\"https://stasis.hypotheses.org/896\\\">https://stasis.hypotheses.org/896</a>.</div>\\n\\n\\n\\n<div
        class=\\\"wp-block-group alignwide has-background is-layout-flow\\\" style=\\\"background-color:#ffffff\\\"><div
        class=\\\"wp-block-group__inner-container\\\">\\n<div class=\\\"wp-block-group
        is-layout-flow\\\"><div class=\\\"wp-block-group__inner-container\\\">\\n<h2
        class=\\\"wp-block-heading has-text-align-center\\\">From Hannibal to Sulla:
        The Birth of Civil War in Republican Rome</h2>\\n\\n\\n\\n<h3 class=\\\"wp-block-heading
        has-text-align-center\\\">Carsten Hjort Lange</h3>\\n\\n\\n\\n<p class=\\\"has-text-align-center\\\"><em>Studies
        in Ancient Civil War</em> 1, 2024</p>\\n\\n\\n\\n<div class=\\\"wp-block-button
        is-style-outline aligncenter\\\"><a class=\\\"wp-block-button__link wp-element-button\\\"
        href=\\\"https://www.degruyter.com/document/isbn/9783111335216/html\\\" target=\\\"_blank\\\"
        rel=\\\"noreferrer noopener\\\">publisher\u2019s site</a></div>\\n</div></div>\\n</div></div>\\n\",\"doi\":\"https://doi.org/10.59350/5b9hv-1xz57\",\"reference\":[{\"key\":\"ref1\",\"url\":\"https://www.degruyter.com/document/isbn/9783111335216/html\"},{\"key\":\"ref2\",\"url\":\"https://www.flickr.com/photos/37527185@n05/50820534063\"},{\"key\":\"ref3\",\"url\":\"https://creativecommons.org/licenses/by/2.0\"},{\"doi\":\"https://doi.org/10.59350/5b9hv-1xz57\",\"key\":\"ref4\"},{\"key\":\"ref5\",\"url\":\"https://stasis.hypotheses.org/896\"}],\"summary\":\"The
        words and concepts we use to describe a particular war speak volumes about
        who we are as individuals and as a community, and where we stand in an unfolding
        conflict. Concepts have always been used to occupy the moral high ground.\",\"tags\":[\"Book
        Launch\",\"Adolf Hitler\",\"Ancient History\",\"Antebellum\",\"Bellum Civile\"],\"title\":\"Civil
        War Seen Through the Lens of Ancient History\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}}],\"out_of\":5205,\"page\":1,\"request_params\":{\"collection_name\":\"posts_sep_2023\",\"per_page\":10,\"q\":\"\"},\"search_cutoff\":false,\"search_time_ms\":4}"
    headers:
      Connection:
      - keep-alive
      accept-ranges:
      - none
      access-control-allow-origin:
      - '*'
      content-encoding:
      - gzip
      content-type:
      - application/json; charset=utf-8
      transfer-encoding:
      - chunked
      vary:
      - accept-encoding
    status:
      code: 200
      message: OK
version: 1
