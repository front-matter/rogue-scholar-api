interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.31.0
    method: GET
    uri: https://fmxr36stzdcbiw7hp-1.a1.typesense.net/collections/posts/documents/search?q=&query_by=tags%2Ctitle%2Cdoi%2Cauthors.name%2Cauthors.url%2Csummary%2Ccontent_html%2Creference&filter_by=published_at%3A%3E%3D+1696550400&sort_by=published_at%3Adesc&per_page=10&page=1
  response:
    body:
      string: "{\"facet_counts\":[],\"found\":66,\"hits\":[{\"document\":{\"authors\":[{\"name\":\"Eric
        Gilliam\"}],\"blog_id\":\"qz67b69\",\"blog_name\":\"FreakTakes\",\"blog_slug\":\"freaktakes\",\"content_html\":\"<p><em>This
        piece is a part of a FreakTakes series. The goal is to put together a series
        of administrative histories on specific DARPA projects just as I have done
        for many industrial R&amp;D labs and other research orgs on FreakTakes. The
        goal \u2014 once I have covered ~20-30 projects \u2014 is to put together
        a larger \u2018ARPA Playbook\u2019 which helps individuals such as PMs in
        ARPA-like orgs navigate the growing catalog of pieces in a way that helps
        them find what they need to make the best decisions possible. In service of
        that, I am including in each post a bulleted list of \u2018pattern language
        tags\u2019 that encompass some categories of DARPA project strategies that
        describe the approaches contained in the piece \u2014 which will later be
        used to organize the ARPA Playbook document. These tags and the piece itself
        should all be considered in draft form until around the Spring of 2024. In
        the meantime, please feel free to reach out to me on <a href=\\\"https://twitter.com/eric_is_weird\\\">Twitter</a>
        or email (egillia3 | at | alumni | dot | stanford | dot | edu) to recommend
        additions/changes to the tags or the pieces. Also, if you have any ideas for
        projects from ARPA history \u2014 good, bad, or complicated \u2014 that would
        be interesting for me to dive into, please feel free to share them!</em></p><p><strong>Pattern
        Language Tags:</strong></p><ul><li><p>Pushing forward the technological base
        via speculative machine building contracts</p></li><li><p>Gray coding</p></li><li><p>Incentivizing
        a group of related goals by organizing them all into a single contract</p></li><li><p>Selecting
        multiple industry contractors for a trial period before choosing the final
        contractor</p></li><li><p>Funding a conservative project to use as a measuring
        stick for more ambitious projects in the area</p></li></ul><h1>Introduction</h1><p>DARPA\u2019s
        multi-decade investment in developing early versions of parallel processing
        computers provides a lot to learn from for ARPA-style PMs. Two of DARPA\u2019s
        most prominent, long-standing investments in this area took two completely
        different approaches to push the technology base forward in a coordinated
        fashion. Both are viewed as successes to some and abject failures to others.
        In this piece, I will dive into the operational structures that informed DARPA\u2019s
        investments both in the ILLIAC IV parallel processing computer in the late
        1960s \u2014 the project was based out of the University of Illinois under
        a researcher named Daniel Slotnick \u2014 and in the young Thinking Machines
        Company in the 1980s and their Connection Machine computer \u2014 which brought
        them fame and infamy in the computing world in the years in which DARPA was
        most heavily involved with the company.&nbsp;</p><p>To start, I\u2019ll dive
        into DARPA\u2019s investment in the ILIAC IV. The cost overruns and time delays
        for which this project became internally famous very heavily informed the
        approach that Stephen Squires took to managing DARPA\u2019s investments in
        Thinking Machines and his computer architectures portfolio as a whole in the
        1980s.</p><h1>ILLIAC IV\u2019s Context</h1><p>When DARPA Director Charles
        Herzfeld took over in the mid-1960s, computing research was not nearly as
        pressing of a defense research issue as others like ballistic missile defense
        or nuclear test detection. The previous director, Jack Ruina, often only met
        with the then-IPTO Director J.C.R. Licklider once or twice a month. Now, that
        is in large part because he both trusted Licklider and attempted to have a
        hands-off management style. However, it also speaks to the lack of priority
        that the Information Processing Techniques Office (IPTO) had within the DARPA
        portfolio at the time. PMs in hotter areas of the portfolio were said to have
        quickly met with Ruina as many as five times a day to discuss small issues.&nbsp;</p><p>As
        the reader surely knows, great work was done by Licklider and his portfolio
        of performers in the early 1960s with this approach. As Herzfeld took over,
        the progression of the field was apparent to him and he thought it had the
        potential to help improve programs all across DARPA and the DoD. He recounted
        his approach to managing the IPTO as he took over, saying:</p><blockquote><p>I
        thought that my job vis-a-vis the computing program, when I was Deputy Director
        and Director, was first of all to make sure to get the best office directors
        we could get, and second, to help them do what they thought they needed to
        do, and third, to look for applications across the projects for computing,
        when you apply the new computing ideas, technology and its capabilities as
        widely as possible.</p></blockquote><p>Ivan Sutherland \u2014 Licklider\u2019s
        young, hand-picked replacement who had gotten his Ph.D. at MIT and had been
        at CMU for several years since graduating \u2014 became IPTO\u2019s Director
        under Herzfeld. Sutherland, while young, had been making a name for himself
        in his field. His Sketchpad program, written for his MIT thesis, would later
        help win him a Turing Award.</p><p>Herzfeld also lived up to his word of helping
        good ideas from this office find funding. One example of this was when, in
        1965, Robert Taylor brought forward the network proposal to Herzfeld that
        would become the ARPANET. Herzfeld, thinking the idea had promise, quickly
        determined that ARPA should fund the idea and took some money that had been
        set aside from a lower-priority program \u2014 almost $500,000 (~$5 million
        today) \u2014 to get started immediately. They would get funds from Congress
        to continue the program in the following fiscal year. This kind of administrative
        move, while not rare in managing the DARPA budget at the time, was more likely
        to be done for higher-priority projects.</p><p>As IPTO\u2019s technology base
        was becoming more mature, its budget began to increase and PMs were looking
        to fund projects that were further along in the development process \u2014
        IPTO\u2019s early-1960s budget had been almost entirely allocated to early-stage
        research.<a class=\\\"footnote-anchor\\\" data-component-name=\\\"FootnoteAnchorToDOM\\\"
        id=\\\"footnote-anchor-1\\\" href=\\\"#footnote-1\\\" target=\\\"_self\\\">1</a>
        One development area of interest was the idea of building a more efficient
        computer leveraging hardware that was more parallel in nature. If implemented
        successfully, it was thought that the machine could help the DoD solve many
        pressing problems that required more computing power \u2014 such as calculating
        nuclear weapons effects or real-time processing of radar data.</p><h1>ILLIAC
        IV\u2019s Beginnings</h1><p>In 1965, Sutherland reached out to Daniel Slotnick
        a month after Slotnick had arrived at the University of Illinois to set up
        a visit at Slotnick\u2019s new office in Urbana-Champaign. Slotnick had become
        acquainted with Sutherland while he was working on his SOLOMON parallel computing
        machine at Westinghouse; this area of work at Westinghouse had many DoD ties.
        In their meeting, Sutherland would ask Slotnick if he was interested in developing
        a large parallel computer at Illinois.&nbsp;</p><p>At least, that is how the
        events unfolded according to Daniel Slotnick\u2019s IEEE memoir. Sutherland,
        in an oral history conducted by William Aspray, seems to remember things differently.
        According to his memory, Slotnick came into his office while he was still
        at Westinghouse and funding for his project was drying up, and an interaction
        resembling the following ensued:</p><blockquote><p>He came into the office
        and said,\\\" Basically, I want to do these things and I cannot do it at Westinghouse,
        because they will not let me do it. Where should I go?\\\" I introduced him
        to some of the university folk. So that one was fairly a long time in the
        doing.</p></blockquote><p>This, also, is a quite believable ordering of events
        given my understanding of how things worked at IPTO in the 1960s. Whether
        Slotnick\u2019s account or Sutherland\u2019s is more accurate does not matter
        much. Regardless, in 1965 Slotnick and Sutherland had a conversation about
        building a parallel computing machine \u2014 which they had clearly discussed
        before \u2014 and Sutherland let Slotnick know that this might be a good time
        to submit a proposal now that he was at the University of Illinois. When the
        proposal was submitted, other IPTO staff as well as Herzfeld supported funding
        the proposal.<a class=\\\"footnote-anchor\\\" data-component-name=\\\"FootnoteAnchorToDOM\\\"
        id=\\\"footnote-anchor-2\\\" href=\\\"#footnote-2\\\" target=\\\"_self\\\">2</a>
        In the next few paragraphs describing the details of how the finer points
        of the contract materialized, I\u2019ll rely on Slotnick\u2019s more clear
        account of the events. Certain small details might differ between the two
        accounts, but none that should matter much for the takeaways of this piece.</p><p>The
        project would be quite similar to Slotnick\u2019s work at Westinghouse, and
        it was an area of obvious interest to IPTO\u2019s mission. After Slotnick
        thought it over, he agreed. Sutherland initially proposed starting the project
        with a small study phase. But this tentative approach was not agreeable to
        Slotnick. Arriving at Illinois, the disappointment of the SOLOMON project
        was fresh on Slotnick\u2019s mind. Slotnick\u2019s intensive work on the SOLOMON
        project jointly funded by the DoD and Westinghouse for two years, Slotnick\u2019s
        Westinghouse team steadily grew to around 100 people. In spite of the team\u2019s
        promising early work on the technology, the project\u2019s steady progress
        came to an abrupt halt when the project\u2019s main sponsor in the DoD suffered
        a tragic drowning accident. The man\u2019s replacement did not back the project
        in the same way. Quickly, the project went from pushing for the funds to build
        a full-scale version of the machine to having no funding at all. Not long
        after, Slotnick found himself at Illinois.</p><p>With this disappointment
        fresh on his mind, Slotnick said he was \u201Canxious to do something else.\u201D
        He only wished to pursue the project with DARPA sponsorship if Sutherland
        meant business.&nbsp; Slotnick described his reaction to Sutherland\u2019s
        proposal of a modest seed funding stage as insistent:</p><blockquote><p>I
        absolutely refused. I wanted a $2 million payment at the outset and a contract
        for a total of $10 million. I did this to make sure that the ARPA commitment
        was real and had passed the highest levels of review.\u201D Sutherland must
        have understood. Slotnick continued,&nbsp; \u201CIvan agreed. I wrote the
        proposal and a few weeks later we had our contract.\u201D</p></blockquote><p>The
        proposed machine was heavily inspired by Slotnik\u2019s time spent developing
        the SOLOMON prototype machines. The newly proposed machine \u2014 the ILLIAC
        IV, named after three earlier computers built at Illinois \u2014 sought to
        achieve over one billion operations per second. Beyond the project\u2019s
        aim to produce a machine with a substantial improvement in computing power,
        its management sought to roll out improvements to much of the component technology
        in the machine. When all was said and done, Slotnick and the DARPA team involved
        believed the machine would have several useful areas for military application.
        DARPA and the armed services research organizations planned to deploy the
        resultant machine to help advance research problems in climate dynamics, anti-submarine
        warfare, ballistic missile defense, and the army\u2019s multiple target problem.
        Additionally, a nuclear research office was interested in the capability of
        the computer to help solve the climate modification problem. Lastly, IPTO
        itself was particularly interested in the computer\u2019s applications to
        researchers in cryptanalysis, simulation of fluid flow, seismic array processing,
        economics, and other problems ideally structured to utilize the computer\u2019s
        parallel processing capabilities.</p><p>Sutherland \u2014 as the IPTO Director
        \u2014 was very explicitly attempting to use the ILLIAC IV project to push
        the boundaries of several component fields of computing simultaneously. As
        later-DARPA Director Stephen Lukasik described the ILLIAC IV team\u2019s full
        frontal assault approach, IPTO and DARPA were \u201Cpushing machine architecture,
        software for parallel machines\u2026medium scale integrated circuits \u2014
        and then the application of that to a variety of important defense problems.\u201D</p><h1>ILLIAC
        IV\u2019s Operations</h1><p>In addition to this project helping push forward
        the technology base in the area of parallel computing, Sutherland hoped that
        it could help spark some efforts from industry in building new computer components
        relevant to this general area of technology. So, while the overarching contract
        for the project was issued to the University of Illinois team under Slotnick,
        the work of an industry subcontractor to build the machines the Illinois team
        designed would also be vital to the project\u2019s success. In addition, contracts
        for the research and engineering development of various components of the
        machine would be given out to other parties from the IPTO performer pool.
        However, the work of these additional parties would generally be less important
        to the project\u2019s success than the work of the Illinois team and the computer
        manufacturer chosen to produce the final machine.</p><p>The Illinois team\u2019s
        initial design specified a machine with four modules of 64 processors under
        the control of a single construction unit which would allow them to simultaneously
        work on the same problem. As Slotnick began to assemble a team at Illinois,
        the team also began to figure out which computer manufacturer to partner with.
        Slotnick was not upset by the project\u2019s partnership structure. From the
        beginning, he felt this was a natural approach to doing a project like the
        ILLIAC IV. Slotnick had come from industry and understood the comparative
        advantages of industry and academia when it came to designing and building
        new computers at that time. As Slotnick put it, \u201CThe days when a university
        could design and fabricate a big machine by itself were over, and we decided
        that while we would do the architectural design and most of the software and
        applications work, we would rely on industry for detail design and fabrication.\u201D</p><p>Since
        selecting the right contractor to build the machine was pivotal to the success
        of the contract, Slotnick and the DARPA team held a competition. Slotnick
        and the DARPA team outlined the major approaches and incorporated them into
        a bid set. In August 1966, after \u201Cmany months of intensive contacts with
        industry,\u201D RCA, the Burroughs Corporation, and Univac were all awarded
        8-month contracts to continue working on the early stages of the contract
        and collaborating with the Illinois team. After this period, in 1967, the
        Burroughs Corporation was selected as the best fit to continue on as a partner
        in building the ILLIAC IV. Burroughs had partnered with Texas Instruments
        in its bid. The plan was for TI to develop the integrated circuits for the
        machine\u2019s all-important process element (PE) array. With that trial period
        over and the DARPA and the Illinois teams having gotten the chance to work
        with all three companies for a time, the industry partner for the project
        was decided upon.   </p><p>Either Larry Roberts or Robert Taylor \u2014 Taylor
        had become the new IPTO Director in the intervening period and Roberts was
        the PM who focused on the ILLIAC IV project \u2014 and the Illinois team awarded
        the related contracts to work on other component parts of the ILLIAC IV to
        the following non-U of I teams:&nbsp;</p><ul><li><p>The design and implementation
        of the FORTRAN compiler for the ILIAC IV to the Applied Data Research Corporation</p></li><li><p>Various
        hardware and software to help link the ILLIAC IV and a trillion bit memory
        with the ARPANET to the Computer Corporation of America</p></li><li><p>And
        work on the development of the interactive front end processor to BBN.</p></li></ul><p>As
        the project progressed, the design and construction phase was riddled with
        research problems and setbacks that forced design changes on the research
        team, delays, and cost overruns. Two main issues contributed to the early
        cost overruns and time delays in the first couple of years of the project.
        The first was TI\u2019s inability to produce the 64-pin emitter-coupled logic
        (ECL) packages. This caused a delay of over a year and a $4 million cost overrun.
        Slotnick documented that TI was not exactly forthright in taking the blame,
        writing, \u201CA great deal of inconclusive finger-pointing went on between
        TI and its suppliers.\u201D Regardless, the problem \u2014 and loss of cash
        and time \u2014 remained. The second major problem was Burroughs\u2019 inability
        to make workable magnetic thin-film PE memories for the evolving design of
        the machine. This problem caused an additional year of delay and another $2
        million cost overrun. Additional problems existed with packaging, circuit
        design, and interconnections for the large machine. But these two problems,
        in the early years of the project, caused the most headaches.</p><p>Slotnick,
        in his writings, explains how these very early setbacks began to affect project
        planning. Certain goals became more modest and the team had to substitute
        in and out certain component technologies to fit in better with the evolving
        design and changing circumstances. Slotnick writes:&nbsp;</p><blockquote><p>We
        retreated from the 64-pin packages to standard 16-pin dual in-line packages.
        In doing so, however, everything got bigger and more expensive. A lot of logic
        design was salvageable as a consequence of making the 16-pin packages logically
        derivative of the abandoned 64-pin packages, but board layout, back-panel
        wiring, and all system level hardware, of course, had to be restarted from
        scratch. The memory situation was even messier. Burroughs had a large investment
        in its thin films and didn\u2019t want to give up on them even after my own
        independent review had concluded they still represented an intolerable development
        risk. Semiconductor manufacturers were just beginning to gear up for memory
        chip manufacturing; the manufacturing means were clearly at hand, or at least
        so it seemed to me, but the chips were not. I made the painful decision to
        drop films and go with semiconductors.</p></blockquote><p>In making this semiconductor
        pivot, Slotnick believed the young Fairchild Semiconductor was easily the
        most qualified supplier. While Burroughs railed against the selection of the
        young company, a panel of independent ARPA experts \u2014 assembled due to
        Burroughs\u2019 \u2018furor\u2019 at Slotnick\u2019s selection of Fairchild
        \u2014 confirmed the decision. Fairchild seemed to be one of the only \u2014
        if not the only \u2014 suppliers delivering high-speed semiconductor memory
        at the time.&nbsp;</p><p>As the project wore on, the projected cost of the
        project roughly tripled and Slotnick was now only promising to deliver a system
        a fourth the size of the one originally promised. In spite of these shifts,
        Larry Roberts \u2014 who would become IPTO Director himself and who was already
        known as the \u201Cfather of computer networks\u201D \u2014 remained behind
        the project. Not only did Roberts, as Slotnick jokingly put it, share in his
        \u201Csick attachment to really big pieces of hardware,\u201D he was very
        interested in the list of applications which the machine was set to work on
        \u2014 including numerical weather prediction, sonar, radar, seismic signal
        processing, and an assortment of other computations array computers tended
        to do well. But, as Slotnick saw it, to justify the escalating costs, making
        the machine available to as large a community of users as possible became
        even more essential. The larger the scale of users the machine achieved, the
        more justified the escalating costs would be. Slotnick believed that this
        was a major factor that drove making the ILLIAC IV easily available over the
        ARPANET to be a much stronger point of emphasis for the project as the years
        wore on.</p><p>In 1970, five years into the constantly delayed project, the
        Illinois team was finally beginning to hone in on a workable machine. But,
        even then, there was still an element of instability in the project\u2019s
        operations. From a technical standpoint, things were beginning to get on more
        sure footing. The PE had been successfully redone with the new TI integrated
        circuits, PE memory chips were being delivered by Fairchild, and other pieces
        of equipment were getting produced on schedule. However, cultural conflicts
        were now coming to a head on the Illinois campus. Not only were stresses between
        Slotnick and those faculty not involved with the million-dollar-a-month project
        being run out of an academic department coming to a head, but Vietnam War-era
        turmoil from the students was making it more difficult to peacefully continue
        the defense-funded project on campus. In the midst of all of these non-scientific
        issues, the project was first moved from within the academic department to
        become its own free-standing center at the university. Then, in January 1971,
        ARPA decided to move the system to the NASA Ames Research Center when it became
        feasible. The move was carried out by Burroughs in 1972 and may have happened
        earlier in the technological development process than DARPA would have preferred.</p><p>Hardware
        problems plagued the machine in its early years at Ames. Early circuit batches
        failed at high rates and in ways that were often difficult to detect. Additionally,
        many of the back-panel connections and terminating transistors functioned
        poorly. While successful runs were made as early as 1973, the team at Ames
        did not have the machine working in a way that could be considered reliable
        until 1975. However, even after the machine was completed, one problem continued
        to cause severe headaches: producing software to run on the machine. This
        would be an area of lasting difficulty that plagued the machine for its entire
        lifetime \u2014 which was admittedly not so long. The difficulty in writing
        software for the machine would be remembered when the next generation of IPTO
        leaders sought to push the area of parallel computing to the next level.</p><p>The
        ILLIAC IV would only be used until about 1981 when NASA replaced the machine
        with a more reliable and easy-to-use successor</p><div class=\\\"captioned-image-container\\\"><figure><a
        class=\\\"image-link is-viewable-img image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg\\\"
        width=\\\"500\\\" height=\\\"341\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:341,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:146356,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1456w\\\" sizes=\\\"100vw\\\" loading=\\\"lazy\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">Three
        men installing a chassis or module onto the ILLIAC IV. Photo provided to the
        Computer History Museum by the Burroughs Corporation. https://www.computerhistory.org/collections/catalog/102651989</figcaption></figure></div><h1>ILLIAC
        IV\u2019s Results</h1><p>By the time ILLIAC IV was finally constructed, the
        total cost was $31 million (over $200 million today) \u2014 substantially
        higher than the original, expected price tag of $8 million. Beyond the cost
        problem, the final machine only had one quadrant of 64 processors rather than
        the originally planned four. Several of the many experimental components of
        the machine could not be made to work and existing components had to be used
        in their place. The resulting machine only achieved one-twentieth of the computing
        power that the researchers originally sought to achieve \u2014 the initial
        goal was one billion operations per second. Also, even though the ILLIAC IV
        was considered finished in 1972, much of the 1972 to 1975 period was spent
        in an intensive effort to correct problems and improve the reliability of
        the machine as researchers affiliated with the armed services were attempting
        to use the machine for practical applications.&nbsp;</p><p>Notwithstanding
        the setbacks, once the ILLIAC IV became operational in 1975, it was thought
        to be the fastest machine in the world for certain types of problems \u2014
        calculations of fluid flow being one example. In the late 1970s, the computer
        was used in service of many of the armed services and DARPA applications on
        which it was intended to be used. This includes the army\u2019s multiple target
        problem, research in cryptanalysis, seismic array processing, economics, and
        more. For the few years in which the machine was in operation, it may have
        been the best computer in the world for solving problems whose calculations
        could be performed in an \u201Call at once\u201D manner and were, thus, ideally
        suited to being solved by a parallel processing computer.&nbsp;</p><p>From
        the standpoint of developing the technology base, the component technology
        of the computer helped improve many pieces of the technology base. Some of
        the prominent examples:</p><ul><li><p>The first large-scale computer to employ
        emitter-coupled logic in its processors \u2014 taking the place of transistor-transistor
        logic. This technology would go on to be used in many high-speed computers
        in subsequent years.&nbsp;</p></li><li><p>Its circuit boards were the first
        to be designed with the aid of a computer. Computer design automation is now
        widely used in industry.</p></li><li><p>The design of its storage technology
        \u2014 which consisted of 64 disks that could be read from and written to
        concurrently \u2014 led to higher speed input/output.</p></li><li><p>The machine\u2019s
        laser memory system as an additional, tertiary memory had a capacity in the
        trillion-bit range and read-in and read-out rates in the million bits per
        second range.</p></li><li><p>ILLIAC IV\u2019s architecture employed a single
        instruction stream to control the multiple data streams involved in interprocessor
        communication.</p></li><li><p>ILLIAC IV was the first large system to employ
        a semiconductor primary store.</p></li></ul><p>On top of all of the new functionality,
        the program also succeeded in (eventually) producing a machine built by a
        joint industry-university collaboration. Slotnick and the Illinois team did
        not believe they\u2019d have been better off working on the project with no
        help from industry. In fact, Slotnick later reflected that he believed running
        such an operation with him and the university as primary contractor could
        have been a mistake \u2014 or something to learn from at the very least. He
        referred to trying to administratively manage such a technically complex operation
        with that sort of headcount, scale, budget, and operational needs as having
        \u201Call the sense of trying to build&nbsp; a battleship in a bathtub.\u201D</p><div
        class=\\\"captioned-image-container\\\"><figure><a class=\\\"image-link is-viewable-img
        image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg\\\"
        width=\\\"500\\\" height=\\\"392\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:392,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:52841,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1456w\\\" sizes=\\\"100vw\\\" loading=\\\"lazy\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">The
        battleship in the bathtub. Engineer Jay Patton is at the oscillosope. The
        two men kneeling are testing one of the boards on a test machine. Photo provided
        to the Computer History Museum by UIUC. https://www.computerhistory.org/collections/catalog/102651994</figcaption></figure></div><h1>ILLIAC
        IV\u2019s Lessons Learned (and Caveats)</h1><p>Joining all of these non-obvious
        developments into a project meant to produce a single, useful machine caused
        substantial time delays and cost escalation. But the field of computing got
        more than nothing in return for the tradeoff. The ambitious design of the
        machine created a technology \u201Cpull\u201D and stimulated the development
        of new component technologies that had the potential to find broad use.&nbsp;</p><p>For
        example, when the project ran into issues incorporating the originally planned
        film memories into the design, Slotnick turned to the nascent Fairchild Semiconductors
        to design the project\u2019s all-important memories to fit with its new design.
        The new design was seen to be quite risky since it required advances in semiconductor
        art, engineering design, and production. But Slotnick was convinced the Fairchild
        team could do the work. This confidence may have come from the fact that Slotnick\u2019s
        old mentor, Rex Rice, headed memory system operations at Fairchild. It also
        may have been due to Fairchild\u2019s rapidly growing reputation as the new
        provider of semiconductors to the NASA program and certain Air Force programs.
        Regardless, in the end, Slotnick was right to put his faith in the Fairchild
        team because they successfully produced the required memory chips and the
        ILLIAC IV was the first to use chips of this design. Some believe that this
        project helped speed up the pace with which semiconductor memories became
        commercially available \u2014 although given the field was growing so quickly,
        it is hard to tell if such a claim is true.&nbsp;</p><p>Taking a step back,
        it is reasonable to question whether or not the ILLIAC IV\u2019s approach
        is the ideal strategy to orchestrate new technological developments. As always,
        the suitability of the approach depends on the situation and what the PM thinks
        is appropriate given the context. Developing so many new pieces of component
        technology to use, for the first time, in one machine seems to leave one obviously
        more prone to cost overruns and time delays than most projects. The upside
        is that the approach provides new funding and a focused direction to help
        bring what could become very useful component technology into existence. In
        the best case, the approach can push a field forward much faster than if one
        had taken a more measured approach. But the approach does also run the substantial
        risk of generating demand for and funding pieces of component technology that
        might be useful to the current project, but will not be useful to the field
        as a whole in later years. For example, if the components required by the
        design of the machine are not reflective of the needs of the field as it progressed
        outside of that individual project. </p><p>In essence, the&nbsp; approach
        of the ILLIAC IV team should be considered a high-risk, high-reward strategy.
        It probably won\u2019t achieve outcomes as stable as those of the gray coding
        approach covered in the following Connection Machine sections, but there are
        still situations where the full frontal assault approach of the ILLIAC IV
        has a high enough expected value to justify the risks. With this approach,
        one may run the risk of cost overruns, delays in producing a working machine,
        and developing components that the broader field turns out to not need. But
        the approach also might help a technological area make discontinuous leaps
        in component technology and overall system performance that would not happen
        by changing one component at a time. Particularly in the early days of a technological
        field, changing multiple design components at once is often an attractive
        approach given that existing approaches might be extremely far from what researchers
        know to be theoretically possible.<a class=\\\"footnote-anchor\\\" data-component-name=\\\"FootnoteAnchorToDOM\\\"
        id=\\\"footnote-anchor-3\\\" href=\\\"#footnote-3\\\" target=\\\"_self\\\">3</a>
        (See this earlier <a href=\\\"https://www.freaktakes.com/p/a-report-on-scientific-branch-creation\\\">FreakTakes
        piece</a> on the field of early molecular biology to read more about this
        strategy.)</p><p>Depending on the political and technical context of the project,
        the full frontal assault approach might be just what is required. But, in
        other situations, it might be considered far too risky. Stephen Squires, with
        his computer architectures program in the more mature DARPA computing portfolio
        of the 1980s, considered the approach far too risky.</p><h1>Connection Machine\u2019s
        Beginnings</h1><p>While the final ILLIAC IV machine did work, Squires took
        the management of the program as a cautionary tale of what not to do when
        developing machines within IPTO. In his eyes, the cost overruns and delays
        in producing the machine were entirely foreseeable given the structure of
        the project. Squires believed the \u201Cfull frontal assault\u201D approach
        of the ILLIAC IV project \u2014 with novel approaches to the machine\u2019s
        architecture, processors, memory, storage, and other components \u2014 pushed
        too many untried technologies at once. Problems in developing or implementing
        any one of the components could negatively impact the entire project. In the
        uncertain enterprise that is research, Squires believed this was an untenable
        risk profile.&nbsp;</p><p>Squires wanted to encourage projects funded through
        his architectures program to limit the number of new technologies a project
        worked on at a time \u2014 usually to one. He called the approach \u201Cgray
        coding.\u201D Roland and Shiman \u2014 who wrote the most extensive history
        on DARPA\u2019s Strategic Computing Initiative of the 1980s and extensively
        interviewed Squires \u2014 expanded on the thinking behind the approach as
        follows:</p><blockquote><p>That way, problems could be easily traced, isolated,
        and corrected, and the chances of catastrophic failure of the system would
        be minimized if not eliminated. Advance of the field would not occur in one
        fell swoop by \u2018frontal assault\u2019 of some \u2018point solution,\u2019
        but through the selected development and installation of key technologies,
        all well-timed and well-coordinated. Advance would occur over a series of
        generations. Once a technology or key concept had proven reliable, a new component
        or design would be implemented and tested.</p></blockquote><p>Squires portion
        of DARPA would still pursue several technological trajectories at once \u2014
        which Squires called the ensemble model \u2014&nbsp; to allow the research
        ecosystem to prove out which one held the most promise.&nbsp;</p><p>Additionally,
        learning from the ILLIAC IV example and his own experience working on systems
        software at the NSA, Squires insisted that software development be emphasized
        as much as hardware work for the architectures being developed. In the case
        of the ILLIAC IV, the development largely focused on getting the hardware
        working and later shifted efforts towards generating useful applications via
        software development. Many believe the ILLIAC IV team emphasizing the hardware
        first and software later was a large part of what took the machines so long
        to finally get working at the back half of the project. Squires was going
        to do things differently.</p><p>Squires planned for the architectures program
        to progress in three phases \u2014 each lasting roughly three years. The projects
        funded in the first phase would generally seek to refine promising architecture
        ideas and develop microelectronics components that would later be used in
        building parallel machines. Projects funded in the second phase would generally
        seek to build full prototypes of the most promising architectures. The projects
        funded in the third phase would integrate technology that now (hopefully)
        existed into composite systems that could support DARPA\u2013relevant applications
        \u2014 such as military applications in computer vision, natural language
        understanding, simulations, and more. Squires hoped that the machines being
        developed would achieve 100 million floating point operations per second by
        1987, a billion by 1989, and a trillion by 1992. In service of this goal,
        all of the early work on smaller-scale prototypes had to be easily scalable
        \u2014 for example, being designed in such a way that they could be easily
        expanded by simply increasing the number or power of processors in the prototype.
        As all of this work progressed, work on new programming languages, compilers,
        tools, and concepts that better served parallel programming would be also
        funded.</p><p>With the high level plan in place, Squires began by traveling
        around and further familiarizing himself with the pool of potential performers
        that might be funded and meeting with those in industry \u2014 such as computer
        manufacturers. In addition, this familiarizing process also entailed a February
        1984 call for qualified sources asking for potential contractors to explain
        what they could do in the areas of signal, symbolic, or multifunction processing
        if they were funded by Squires\u2019 architectures program.</p><p>Squires,
        at the time, primarily issued the formal solicitation for informational purposes
        \u2014 as a way to see what was out there, not necessarily planning to immediately
        award any contracts to the proposals submitted. But one proposal jumped off
        the page from the pool of submissions \u2014 two-thirds of which from industry
        and one-third from academia. That proposal was a big black notebook sent in
        by a newly formed company called Thinking Machines Corporation (TMC) outlining
        their technical and business plans for a computer called the Connection Machine.</p><h1>Connection
        Machine\u2019s Operations</h1><p>The proposed machine was exactly what Squires
        was looking for. The machine the company proposed was an ambitious, truly
        parallel hardware architecture that seemed like it could scale up by simply
        adding more or better processors. In spite of the ambitious design, Squires
        still saw the approach as conservative \u2014 in a positive way. Not only
        had the Thinking Machines Corporation (TMC) already developed a workable VLSI
        design for the processor chips \u2014 which was one notable source of technological
        risk already mitigated \u2014 but the company also set out to construct the
        architecture using as many commonly used components as possible. In fact,
        in several areas they went as far as to use components that were far simpler
        than the ones commonly used in other machines at the time. One example is
        that, for its first machine, TMC planned to use the simplest 1-bit processors
        instead of the more powerful 4, 8, or 16-bit processors commonly in use at
        the time. In addition, the Connection Machine would not even be a stand-alone
        machine in the early iterations. The front end used to access the machine
        would be a more commonly used computer \u2014 a LISP machine in this case.
        In this early stage, SC\u2019s main goal was to demonstrate the feasibility
        of basic parallel processing concepts while not adding needless complications
        to projects that did not serve the main goal. TMC \u2014 maybe not surprisingly
        since its founder came from a lab that was quite used to working with DARPA
        PMs \u2014 clearly got the memo and submitted a proposal that perfectly complied
        with the gray coding approach while maintaining an ambitious vision.</p><p>After
        some meetings between DARPA staff and TMC executives, it seemed clear to DARPA
        that TMC as a venture was well thought out from both the business and technical
        perspectives. In early 1984, $3 million (~$9 million today) was approved to
        fund TMC\u2019s work on a small prototype of the machine containing 16k processors
        \u2014 with the option for an additional $1.65 million to scale up the prototype
        to 64k processors. Raising an additional $16 million in private investments
        around this time from private investors, TMC was well-capitalized and ready
        to begin work on its first machine.</p><p>Even having laid out a plan of action
        that complied with Squires\u2019 gray coding framework, the work was daunting
        for the company. Danny Hillis, the CEO of TMC who came up with the idea for
        the Connection Machine during his Ph.D. under Marvin Minksy at MIT, said the
        early work building the simplified version of the Connection Machine underway
        in 1984-1985 was \u201Coverwhelming.\u201D He continued, describing the early
        work of the company in a 1989 <em>Physics Today</em> article:</p><blockquote><p>We
        had to design our own silicon integrated circuits, with processors and a router.
        We also had to invent packaging and cooling mechanisms, write compilers and
        assemblers, devise ways of testing processors simultaneously, and so on. Even
        simple problems like wiring the boards together took on a whole new meaning
        when working with tens of thousands of processors. In retrospect, if we had
        had any understanding of how complicated the project was going to be, we never
        would have started.</p></blockquote><p>This reflection from Hillis probably
        lends some credibility to Squires\u2019 gray coding approach having been a
        good fit for this particular project. The design and need to develop new software
        approaches for making use of the novel design would also bring with it more
        difficulties. There was no need to compound on these difficulties by ensuring
        the initial prototype had the most cutting-edge component tech possible. Novel
        engineering development projects like this were sure to run into plenty of
        issues that required novel solutions to overcome. As Hillis stated, they ran
        into so many of these in the early days that they might not have even undertaken
        the project had they understood how painful it would be. As the project grew
        more complicated, Hillis seemed thankful to have Manhattan Project alum Richard
        Feynman around as an intern for some summers \u2014 Feynman\u2019s son Carl
        had worked with Hillis when Hillis was a grad student and Carl was an undergrad
        at MIT. Feynman seemed relatively influential in pushing the company to take
        a page out of the Manhattan Project\u2019s management playbook and designate
        a group lead in each area of technology \u2014 such as software, packaging,
        electronics, etc. Feynman \u2014 who was initially brought on to help brainstorm
        applications for the computer \u2014 also pushed the company to hold regular
        seminars of invited speakers, often scientists, who the company believed might
        have interesting use cases for the machine.</p><p>While the project did have
        its difficulties in the early years, in spite of Hillis\u2019s statement about
        the project being \u201Coverwhelming,\u201D the project was considered the
        gem of the architectures portfolio in the early years of SC. Squires\u2019
        program had also invested in other prototype-stage projects for at least a
        two year trial stage \u2014 which all had some positive developments as well
        as some downsides. Each of the general-purpose, prototype-stage projects was
        meant to help in the architectures program\u2019s larger goals of identifying
        promising approaches to parallel computing and providing experience working
        with the technology. Simultaneously, smaller projects called \u201Caccelerators\u201D
        were funded with the goal of doing development work to produce components
        that could later improve the performance of the general-purpose machines.
        Besides the Connection Machine, some of the architectures program\u2019s other
        general-purpose prototype projects included:</p><ul><li><p>BBN\u2019s Butterfly
        computer series</p><ul><li><p>The Butterfly computer had been in the DARPA
        funding pipeline since 1977, with funding going to one of DARPA\u2019s workhouse
        computing research contractors, BBN. At the start of the architectures program,
        a model of the Butterfly with 10 processors already existed. BBN received
        funding to continue to scale the prototype to 128 processors. The butterfly
        computer was a coarse-grained shared-memory machine \u2014 coarse-grained
        machines have fewer but more powerful processors capable of running entire
        programs in a moderately parallel but not truly parallel fashion. This technology
        was both further along in development than smooth-grained parallel computers
        \u2014 like the Connection Machine \u2014 were. Additionally, machines like
        the Butterfly could easily make use of traditional programming methodologies
        rather than developing new ones. For those reasons, the Butterfly was meant
        to serve as the benchmark against which progress on other machines in the
        portfolio would be measured.&nbsp;</p></li></ul></li><li><p>CMU\u2019s Warp</p><ul><li><p>The
        architectures program also contracted CMU to build Warp. Warp was a systolic
        array machine that sought to achieve more efficient performance using an approach
        in which processors are connected sequentially and data flows from one processor
        to the next with each performing a different operation \u2014 analogous to
        an assembly line. CMU had received this contract based on its prior demonstration
        of the systolic approach having built a programmable systolic chip.</p></li></ul></li><li><p>Columbia\u2019s
        DADO</p><ul><li><p>DADO was designed as a coarse-grained parallel machine.
        DADO was one of two tree-structured machines \u2014 in which a central processing
        unit was connected to two others, and each of those to two others, etc. \u2014
        started at Columbia University in 1981. DARPA discontinued funding for this
        project after the early prototype stage.</p></li></ul></li><li><p>Columbia\u2019s
        Non-Von</p><ul><li><p>The second of Columbia\u2019s two tree-structured machines,
        the Non-Von, was somewhere in between the DADO and the Connection Machine
        in terms of how fine-grained it was. A machine like this one would likely
        prove simpler to program than a Connection Machine, but would also not achieve
        quite the level of parallelization. The Non-Von sought to prove useful in
        managing and manipulating large databases. DARPA also discontinued funding
        for this project after the early prototype stage.</p></li></ul></li><li><p>TI\u2019s
        compact LISP machine</p><ul><li><p>TI sought DARPA funding to fund its development
        of a miniature version of a LISP machine. To do this, TI planned to implement
        LISP processors onto individual chips. TI hoped that the entire machine would
        fit on nine small printed circuit cards and, in the end, would be small enough
        to be embedded in computers of military systems such as within a pilot\u2019s
        cockpit to help in expert decision making. DARPA agreed to the $6 million
        in hardware costs and TI, sharing the costs, funded the $6 million in software
        development.</p></li></ul></li></ul><p>But none of these projects seemed to
        carry as much promise as the Connection Machine and its model of performing
        identical operations simultaneously across its many, many processors. However,
        each of the other machines did often have at least one piece of component
        technology in development that DARPA was intrigued to pursue further. For
        example, the Non-Von project developed a way of using multiple disk heads
        in operating its storage system. TMC later incorporated this idea into one
        of its later prototypes \u2014 all SC-funded developments were eligible to
        be used by other SC contractors. This is just one case of discontinued portfolio
        projects still finding help push the technology base forward in Squires\u2019
        portfolio.</p><p>As the Connection Machine project got underway, Hillis had
        arranged for DARPA funding to install a LISP machine at the chip foundry TMC
        worked with in California. With that machine installed, TMC could test chips
        at the foundry rather than having them shipped to Massachusetts to test them
        there. By mid-May 1985, a little over a year after TMC had received its first
        DARPA grant, the 16k processor prototype had been finished \u2014 more than
        a month ahead of schedule. After DARPA checked out the machine in the TMC
        offices, DARPA immediately invoked the option for the 64k processor machine.
        This scaled-up prototype of the machine, too, would be finished ahead of schedule
        \u2014 by the end of 1985. Hillis\u2019 design seemed to be scaling the way
        he and DARPA had hoped. And for all of the sub-problems that the initial design
        could not account for, the very talented (and growing) TMC research team \u2014
        largely drawn from the staff and students of MIT, CMU, Yale, Stanford, and
        other elite pools of computer engineering development talent \u2014 was proving
        more than up to the task of solving.&nbsp;</p><p>By the spring of 1986, the
        first Connection Machine \u2014 the CM1 \u2014 was being manufactured and
        was available for sale. The machine\u2019s chips each contained 16 customized
        1-bit VLSI processors. Thirty-two of these chips would be printed on a circuit
        board and mounted next to each other \u2014 eventually making up four separate
        quadrants of 16k processors. Each quadrant could stand alone as a cheaper,
        scaled-down model of the 64k processor machine. In spite of the novelty of
        the truly parallel design of the architecture, each of the components remained
        quite safe and reliable to use \u2014 as the gray coding approach intended.
        The 1-bit processors were basic and slow compared to the state-of-the-art
        processors at the time, but they also produced so little heat that the machine
        did not require much cooling. Installation of the machine on a buyer\u2019s
        site was easy and could be readily set up with a LISP machine on-site \u2014
        which customers knew how to use. However, customers did have to learn new,
        Connection Machines-workable, versions of C and LISP to make full use of the
        machine\u2019s capabilities while programming. The user did not need to fully
        understand how memory assignment and other aspects of the machines worked
        with these languages, but learning the languages was a necessity.</p><p>With
        the project going well, in the beginning of 1986 DARPA funded TMC for an additional
        $3.8 million over two years to find ways to exploit the CM1 and CM2 on various
        scientific and military applications. While the Connection Machines were still
        in the process of becoming truly useful machines, this particular generation
        of DARPA\u2019s computing programs carried a strong political emphasis on
        pushing successful research projects into application mode as early as possible.
        This iteration of DARPA funding for TMC included covering work on projects
        to further improve software development practices for the machine, develop
        the machine so it could be used as a network\u2019s server, increase storage
        capabilities to aid in work on data-intensive problems, and developing a training
        program to help others learn to use the machine. Beyond this $3.8 million,
        DARPA would also serve as TMC\u2019s biggest buyer in the coming years.&nbsp;</p><p>With
        this DARPA funding, the work continued on into more mature prototyping and
        scaling stages. As of 1987, around twelve CM1s had been purchased from TMC
        \u2014 about half of them purchased by DARPA for its contractor community.
        The machines were typically purchased for around $5 million a piece in 1987
        (~$14 million in 2023). As the TMC team sought new applications for its machines,
        under Hillis\u2019 leadership, they eagerly sought out projects with many
        in the research community. This included partnering with physicists, astronomers,
        geologists, biologists, and chemists to understand the details of their applications.
        Someone like Feynman worked in application areas at TMC such as problems in
        database searches, geophysical modeling, protein folding, analyzing images,
        and simulated evolution. Establishing partnerships with researchers proved,
        unsurprisingly, quite natural to the organization. It should be remembered
        that, during his graduate work, Hillis designed the machine to be something
        like an optimal tool for researchers studying artificial intelligence \u2014
        as well as other complex research problems. People like himself, his former
        lab mates in Minsky\u2019s lab, and other researchers from similar environments
        were the kinds of people who staffed the company as well as its initial, intended
        customers.</p><p>The company did not prove nearly as eager to establish profitable
        partnerships with industry as it did partnerships with researchers. Of course,
        not all of the problems the company explored applying the machine to were
        research related. For example, Feynman also worked on one application area
        related to reading insurance forms. However, this sort of application work
        did seem to be a less natural fit for Hillis and the company.&nbsp; Hillis,
        after all, saw the Connection Machine as a machine for science. In practice,
        TMC very clearly preferred not to make compromises to the goal of pushing
        science forward in search of increased revenue. For now, DARPA was TMC\u2019s
        guardian angel when it came to buying machines, helping broker sales with
        research consumers of the machines that DARPA didn\u2019t buy itself, and
        funding the company\u2019s R&amp;D work.</p><p>The CM1 \u2014 which TMC largely
        relied on DARPA to facilitate purchases for \u2014 did not only have limited
        market potential outside of the DARPA universe because of the price tag of
        its machines, but also, simply, because of what they just couldn\u2019t do.
        From a consumer standpoint, the first major deficiency was that the machines
        could not yet run the standard computer language of the scientific community:
        FORTRAN. The second major deficiency was even though the CM1 could perform
        ten times as many computations per second as many contemporary, commercial
        supercomputers \u2014 2.5 billion operations per second vs. a few hundred
        million \u2014 the 2.5 billion operations it could do per second were not
        floating-point operations. And floating-point operations were vital to most
        computationally intensive research problems at the time. Nevertheless, DARPA
        likely felt comfortable facilitating these purchases because much of the SC
        program emphasized folding other SC-developed component technologies into
        projects sooner rather than later. At the beginning of SC, DARPA had also
        facilitated the purchases of the young BBN Butterfly computer to some of its
        contractors in order to ensure they had the most up-to-date technology available
        to them and their development work. So, TMC was not the only contractor in
        the architectures portfolio to have received this sort of favorable treatment
        from DARPA.</p><p>With the deficiencies of the CM1 in mind, TMC got to work
        incorporating the clearly needed functionality into its CM2. Surely this lack
        of functionality, at this stage in the development process, was not considered
        a failure by any means. The first successful prototype \u2014 the CM1 \u2014
        lacking much needed functionality from the user\u2019s perspective was likely
        just a symptom of the gray coding approach. The project was likely unfolding
        as Squires hoped successful architectures projects would \u2014 in successive
        generations. By this time, the architectures program, headlined by TMC,&nbsp;was
        clearly proving that parallel computing could be very workable. From a technology
        development standpoint, that was no small thing, even if the tech was not
        the most powerful or user friendly yet.&nbsp;</p><p>The CM2 was brought to
        market in April 1986 and could run FORTRAN as well as do floating-point operations.
        This version of the machine was much more usable to the scientific community
        and the machine's very novel, massively parallel architecture still required
        its customers to learn to use special software and new programming techniques.
        Using these programming languages proved difficult for even the machine\u2019s
        quite savvy researcher-customers and far more trouble than it was worth for
        almost all commercial customers. An additional factor that made the machine
        more trouble than it was worth for many corporate customers was that the machine's
        rate of breakdowns and subsequent downtime, while not an issue to academic
        work schedules, was far more than corporate customers were comfortable with.</p><p>Some
        in the research community found useful applications for the new machines in
        areas such as fluid flow dynamics and large database searches. Yet, in spite
        of these successes, the machines were not yet considered very usable to their
        potential customers. Years later, Dave Waltz \u2014 the head of TMC\u2019s
        AI group \u2014 recounted that most of the CM2\u2019s early customers were
        not using the machine correctly. In his eyes, TMC had built a machine that
        worked far more like a human brain than a sequential computer like the Cray
        \u2014 the most successful commercial supercomputer company at the time \u2014
        but, he noted, that people actually knew how to write programs for a Cray!
        Many of the customers were running&nbsp; their computations in such a way
        that they used the floating point processors in the CM2s in a standard way
        and largely ignored the machine\u2019s novel, 64,000 single-bit processors.
        But, with the technology continually progressing, DARPA continued to help
        TMC facilitate sales. As one of TMC\u2019s research directors later recounted,
        <strong>&nbsp;</strong>\\\"Our charter wasn't to look at a machine and figure
        out the commercial profit. Our charter was to build an interesting machine.\\\"</p><p>As
        the Cold War waned and the acute need for military supercomputing and near-term
        AI applications lessened, the Bush administration and the president\u2019s
        main science advisor \u2014 nuclear physicist Allan Bromley \u2014 turned
        its eye towards helping solve \u201Cgrand science challenges.<a class=\\\"footnote-anchor\\\"
        data-component-name=\\\"FootnoteAnchorToDOM\\\" id=\\\"footnote-anchor-4\\\"
        href=\\\"#footnote-4\\\" target=\\\"_self\\\">4</a>\u201D These challenges
        included problems related to climate modeling, analyzing protein folding patterns,
        mapping the human genome, earthquake prediction, and learning more about the
        underpinnings of quantum mechanics. Bromley seemed to believe that these problems
        required enormous computing power rather than artificial intelligence. From
        the modern perspective, smart people could argue whether or not Bromley was
        right in that idea. But, surely, massive increases in computing power paved
        the way for specific solutions in those problem domains \u2014 even if novel
        AI methods were what pushed some of those things across the finish line. Regardless,
        that general ethos was what drove the government\u2019s new High Performance
        Computing and Communications (HPCC) program. DARPA was the lead agency in
        this work and received an additional budget of several billion dollars through
        1996 for the program. One of the primary goals of the new program was a computer
        that could achieve one trillion computations per second.</p><p>Naturally,
        DARPA turned to TMC and gave the company some of this funding to help the
        company expand on its work that was so successful thus far. DARPA granted
        the company a $12 million initial contract to produce a scaled-down machine
        that could theoretically hit the trillion computations per second benchmark
        by 1992. It was in this period when TMC was working on the CM5 \u2014 TMC
        CEO Sheryl Handler skipped \u201CCM3\u201D and \u201CCM4\u201D in the hopes
        that the naming curveball would make espionage harder for would-be infiltrators
        \u2014 that the music stopped for the young technology darling.</p><p>In this
        period, as TMC was hitting its objectives and in a great place from a technology
        development standpoint, they began to spend as if they were having equivalent
        levels of commercial success \u2014 which they were not. Company executive
        Sheryl Handler had been brought in to co-found the company to bring a level
        of business maturity to the operation, but that proved to be a mistake. Her
        corporate spending habits were, putting it charitably, a bit decadent. This
        habit showed itself early on in the company\u2019s lifespan and continued
        to worsen as the 1980s wore on. As early as 1984, the very young TMC moved
        into the top two floors of the expensive Carter Ink building and began spending
        money on modern, luxurious tech company-like amenities. A couple of these
        expenses included shelling out for a custom design plan and a gourmet chef.
        In 1989, as TMC\u2019s work on the CM5 was getting underway, Handler signed
        a whopping 10-year lease with the Carter Ink building which cost the company
        $6 million a year \u2014 the $37 per square foot for the lease was reportedly
        4.5 times higher than Lotus Development Corp. was paying just down the road.
        Additionally, management rapidly increased headcount by around 40% around
        this time \u2014 resulting in a staff of around 400. The company\u2019s spending
        was more in line with the company\u2019s growing status in the computing world
        than its actual financial standing. As Stephen Wolfram described the company\u2019s
        status at the time, the company was \u201Cthe place that foreign trade delegations
        would come to visit to see where American business was at these days.\u201D
        An IBM computer scientist put it a little differently when he described the
        company as having cornered the market \u201Con sex appeal in high-performance
        computing.\u201D</p><p>While TMC was in the process of developing machines
        that were on their way to doing things that no other computer could seemingly
        do at the time, they were not producing machines that the market wanted as
        measured in total dollars of actual market demand. With the launch of the
        government\u2019s HPCC initiative, the company\u2019s research emphasis shifted
        even more strongly into building the most powerful computer possible by 1992.
        As it did this, the company continued to put off making the developments it
        needed to make to find some level of product market fit. The company had produced
        a machine that cost millions, but was almost exclusively sourcing its customers
        from the research community. The number of customers and the budget per customer
        in this community was far more limited than it was in the corporate sphere.</p><p>As
        work on the CM2 and later CM5 progressed, it does seem that the company had
        at least some opportunity to slowly ramp up efforts in the commercial sector.
        For example, in the late 1980s TMC sold two Connection Machines to American
        Express as database mining for improved customer analytics was a growing trend
        at the time. There was some internal debate about whether or not to start
        a business supercomputing group at the company \u2014 which would be an obvious
        \u201Cyes\u201D for most young and growing companies with shareholders \u2014
        but nothing much came of it. The company, in practice, clearly preferred to
        serve the research vision that drove its founder and employees to pursue computing
        over maximizing shareholder value.</p><p>As the work got underway on the CM5,
        progress was not as smooth as it had been on the previous two generations
        of machines. The first CM5 would not be completed until October 1991 \u2014
        and even then it was not quite as powerful as promised because there was a
        delay in producing chips that fit the machine\u2019s design. However, throughout
        this 1988 to 1991 period, TMC was peaking from a revenue standpoint. In 1989,
        thanks to the company\u2019s good reputation within DARPA\u2019s walls, the
        company achieved its first profit \u2014 $700,000 profit on revenues of $45
        million. Then, in 1990 the company saw profits of $1 million on revenues of
        $65 million. This would be the peak of TMC\u2019s profits.</p><p>Sadly, for
        TMC and the world of early 1990s parallel computing, that was as good as things
        would ever get for the firm. The company\u2019s introduction of an early version
        of the CM5 in late 1991 was lauded by Hillis as possessing the highest theoretical
        peak performance if you added enough processors to it \u2014 but that particular
        version of the machine was less powerful than the CM2. It\u2019s very possible
        that with more DARPA funding and further work upgrading the prototype, this
        branch of the technology base would have continued to progress from within
        the expensive walls of the Carter Ink building, but that never came to be.</p><p>In
        August 1991, in the midst of the HPCC initiative\u2019s leaders beginning
        to make plans for how to deploy most of its sizable budget moving forward,
        the <em>Wall Street Journal</em> released a piece diving into DARPA\u2019s
        subsidies of at least two dozen Connection Machines in addition to the machines
        it had bought. While this in and of itself was not necessarily illegal or
        even nefarious, this particular political era was one in which anything that
        looked like \u201Cindustrial policy\u201D or government interference in markets
        was extremely scrutinized. DARPA buying its contractor pool BBN Butterfly
        machines and some machines that resulted from CMU\u2019s Warp project does
        not seem to have received negative press in the same way. This disparity in
        PR outcomes might be because buying machines that might soon become obsolete
        due to technology in development within your portfolio did not come off as
        bad as subsidizing machines that were not yet considered to be truly finished,
        market-competitive products.&nbsp;</p><p>Regardless of why the negative press
        came to be, the Bush administration made a point of ensuring that \u201Cindustrial
        policy\u201D-like help to TMC did not continue, and the company did not fare
        so well when it was abruptly thrust out into the competition of the commercial
        market. In 1992, TMC reported a $17 million loss. Massive layoffs and spending
        cuts began around this time. A new CEO was brought on who attempted to broker
        some kind of merger or partnership deal with Sun Microsystems and IBM, but
        TMC\u2019s balance sheet was more toxic than its tech was impressive. To give
        the reader an idea: TMC owed $36 million in rent to the Carter Ink Building
        over the next six years for its ill-advised ten-year lease. And that was just
        one of the apparently many dark spots on its balance sheet. A deal like the
        one the new CEO sought could surely have been brokered around 1989 or 1990,
        when the company still seemed several years ahead of the field in the rapidly
        developing field of parallel computation and was beginning to show profits.
        However, the company had rejected offers for mergers and acquisitions at that
        time.</p><p>TMC filed for Chapter 11 bankruptcy in late 1993 and re-emerged
        from this as a small software company attempting to sell programs to run on
        their former competitors' parallel computers.</p><div class=\\\"captioned-image-container\\\"><figure><a
        class=\\\"image-link is-viewable-img image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg\\\"
        width=\\\"384\\\" height=\\\"384\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:600,&quot;resizeWidth&quot;:384,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1456w\\\" sizes=\\\"100vw\\\" loading=\\\"lazy\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">Photo
        of the CM-1 in 1985. I usually share all the photos on this Substack in black
        and white, but I shared this one in color so maybe the reader could recognize
        the computer from the CM series\u2019 cameo in the Jurassic Park movie. Photo
        via the Computer History Museum. https://www.computerhistory.org/revolution/supercomputers/10/73/284</figcaption></figure></div><h1>Connection
        Machine\u2019s Results</h1><p>Thinking Machines had been capable of hanging
        onto the dream of building a machine that helped raise all boats in the research
        community when it was fueled by DARPA funding. With DARPA funding, it was
        able to largely ignore the commercial market and commit itself to pushing
        the technology base forward with DARPA\u2019s relatively rare form of risk-tolerant,
        farther-sighted capital. And they were doing a good job of meeting DARPA\u2019s
        needs. However, TMC was spending its privately raised capital as if they were
        doing an equally good job courting commercial customers as they were hitting
        computing benchmarks. And that was absolutely not the case. Of the 125 machines
        the company sold in its entire history, only ten percent were to commercial
        customers. So, when the DARPA relationship took a turn, the company inevitably
        had to file for Chapter 11 bankruptcy after some rushed attempts at solvency.</p><p>The
        company surely succeeded in its primary goal: producing a machine that proved
        truly parallel machines could work. Also, the firm did this while developing
        its technology, step-by-step, in a way that fit with DARPA\u2019s preferred
        approach to moving the architectures program\u2019s technology base along
        at the time: gray coding. From a technological standpoint, for the first five
        or six years it seems the company did an effective job of moving technological
        progress forward \u2014 in the form of a machine with steadily increasing
        functionality \u2014 on a consistent timeline that DARPA was very happy about.
        From a commercial perspective, however, the company was clearly a failure.
        Hillis himself admitted as much. At the time of the company\u2019s bankruptcy,
        Hillis was asked how he felt. He replied, \u201CSad, that the people who put
        so much in \u2014 the employees and the investors \u2014 won't necessarily
        be the ones who benefit. And sad that I haven't been able to build a technology
        success into a business success.\u201D DARPA surely got more out of the program
        for what it spent than the company\u2019s investors did.&nbsp;</p><p>In retrospect,
        DARPA\u2019s general thinking on the problem area seems to have proven quite
        prescient. A major application area like image processing was exactly what
        they believed building a truly parallel processing machine like the Connection
        Machine could do. In the seminar series that Feynman encouraged TMC to put
        on, they even had a speaker from a not-so-popular field called neural networks
        come and talk. This story is surely proof that the DARPA ecosystem was, in
        many ways, functioning quite well \u2014 even if no commercial success came
        from TMC. Hillis and individuals at DARPA were in contact, talking about securing
        money for the idea contained in his Ph.D. thesis, at least as early as 1983.
        DARPA\u2019s awareness of his idea and the young company almost from the moment
        of inception \u2014 as well as its recognition of Hillis\u2019 ability to
        lead such an operation from a technical perspective \u2014 should surely be
        seen as a feather in DARPA\u2019s cap. This awareness, of course, was possibly
        facilitated by Hillis\u2019 prior connection with Minsky\u2019s DARPA-funded
        lab at the heavily DARPA-funded MIT. But these kinds of interpersonal networks
        at key universities that were very good at certain sub-areas of research \u2014
        such as MIT and CMU in AI research \u2014 were what DARPA PMs made it their
        business to maintain. That sort of relationship management, first and foremost,
        will always be one of the primary pieces of the puzzle that determines whether
        or not ARPA-like organizations succeed or fail.</p><p>A final note on TMC\u2019s
        results: it is very possible that even a change as small as Hillis partnering
        with a much more responsible, business side co-founder than Handler \u2014
        who proved adept at fundraising, but deficient in many key areas that TMC
        required \u2014 could have been enough to shepherd the company to a longer
        lifespan. From a commercial strategy standpoint, an individual like this may
        have helped TMC find commercial uses that fit in with the company\u2019s goals
        and culture. From a financial standpoint, a better CEO might have ensured
        that the company either did not raise money from misaligned capital pools
        or did not spend the money it did raise as poorly as it did.&nbsp;</p><h1>Connection
        Machine\u2019s Lessons Learned (and Caveats)</h1><p>One quote that encapsulates
        several of the PM-relevant lessons from the Thinking Machines Corporation\u2019s
        work and death was said by Hillis as the company was filing for bankruptcy,
        \u201CThe real money is in handling Wal-Mart\u2019s inventory rather than
        searching for the origins of the universe.\u201D From the perspective of a
        pure venture capitalist, this quote might read as the complaint of an academic
        wishing that the financial markets rewarded what researchers found important.
        Of course, that is not the ideal perspective most venture capitalists would
        not want in their portfolio founders. But the quote can also be read as Hillis
        calling attention to what might be a very major market inefficiency. Research
        companies like Thinking Machines are great vehicles for building new, exploratory
        technology, but there is often a very salient tradeoff between a company like
        this moving the technology base forward and profit-maximizing for shareholders
        in the near term.</p><p>Let\u2019s not forget, the ILLIAC IV project did not
        only run into difficulties because its team was attempting to build a machine
        with too many novel components at once. Slotnick\u2019s \u201Cbattleship in
        a bathtub\u201D remark was explicitly describing how ill-suited modern universities
        had become to building big, new machines. Hillis did not make the identical
        mistake of trying to build the Connection Machine from within a university,
        but he still did make a mistake in choosing his organizational setup. TMC
        ran into the conundrum of raising money from private investors \u2014 who
        would naturally prefer the company do things like build a computer to handle
        Wal-Mart\u2019s inventory \u2014 as well as a funder (and initial customer)
        in DARPA that primarily cared about building a computer to move technology
        forward. In most cases, a company is going to make one of its two (rather
        different) funders unhappy in a situation where their incentives are only
        partially aligned.</p><p>Had Thinking Machines only pursued DARPA funding
        and spent more frugally \u2014 as an entity like ISI, which ran DARPA\u2019s
        MOSIS program, would have done \u2014 it is very possible the company would
        have continued to progress as the darling of DARPA\u2019s architectures program
        for the foreseeable future. If the company would have taken this approach,
        it could still have raised money from private markets at some point, just
        later on in its existence when it had a product that was closer to market-ready
        and had a more natural customer. Additionally, the company could have attempted
        to find a variety of customers so it would not have all its eggs in the DARPA
        basket the way BBN (covered at length in a coming piece) was known to have
        done in the 1960s and 1970s. BBN, in its early existence, did not seem to
        be purely profit-maximizing. Rather, it sought a variety of contracts \u2014
        from DARPA, private philanthropies, engineering industry, and more \u2014
        that its researchers found new and useful, but that still paid enough to facilitate
        a rather profitable operation.</p><p>Of course, none of this alternative history
        brings any assurance of success either. The political winds may have blown
        against TMC one way or another regardless. It should be noted, though, that
        Hillis\u2019 Wal-Mart quote was predictive in some sense. The company that
        did make possibly the biggest parallel computing breakthrough in the late
        1990s \u2014 NVIDIA with its new GPU \u2014 found a way to fund its breakthrough
        research by servicing a market that required high-tech solutions and was surprisingly
        large given how unimportant it was in the grand scheme of things: gaming graphics.&nbsp;</p><p>It
        is also important to note that, in the first five or six years of TMC\u2019s
        existence, it seems that the gray coding approach was working just as Squires
        had hoped. TMC was often putting out versions of Connection Machines that
        did not have the functionality or usability that many users would have wanted.
        But that could be considered a marketing issue as much as anything else. In
        bringing new functionality to the machines in this steady way, this gradual
        decrease in user dissatisfaction seems par for the course. The rate of progress
        of the Connection Machine seems, generally, far preferable to the messy process
        that was the ILLIAC IV development. Of course, individual PMs will know best
        when either approach is most suitable to a given project or portfolio. A variety
        of factors can impact the decision and there is no one-size-fits-all solution.
        One factor that could impact a decision like this, for example, is the higher
        the probability of failures or surprises in developing components in a given
        project, the more gray coding might be the clearly responsible approach. However,
        there are obviously also cases in which multiple components need to be changed
        between iterations of a machine for anything truly new to come of the process.
        When to apply either approach is, as always, best left up to a smart PM.</p><p>While
        the legacy of the Connection Machine is a complicated one, it was still the
        gem in an architectures portfolio that accomplished what it sought to do.
        As Roland and Shiman put it:</p><blockquote><p>The first stage of the SC architectures
        program proved unquestionably that parallelism was viable and did have promise
        in solving many real-world problems\u2026According to Professor John Hennessy
        of Stanford University, a respected authority on microelectronics and computer
        architectures, computer designers before SC, in their quest for greater speed
        and performance, kept running into the wall posed by the fundamental physical
        limits to the improvement of conventional systems and components. Each architect
        would try, and each would join the pile at the base of the wall. It was Squires,
        he said, who showed the way, and, through parallelism, led designers over
        the wall.</p></blockquote><p></p><p>Hopefully the collection of history and
        lessons in this post prove of some use to PMs thinking through decisions like
        those that arose in the ILLIAC IV and CM1-CM5 projects.</p><p class=\\\"button-wrapper\\\"
        data-attrs=\\\"{&quot;url&quot;:&quot;https://www.freaktakes.com/p/illiac-iv-and-the-connection-machine?utm_source=substack&utm_medium=email&utm_content=share&action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\\\"
        data-component-name=\\\"ButtonCreateButton\\\"><a class=\\\"button primary\\\"
        href=\\\"https://www.freaktakes.com/p/illiac-iv-and-the-connection-machine?utm_source=substack&utm_medium=email&utm_content=share&action=share\\\"><span>Share</span></a></p><p
        class=\\\"button-wrapper\\\" data-attrs=\\\"{&quot;url&quot;:&quot;https://www.freaktakes.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe
        now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\\\" data-component-name=\\\"ButtonCreateButton\\\"><a
        class=\\\"button primary\\\" href=\\\"https://www.freaktakes.com/subscribe?\\\"><span>Subscribe
        now</span></a></p><p><em>Stay tuned for next week\u2019s pieces! They explore
        DARPA\u2019s ~1980 work in speech and computer vision applications, expert
        systems, and its programs for moving the field of chip research forward. </em></p><ul><li><p><em>Coming
        Sunday</em></p><ul><li><p><em>MOSIS: The 1980s DARPA 'Silicon Broker'</em></p></li></ul></li><li><p><em>Coming
        Monday/Tuesday</em></p><ul><li><p>The Autonomous Land Vehicle, Pilot's Associate,
        and Battle Management: Three North Star Application Projects from DARPA's
        Strategic Computing Portfolio</p></li></ul></li></ul><h4><strong>Specific
        Links:</strong></h4><p><strong>ILLIAC IV</strong></p><ul><li><p>Slotnick\u2019s
        <a href=\\\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4392921\\\">The
        Conception and Development of Parallel Processors: A Personal Memoir</a></p><ul><li><p>An
        extremely useful source. Short, but quite thorough. Dives further into Slotnick\u2019s
        journey from grad student to eventually running the ILLIAC IV project and
        his perspectives and thoughts on the events as they happened.&nbsp;</p></li></ul></li><li><p>Chapter
        17 of Van Atta\u2019s <a href=\\\"https://apps.dtic.mil/sti/citations/ADA239925\\\">DARPA
        Technical Accomplishments: An Historical Review of Selected DARPA Projects,
        Volume 1</a></p><ul><li><p>A 1990, third party perspective on the planning,
        operations, and lessons learned from the ILLIAC IV project</p></li></ul></li><li><p>Chapter
        5, pages 160-164 of <a href=\\\"https://amzn.to/3QFebf3\\\">Strategic Computing:
        How DARPA Built the Computer Age</a>&nbsp;</p><ul><li><p>Particularly emphasizes
        the DARPA internal memory and different interpretations of the ILLIAC IV projects
        at DARPA during the 1980s.</p></li></ul></li><li><p>Chapter 6, pages 264-268
        of <a href=\\\"https://www.amazon.com/dp/0801851521?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details\\\">Transforming
        Computer Technology: Information Processing for the Pentagon, 1962-1986</a></p><ul><li><p>Particularly
        emphasizes how IPTO first got into parallel computing and the plans and early
        project planning of the ILLIAC IV project from a high level</p></li></ul></li><li><p>Chapter
        2 of <a href=\\\"https://www.amazon.com/dp/0801851521?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details\\\">Transforming
        Computer Technology: Information Processing for the Pentagon, 1962-1986</a></p><ul><li><p>Particularly
        dives into much of the early administrative history of IPTO \u2014 including
        things like budgets, norms for discussing and sourcing projects, etc.</p></li></ul></li><li><p>Slotnick\u2019s
        <em>Scientific American </em>article, <a href=\\\"https://www.jstor.org/stable/pdf/24927727.pdf?refreqid=excelsior%3A4f0e2e24a1a28f71d08d31ef99082736&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1\\\">The
        Fastest Computer</a></p><ul><li><p>A 1971 article outlining the project\u2019s
        goals and achievements for a public audience.&nbsp;</p></li><li><p>The article
        also contains the performance metrics and specs of various aspects of the
        machine as of 1971.&nbsp;</p></li><li><p>The article also spends several pages
        walking through an example problem from mathematical physics, walking the
        reader through how the ILLIAC IV \u2014 or any parallel processor for that
        matter \u2014 would perform its operations and solve the problem faster than
        a traditional computer at the time.&nbsp;</p></li></ul></li></ul><p><strong>Connection
        Machine</strong></p><ul><li><p>Chapter 5, pages 158-184 of <a href=\\\"https://amzn.to/3QFebf3\\\">Strategic
        Computing: How DARPA Built the Computer Age</a>&nbsp;</p><ul><li><p>Provides
        more detail on the history of Squires\u2019 approach to the architectures
        program and other projects that made up the portfolio. Some details about
        how the projects were run can also be found there.&nbsp;</p></li></ul></li><li><p><a
        href=\\\"http://www.bitsavers.org/pdf/thinkingMachines/The_Rise_and_Fall_of_Thinking_Machines_1995.pdf\\\">The
        Rise and Fall of Thinking Machines</a></p><ul><li><p>A thorough article diving
        into the lifespan of the company from a business and cultural perspective.
        The piece covers Thinking Machine\u2019s corporate decision-making and relationships
        from its early years until its bankruptcy.</p></li></ul></li><li><p><a href=\\\"https://longnow.org/essays/richard-feynman-connection-machine/\\\">Richard
        Feynman and the Connection Machine</a></p><ul><li><p>This <em>Physics Today
        </em>article, written by Hillis himself, not only discusses Feynman and his
        work with the company, but also paints a picture of what went on in the early
        days of the company when it was a bunch of technical researchers working out
        of an old mansion outside of Boston.</p></li></ul></li><li><p><a href=\\\"https://www.washingtonpost.com/archive/business/1994/08/28/thinking-machines-rather-than-markets/db29b5ee-0107-43e2-89b0-c22518a3a02c/\\\">Thinking
        Machines Rather Than Markets</a></p><ul><li><p>This <em>Washington Post </em>article,
        written in the midst of TMC\u2019s demise, provides additional facts about
        the situation, but will be primarily useful to readers of this piece in helping
        them understand how reasonable-seeming help provided by an actor like DARPA
        can be portrayed depending on the given political climate.</p></li></ul><p></p></li></ul><h4><strong>General
        Links:</strong></h4><ul><li><p><a href=\\\"https://amzn.to/3SqvwJu\\\">Strategic
        Computing: How DARPA Built the Computer Age</a></p></li><li><p><a href=\\\"https://amzn.to/3tSafxW\\\">Transforming
        Computer Technology: Information Processing for the Pentagon, 1962-1986</a></p></li></ul><div
        class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a id=\\\"footnote-1\\\"
        href=\\\"#footnote-anchor-1\\\" class=\\\"footnote-number\\\" contenteditable=\\\"false\\\"
        target=\\\"_self\\\">1</a><div class=\\\"footnote-content\\\"><p>In 1962,
        IPTO's $9 million budget was entirely earmarked for category 6.1 research.</p></div></div><div
        class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a id=\\\"footnote-2\\\"
        href=\\\"#footnote-anchor-2\\\" class=\\\"footnote-number\\\" contenteditable=\\\"false\\\"
        target=\\\"_self\\\">2</a><div class=\\\"footnote-content\\\"><p>For the rest
        of the interpersonal aspects of the accounts in the next few paragraphs, I\u2019ll
        rely on Slotnick\u2019s memoir since he seemed to have a more clear recollection
        of the order in which things happened than Sutherland. </p></div></div><div
        class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a id=\\\"footnote-3\\\"
        href=\\\"#footnote-anchor-3\\\" class=\\\"footnote-number\\\" contenteditable=\\\"false\\\"
        target=\\\"_self\\\">3</a><div class=\\\"footnote-content\\\"><p>Another FreakTakes
        piece dives into the analogous case of early molecular biology researchers
        \u2014 many of whom would eventually win Nobel Prizes \u2014 taking risks
        and changing multiple parameters between experiments. The piece explores what
        they did and why they thought that was an optimal strategy for a field as
        young as molecular biology.</p></div></div><div class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a
        id=\\\"footnote-4\\\" href=\\\"#footnote-anchor-4\\\" class=\\\"footnote-number\\\"
        contenteditable=\\\"false\\\" target=\\\"_self\\\">4</a><div class=\\\"footnote-content\\\"><p>For
        those curious about what \\\"AI applications\\\" looked like in the mid-1980s,
        check out next week's post on the Autonomous Land Vehicle, Pilot's Associate,
        and Battle Management</p><p></p></div></div>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/ewpnk-67998\",\"guid\":\"138555751\",\"id\":\"368c3f7f-9b84-4710-a8d8-16dbee2637c3\",\"image\":\"https://substack-post-media.s3.amazonaws.com/public/images/78b55c38-2baa-4158-a06f-e56e3796e919_800x800.jpeg\",\"language\":\"en\",\"published_at\":1699040651,\"reference\":[],\"relationships\":[],\"summary\":\"DARPA's
        varied approaches to developing early parallel computers\",\"tags\":[],\"title\":\"ILLIAC
        IV and the Connection Machine\",\"updated_at\":1699040651,\"url\":\"https://www.freaktakes.com/p/illiac-iv-and-the-connection-machine\"},\"highlight\":{\"authors\":[{\"name\":\"Eric
        Gilliam\"}],\"content_html\":\"<p><em>This piece is a part of a FreakTakes
        series. The goal is to put together a series of administrative histories on
        specific DARPA projects just as I have done for many industrial R&amp;D labs
        and other research orgs on FreakTakes. The goal \u2014 once I have covered
        ~20-30 projects \u2014 is to put together a larger \u2018ARPA Playbook\u2019
        which helps individuals such as PMs in ARPA-like orgs navigate the growing
        catalog of pieces in a way that helps them find what they need to make the
        best decisions possible. In service of that, I am including in each post a
        bulleted list of \u2018pattern language tags\u2019 that encompass some categories
        of DARPA project strategies that describe the approaches contained in the
        piece \u2014 which will later be used to organize the ARPA Playbook document.
        These tags and the piece itself should all be considered in draft form until
        around the Spring of 2024. In the meantime, please feel free to reach out
        to me on <a href=\\\"https://twitter.com/eric_is_weird\\\">Twitter</a> or
        email (egillia3 | at | alumni | dot | stanford | dot | edu) to recommend additions/changes
        to the tags or the pieces. Also, if you have any ideas for projects from ARPA
        history \u2014 good, bad, or complicated \u2014 that would be interesting
        for me to dive into, please feel free to share them!</em></p><p><strong>Pattern
        Language Tags:</strong></p><ul><li><p>Pushing forward the technological base
        via speculative machine building contracts</p></li><li><p>Gray coding</p></li><li><p>Incentivizing
        a group of related goals by organizing them all into a single contract</p></li><li><p>Selecting
        multiple industry contractors for a trial period before choosing the final
        contractor</p></li><li><p>Funding a conservative project to use as a measuring
        stick for more ambitious projects in the area</p></li></ul><h1>Introduction</h1><p>DARPA\u2019s
        multi-decade investment in developing early versions of parallel processing
        computers provides a lot to learn from for ARPA-style PMs. Two of DARPA\u2019s
        most prominent, long-standing investments in this area took two completely
        different approaches to push the technology base forward in a coordinated
        fashion. Both are viewed as successes to some and abject failures to others.
        In this piece, I will dive into the operational structures that informed DARPA\u2019s
        investments both in the ILLIAC IV parallel processing computer in the late
        1960s \u2014 the project was based out of the University of Illinois under
        a researcher named Daniel Slotnick \u2014 and in the young Thinking Machines
        Company in the 1980s and their Connection Machine computer \u2014 which brought
        them fame and infamy in the computing world in the years in which DARPA was
        most heavily involved with the company.&nbsp;</p><p>To start, I\u2019ll dive
        into DARPA\u2019s investment in the ILIAC IV. The cost overruns and time delays
        for which this project became internally famous very heavily informed the
        approach that Stephen Squires took to managing DARPA\u2019s investments in
        Thinking Machines and his computer architectures portfolio as a whole in the
        1980s.</p><h1>ILLIAC IV\u2019s Context</h1><p>When DARPA Director Charles
        Herzfeld took over in the mid-1960s, computing research was not nearly as
        pressing of a defense research issue as others like ballistic missile defense
        or nuclear test detection. The previous director, Jack Ruina, often only met
        with the then-IPTO Director J.C.R. Licklider once or twice a month. Now, that
        is in large part because he both trusted Licklider and attempted to have a
        hands-off management style. However, it also speaks to the lack of priority
        that the Information Processing Techniques Office (IPTO) had within the DARPA
        portfolio at the time. PMs in hotter areas of the portfolio were said to have
        quickly met with Ruina as many as five times a day to discuss small issues.&nbsp;</p><p>As
        the reader surely knows, great work was done by Licklider and his portfolio
        of performers in the early 1960s with this approach. As Herzfeld took over,
        the progression of the field was apparent to him and he thought it had the
        potential to help improve programs all across DARPA and the DoD. He recounted
        his approach to managing the IPTO as he took over, saying:</p><blockquote><p>I
        thought that my job vis-a-vis the computing program, when I was Deputy Director
        and Director, was first of all to make sure to get the best office directors
        we could get, and second, to help them do what they thought they needed to
        do, and third, to look for applications across the projects for computing,
        when you apply the new computing ideas, technology and its capabilities as
        widely as possible.</p></blockquote><p>Ivan Sutherland \u2014 Licklider\u2019s
        young, hand-picked replacement who had gotten his Ph.D. at MIT and had been
        at CMU for several years since graduating \u2014 became IPTO\u2019s Director
        under Herzfeld. Sutherland, while young, had been making a name for himself
        in his field. His Sketchpad program, written for his MIT thesis, would later
        help win him a Turing Award.</p><p>Herzfeld also lived up to his word of helping
        good ideas from this office find funding. One example of this was when, in
        1965, Robert Taylor brought forward the network proposal to Herzfeld that
        would become the ARPANET. Herzfeld, thinking the idea had promise, quickly
        determined that ARPA should fund the idea and took some money that had been
        set aside from a lower-priority program \u2014 almost $500,000 (~$5 million
        today) \u2014 to get started immediately. They would get funds from Congress
        to continue the program in the following fiscal year. This kind of administrative
        move, while not rare in managing the DARPA budget at the time, was more likely
        to be done for higher-priority projects.</p><p>As IPTO\u2019s technology base
        was becoming more mature, its budget began to increase and PMs were looking
        to fund projects that were further along in the development process \u2014
        IPTO\u2019s early-1960s budget had been almost entirely allocated to early-stage
        research.<a class=\\\"footnote-anchor\\\" data-component-name=\\\"FootnoteAnchorToDOM\\\"
        id=\\\"footnote-anchor-1\\\" href=\\\"#footnote-1\\\" target=\\\"_self\\\">1</a>
        One development area of interest was the idea of building a more efficient
        computer leveraging hardware that was more parallel in nature. If implemented
        successfully, it was thought that the machine could help the DoD solve many
        pressing problems that required more computing power \u2014 such as calculating
        nuclear weapons effects or real-time processing of radar data.</p><h1>ILLIAC
        IV\u2019s Beginnings</h1><p>In 1965, Sutherland reached out to Daniel Slotnick
        a month after Slotnick had arrived at the University of Illinois to set up
        a visit at Slotnick\u2019s new office in Urbana-Champaign. Slotnick had become
        acquainted with Sutherland while he was working on his SOLOMON parallel computing
        machine at Westinghouse; this area of work at Westinghouse had many DoD ties.
        In their meeting, Sutherland would ask Slotnick if he was interested in developing
        a large parallel computer at Illinois.&nbsp;</p><p>At least, that is how the
        events unfolded according to Daniel Slotnick\u2019s IEEE memoir. Sutherland,
        in an oral history conducted by William Aspray, seems to remember things differently.
        According to his memory, Slotnick came into his office while he was still
        at Westinghouse and funding for his project was drying up, and an interaction
        resembling the following ensued:</p><blockquote><p>He came into the office
        and said,\\\" Basically, I want to do these things and I cannot do it at Westinghouse,
        because they will not let me do it. Where should I go?\\\" I introduced him
        to some of the university folk. So that one was fairly a long time in the
        doing.</p></blockquote><p>This, also, is a quite believable ordering of events
        given my understanding of how things worked at IPTO in the 1960s. Whether
        Slotnick\u2019s account or Sutherland\u2019s is more accurate does not matter
        much. Regardless, in 1965 Slotnick and Sutherland had a conversation about
        building a parallel computing machine \u2014 which they had clearly discussed
        before \u2014 and Sutherland let Slotnick know that this might be a good time
        to submit a proposal now that he was at the University of Illinois. When the
        proposal was submitted, other IPTO staff as well as Herzfeld supported funding
        the proposal.<a class=\\\"footnote-anchor\\\" data-component-name=\\\"FootnoteAnchorToDOM\\\"
        id=\\\"footnote-anchor-2\\\" href=\\\"#footnote-2\\\" target=\\\"_self\\\">2</a>
        In the next few paragraphs describing the details of how the finer points
        of the contract materialized, I\u2019ll rely on Slotnick\u2019s more clear
        account of the events. Certain small details might differ between the two
        accounts, but none that should matter much for the takeaways of this piece.</p><p>The
        project would be quite similar to Slotnick\u2019s work at Westinghouse, and
        it was an area of obvious interest to IPTO\u2019s mission. After Slotnick
        thought it over, he agreed. Sutherland initially proposed starting the project
        with a small study phase. But this tentative approach was not agreeable to
        Slotnick. Arriving at Illinois, the disappointment of the SOLOMON project
        was fresh on Slotnick\u2019s mind. Slotnick\u2019s intensive work on the SOLOMON
        project jointly funded by the DoD and Westinghouse for two years, Slotnick\u2019s
        Westinghouse team steadily grew to around 100 people. In spite of the team\u2019s
        promising early work on the technology, the project\u2019s steady progress
        came to an abrupt halt when the project\u2019s main sponsor in the DoD suffered
        a tragic drowning accident. The man\u2019s replacement did not back the project
        in the same way. Quickly, the project went from pushing for the funds to build
        a full-scale version of the machine to having no funding at all. Not long
        after, Slotnick found himself at Illinois.</p><p>With this disappointment
        fresh on his mind, Slotnick said he was \u201Canxious to do something else.\u201D
        He only wished to pursue the project with DARPA sponsorship if Sutherland
        meant business.&nbsp; Slotnick described his reaction to Sutherland\u2019s
        proposal of a modest seed funding stage as insistent:</p><blockquote><p>I
        absolutely refused. I wanted a $2 million payment at the outset and a contract
        for a total of $10 million. I did this to make sure that the ARPA commitment
        was real and had passed the highest levels of review.\u201D Sutherland must
        have understood. Slotnick continued,&nbsp; \u201CIvan agreed. I wrote the
        proposal and a few weeks later we had our contract.\u201D</p></blockquote><p>The
        proposed machine was heavily inspired by Slotnik\u2019s time spent developing
        the SOLOMON prototype machines. The newly proposed machine \u2014 the ILLIAC
        IV, named after three earlier computers built at Illinois \u2014 sought to
        achieve over one billion operations per second. Beyond the project\u2019s
        aim to produce a machine with a substantial improvement in computing power,
        its management sought to roll out improvements to much of the component technology
        in the machine. When all was said and done, Slotnick and the DARPA team involved
        believed the machine would have several useful areas for military application.
        DARPA and the armed services research organizations planned to deploy the
        resultant machine to help advance research problems in climate dynamics, anti-submarine
        warfare, ballistic missile defense, and the army\u2019s multiple target problem.
        Additionally, a nuclear research office was interested in the capability of
        the computer to help solve the climate modification problem. Lastly, IPTO
        itself was particularly interested in the computer\u2019s applications to
        researchers in cryptanalysis, simulation of fluid flow, seismic array processing,
        economics, and other problems ideally structured to utilize the computer\u2019s
        parallel processing capabilities.</p><p>Sutherland \u2014 as the IPTO Director
        \u2014 was very explicitly attempting to use the ILLIAC IV project to push
        the boundaries of several component fields of computing simultaneously. As
        later-DARPA Director Stephen Lukasik described the ILLIAC IV team\u2019s full
        frontal assault approach, IPTO and DARPA were \u201Cpushing machine architecture,
        software for parallel machines\u2026medium scale integrated circuits \u2014
        and then the application of that to a variety of important defense problems.\u201D</p><h1>ILLIAC
        IV\u2019s Operations</h1><p>In addition to this project helping push forward
        the technology base in the area of parallel computing, Sutherland hoped that
        it could help spark some efforts from industry in building new computer components
        relevant to this general area of technology. So, while the overarching contract
        for the project was issued to the University of Illinois team under Slotnick,
        the work of an industry subcontractor to build the machines the Illinois team
        designed would also be vital to the project\u2019s success. In addition, contracts
        for the research and engineering development of various components of the
        machine would be given out to other parties from the IPTO performer pool.
        However, the work of these additional parties would generally be less important
        to the project\u2019s success than the work of the Illinois team and the computer
        manufacturer chosen to produce the final machine.</p><p>The Illinois team\u2019s
        initial design specified a machine with four modules of 64 processors under
        the control of a single construction unit which would allow them to simultaneously
        work on the same problem. As Slotnick began to assemble a team at Illinois,
        the team also began to figure out which computer manufacturer to partner with.
        Slotnick was not upset by the project\u2019s partnership structure. From the
        beginning, he felt this was a natural approach to doing a project like the
        ILLIAC IV. Slotnick had come from industry and understood the comparative
        advantages of industry and academia when it came to designing and building
        new computers at that time. As Slotnick put it, \u201CThe days when a university
        could design and fabricate a big machine by itself were over, and we decided
        that while we would do the architectural design and most of the software and
        applications work, we would rely on industry for detail design and fabrication.\u201D</p><p>Since
        selecting the right contractor to build the machine was pivotal to the success
        of the contract, Slotnick and the DARPA team held a competition. Slotnick
        and the DARPA team outlined the major approaches and incorporated them into
        a bid set. In August 1966, after \u201Cmany months of intensive contacts with
        industry,\u201D RCA, the Burroughs Corporation, and Univac were all awarded
        8-month contracts to continue working on the early stages of the contract
        and collaborating with the Illinois team. After this period, in 1967, the
        Burroughs Corporation was selected as the best fit to continue on as a partner
        in building the ILLIAC IV. Burroughs had partnered with Texas Instruments
        in its bid. The plan was for TI to develop the integrated circuits for the
        machine\u2019s all-important process element (PE) array. With that trial period
        over and the DARPA and the Illinois teams having gotten the chance to work
        with all three companies for a time, the industry partner for the project
        was decided upon.   </p><p>Either Larry Roberts or Robert Taylor \u2014 Taylor
        had become the new IPTO Director in the intervening period and Roberts was
        the PM who focused on the ILLIAC IV project \u2014 and the Illinois team awarded
        the related contracts to work on other component parts of the ILLIAC IV to
        the following non-U of I teams:&nbsp;</p><ul><li><p>The design and implementation
        of the FORTRAN compiler for the ILIAC IV to the Applied Data Research Corporation</p></li><li><p>Various
        hardware and software to help link the ILLIAC IV and a trillion bit memory
        with the ARPANET to the Computer Corporation of America</p></li><li><p>And
        work on the development of the interactive front end processor to BBN.</p></li></ul><p>As
        the project progressed, the design and construction phase was riddled with
        research problems and setbacks that forced design changes on the research
        team, delays, and cost overruns. Two main issues contributed to the early
        cost overruns and time delays in the first couple of years of the project.
        The first was TI\u2019s inability to produce the 64-pin emitter-coupled logic
        (ECL) packages. This caused a delay of over a year and a $4 million cost overrun.
        Slotnick documented that TI was not exactly forthright in taking the blame,
        writing, \u201CA great deal of inconclusive finger-pointing went on between
        TI and its suppliers.\u201D Regardless, the problem \u2014 and loss of cash
        and time \u2014 remained. The second major problem was Burroughs\u2019 inability
        to make workable magnetic thin-film PE memories for the evolving design of
        the machine. This problem caused an additional year of delay and another $2
        million cost overrun. Additional problems existed with packaging, circuit
        design, and interconnections for the large machine. But these two problems,
        in the early years of the project, caused the most headaches.</p><p>Slotnick,
        in his writings, explains how these very early setbacks began to affect project
        planning. Certain goals became more modest and the team had to substitute
        in and out certain component technologies to fit in better with the evolving
        design and changing circumstances. Slotnick writes:&nbsp;</p><blockquote><p>We
        retreated from the 64-pin packages to standard 16-pin dual in-line packages.
        In doing so, however, everything got bigger and more expensive. A lot of logic
        design was salvageable as a consequence of making the 16-pin packages logically
        derivative of the abandoned 64-pin packages, but board layout, back-panel
        wiring, and all system level hardware, of course, had to be restarted from
        scratch. The memory situation was even messier. Burroughs had a large investment
        in its thin films and didn\u2019t want to give up on them even after my own
        independent review had concluded they still represented an intolerable development
        risk. Semiconductor manufacturers were just beginning to gear up for memory
        chip manufacturing; the manufacturing means were clearly at hand, or at least
        so it seemed to me, but the chips were not. I made the painful decision to
        drop films and go with semiconductors.</p></blockquote><p>In making this semiconductor
        pivot, Slotnick believed the young Fairchild Semiconductor was easily the
        most qualified supplier. While Burroughs railed against the selection of the
        young company, a panel of independent ARPA experts \u2014 assembled due to
        Burroughs\u2019 \u2018furor\u2019 at Slotnick\u2019s selection of Fairchild
        \u2014 confirmed the decision. Fairchild seemed to be one of the only \u2014
        if not the only \u2014 suppliers delivering high-speed semiconductor memory
        at the time.&nbsp;</p><p>As the project wore on, the projected cost of the
        project roughly tripled and Slotnick was now only promising to deliver a system
        a fourth the size of the one originally promised. In spite of these shifts,
        Larry Roberts \u2014 who would become IPTO Director himself and who was already
        known as the \u201Cfather of computer networks\u201D \u2014 remained behind
        the project. Not only did Roberts, as Slotnick jokingly put it, share in his
        \u201Csick attachment to really big pieces of hardware,\u201D he was very
        interested in the list of applications which the machine was set to work on
        \u2014 including numerical weather prediction, sonar, radar, seismic signal
        processing, and an assortment of other computations array computers tended
        to do well. But, as Slotnick saw it, to justify the escalating costs, making
        the machine available to as large a community of users as possible became
        even more essential. The larger the scale of users the machine achieved, the
        more justified the escalating costs would be. Slotnick believed that this
        was a major factor that drove making the ILLIAC IV easily available over the
        ARPANET to be a much stronger point of emphasis for the project as the years
        wore on.</p><p>In 1970, five years into the constantly delayed project, the
        Illinois team was finally beginning to hone in on a workable machine. But,
        even then, there was still an element of instability in the project\u2019s
        operations. From a technical standpoint, things were beginning to get on more
        sure footing. The PE had been successfully redone with the new TI integrated
        circuits, PE memory chips were being delivered by Fairchild, and other pieces
        of equipment were getting produced on schedule. However, cultural conflicts
        were now coming to a head on the Illinois campus. Not only were stresses between
        Slotnick and those faculty not involved with the million-dollar-a-month project
        being run out of an academic department coming to a head, but Vietnam War-era
        turmoil from the students was making it more difficult to peacefully continue
        the defense-funded project on campus. In the midst of all of these non-scientific
        issues, the project was first moved from within the academic department to
        become its own free-standing center at the university. Then, in January 1971,
        ARPA decided to move the system to the NASA Ames Research Center when it became
        feasible. The move was carried out by Burroughs in 1972 and may have happened
        earlier in the technological development process than DARPA would have preferred.</p><p>Hardware
        problems plagued the machine in its early years at Ames. Early circuit batches
        failed at high rates and in ways that were often difficult to detect. Additionally,
        many of the back-panel connections and terminating transistors functioned
        poorly. While successful runs were made as early as 1973, the team at Ames
        did not have the machine working in a way that could be considered reliable
        until 1975. However, even after the machine was completed, one problem continued
        to cause severe headaches: producing software to run on the machine. This
        would be an area of lasting difficulty that plagued the machine for its entire
        lifetime \u2014 which was admittedly not so long. The difficulty in writing
        software for the machine would be remembered when the next generation of IPTO
        leaders sought to push the area of parallel computing to the next level.</p><p>The
        ILLIAC IV would only be used until about 1981 when NASA replaced the machine
        with a more reliable and easy-to-use successor</p><div class=\\\"captioned-image-container\\\"><figure><a
        class=\\\"image-link is-viewable-img image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg\\\"
        width=\\\"500\\\" height=\\\"341\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:341,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:146356,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa19f297e-74cf-4ee6-b53e-a0d2c1e371b5_500x341.jpeg
        1456w\\\" sizes=\\\"100vw\\\" loading=\\\"lazy\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">Three
        men installing a chassis or module onto the ILLIAC IV. Photo provided to the
        Computer History Museum by the Burroughs Corporation. https://www.computerhistory.org/collections/catalog/102651989</figcaption></figure></div><h1>ILLIAC
        IV\u2019s Results</h1><p>By the time ILLIAC IV was finally constructed, the
        total cost was $31 million (over $200 million today) \u2014 substantially
        higher than the original, expected price tag of $8 million. Beyond the cost
        problem, the final machine only had one quadrant of 64 processors rather than
        the originally planned four. Several of the many experimental components of
        the machine could not be made to work and existing components had to be used
        in their place. The resulting machine only achieved one-twentieth of the computing
        power that the researchers originally sought to achieve \u2014 the initial
        goal was one billion operations per second. Also, even though the ILLIAC IV
        was considered finished in 1972, much of the 1972 to 1975 period was spent
        in an intensive effort to correct problems and improve the reliability of
        the machine as researchers affiliated with the armed services were attempting
        to use the machine for practical applications.&nbsp;</p><p>Notwithstanding
        the setbacks, once the ILLIAC IV became operational in 1975, it was thought
        to be the fastest machine in the world for certain types of problems \u2014
        calculations of fluid flow being one example. In the late 1970s, the computer
        was used in service of many of the armed services and DARPA applications on
        which it was intended to be used. This includes the army\u2019s multiple target
        problem, research in cryptanalysis, seismic array processing, economics, and
        more. For the few years in which the machine was in operation, it may have
        been the best computer in the world for solving problems whose calculations
        could be performed in an \u201Call at once\u201D manner and were, thus, ideally
        suited to being solved by a parallel processing computer.&nbsp;</p><p>From
        the standpoint of developing the technology base, the component technology
        of the computer helped improve many pieces of the technology base. Some of
        the prominent examples:</p><ul><li><p>The first large-scale computer to employ
        emitter-coupled logic in its processors \u2014 taking the place of transistor-transistor
        logic. This technology would go on to be used in many high-speed computers
        in subsequent years.&nbsp;</p></li><li><p>Its circuit boards were the first
        to be designed with the aid of a computer. Computer design automation is now
        widely used in industry.</p></li><li><p>The design of its storage technology
        \u2014 which consisted of 64 disks that could be read from and written to
        concurrently \u2014 led to higher speed input/output.</p></li><li><p>The machine\u2019s
        laser memory system as an additional, tertiary memory had a capacity in the
        trillion-bit range and read-in and read-out rates in the million bits per
        second range.</p></li><li><p>ILLIAC IV\u2019s architecture employed a single
        instruction stream to control the multiple data streams involved in interprocessor
        communication.</p></li><li><p>ILLIAC IV was the first large system to employ
        a semiconductor primary store.</p></li></ul><p>On top of all of the new functionality,
        the program also succeeded in (eventually) producing a machine built by a
        joint industry-university collaboration. Slotnick and the Illinois team did
        not believe they\u2019d have been better off working on the project with no
        help from industry. In fact, Slotnick later reflected that he believed running
        such an operation with him and the university as primary contractor could
        have been a mistake \u2014 or something to learn from at the very least. He
        referred to trying to administratively manage such a technically complex operation
        with that sort of headcount, scale, budget, and operational needs as having
        \u201Call the sense of trying to build&nbsp; a battleship in a bathtub.\u201D</p><div
        class=\\\"captioned-image-container\\\"><figure><a class=\\\"image-link is-viewable-img
        image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg\\\"
        width=\\\"500\\\" height=\\\"392\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:392,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:52841,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21bfa1cf-200d-4d05-855b-30c5cbeec880_500x392.jpeg
        1456w\\\" sizes=\\\"100vw\\\" loading=\\\"lazy\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">The
        battleship in the bathtub. Engineer Jay Patton is at the oscillosope. The
        two men kneeling are testing one of the boards on a test machine. Photo provided
        to the Computer History Museum by UIUC. https://www.computerhistory.org/collections/catalog/102651994</figcaption></figure></div><h1>ILLIAC
        IV\u2019s Lessons Learned (and Caveats)</h1><p>Joining all of these non-obvious
        developments into a project meant to produce a single, useful machine caused
        substantial time delays and cost escalation. But the field of computing got
        more than nothing in return for the tradeoff. The ambitious design of the
        machine created a technology \u201Cpull\u201D and stimulated the development
        of new component technologies that had the potential to find broad use.&nbsp;</p><p>For
        example, when the project ran into issues incorporating the originally planned
        film memories into the design, Slotnick turned to the nascent Fairchild Semiconductors
        to design the project\u2019s all-important memories to fit with its new design.
        The new design was seen to be quite risky since it required advances in semiconductor
        art, engineering design, and production. But Slotnick was convinced the Fairchild
        team could do the work. This confidence may have come from the fact that Slotnick\u2019s
        old mentor, Rex Rice, headed memory system operations at Fairchild. It also
        may have been due to Fairchild\u2019s rapidly growing reputation as the new
        provider of semiconductors to the NASA program and certain Air Force programs.
        Regardless, in the end, Slotnick was right to put his faith in the Fairchild
        team because they successfully produced the required memory chips and the
        ILLIAC IV was the first to use chips of this design. Some believe that this
        project helped speed up the pace with which semiconductor memories became
        commercially available \u2014 although given the field was growing so quickly,
        it is hard to tell if such a claim is true.&nbsp;</p><p>Taking a step back,
        it is reasonable to question whether or not the ILLIAC IV\u2019s approach
        is the ideal strategy to orchestrate new technological developments. As always,
        the suitability of the approach depends on the situation and what the PM thinks
        is appropriate given the context. Developing so many new pieces of component
        technology to use, for the first time, in one machine seems to leave one obviously
        more prone to cost overruns and time delays than most projects. The upside
        is that the approach provides new funding and a focused direction to help
        bring what could become very useful component technology into existence. In
        the best case, the approach can push a field forward much faster than if one
        had taken a more measured approach. But the approach does also run the substantial
        risk of generating demand for and funding pieces of component technology that
        might be useful to the current project, but will not be useful to the field
        as a whole in later years. For example, if the components required by the
        design of the machine are not reflective of the needs of the field as it progressed
        outside of that individual project. </p><p>In essence, the&nbsp; approach
        of the ILLIAC IV team should be considered a high-risk, high-reward strategy.
        It probably won\u2019t achieve outcomes as stable as those of the gray coding
        approach covered in the following Connection Machine sections, but there are
        still situations where the full frontal assault approach of the ILLIAC IV
        has a high enough expected value to justify the risks. With this approach,
        one may run the risk of cost overruns, delays in producing a working machine,
        and developing components that the broader field turns out to not need. But
        the approach also might help a technological area make discontinuous leaps
        in component technology and overall system performance that would not happen
        by changing one component at a time. Particularly in the early days of a technological
        field, changing multiple design components at once is often an attractive
        approach given that existing approaches might be extremely far from what researchers
        know to be theoretically possible.<a class=\\\"footnote-anchor\\\" data-component-name=\\\"FootnoteAnchorToDOM\\\"
        id=\\\"footnote-anchor-3\\\" href=\\\"#footnote-3\\\" target=\\\"_self\\\">3</a>
        (See this earlier <a href=\\\"https://www.freaktakes.com/p/a-report-on-scientific-branch-creation\\\">FreakTakes
        piece</a> on the field of early molecular biology to read more about this
        strategy.)</p><p>Depending on the political and technical context of the project,
        the full frontal assault approach might be just what is required. But, in
        other situations, it might be considered far too risky. Stephen Squires, with
        his computer architectures program in the more mature DARPA computing portfolio
        of the 1980s, considered the approach far too risky.</p><h1>Connection Machine\u2019s
        Beginnings</h1><p>While the final ILLIAC IV machine did work, Squires took
        the management of the program as a cautionary tale of what not to do when
        developing machines within IPTO. In his eyes, the cost overruns and delays
        in producing the machine were entirely foreseeable given the structure of
        the project. Squires believed the \u201Cfull frontal assault\u201D approach
        of the ILLIAC IV project \u2014 with novel approaches to the machine\u2019s
        architecture, processors, memory, storage, and other components \u2014 pushed
        too many untried technologies at once. Problems in developing or implementing
        any one of the components could negatively impact the entire project. In the
        uncertain enterprise that is research, Squires believed this was an untenable
        risk profile.&nbsp;</p><p>Squires wanted to encourage projects funded through
        his architectures program to limit the number of new technologies a project
        worked on at a time \u2014 usually to one. He called the approach \u201Cgray
        coding.\u201D Roland and Shiman \u2014 who wrote the most extensive history
        on DARPA\u2019s Strategic Computing Initiative of the 1980s and extensively
        interviewed Squires \u2014 expanded on the thinking behind the approach as
        follows:</p><blockquote><p>That way, problems could be easily traced, isolated,
        and corrected, and the chances of catastrophic failure of the system would
        be minimized if not eliminated. Advance of the field would not occur in one
        fell swoop by \u2018frontal assault\u2019 of some \u2018point solution,\u2019
        but through the selected development and installation of key technologies,
        all well-timed and well-coordinated. Advance would occur over a series of
        generations. Once a technology or key concept had proven reliable, a new component
        or design would be implemented and tested.</p></blockquote><p>Squires portion
        of DARPA would still pursue several technological trajectories at once \u2014
        which Squires called the ensemble model \u2014&nbsp; to allow the research
        ecosystem to prove out which one held the most promise.&nbsp;</p><p>Additionally,
        learning from the ILLIAC IV example and his own experience working on systems
        software at the NSA, Squires insisted that software development be emphasized
        as much as hardware work for the architectures being developed. In the case
        of the ILLIAC IV, the development largely focused on getting the hardware
        working and later shifted efforts towards generating useful applications via
        software development. Many believe the ILLIAC IV team emphasizing the hardware
        first and software later was a large part of what took the machines so long
        to finally get working at the back half of the project. Squires was going
        to do things differently.</p><p>Squires planned for the architectures program
        to progress in three phases \u2014 each lasting roughly three years. The projects
        funded in the first phase would generally seek to refine promising architecture
        ideas and develop microelectronics components that would later be used in
        building parallel machines. Projects funded in the second phase would generally
        seek to build full prototypes of the most promising architectures. The projects
        funded in the third phase would integrate technology that now (hopefully)
        existed into composite systems that could support DARPA\u2013relevant applications
        \u2014 such as military applications in computer vision, natural language
        understanding, simulations, and more. Squires hoped that the machines being
        developed would achieve 100 million floating point operations per second by
        1987, a billion by 1989, and a trillion by 1992. In service of this goal,
        all of the early work on smaller-scale prototypes had to be easily scalable
        \u2014 for example, being designed in such a way that they could be easily
        expanded by simply increasing the number or power of processors in the prototype.
        As all of this work progressed, work on new programming languages, compilers,
        tools, and concepts that better served parallel programming would be also
        funded.</p><p>With the high level plan in place, Squires began by traveling
        around and further familiarizing himself with the pool of potential performers
        that might be funded and meeting with those in industry \u2014 such as computer
        manufacturers. In addition, this familiarizing process also entailed a February
        1984 call for qualified sources asking for potential contractors to explain
        what they could do in the areas of signal, symbolic, or multifunction processing
        if they were funded by Squires\u2019 architectures program.</p><p>Squires,
        at the time, primarily issued the formal solicitation for informational purposes
        \u2014 as a way to see what was out there, not necessarily planning to immediately
        award any contracts to the proposals submitted. But one proposal jumped off
        the page from the pool of submissions \u2014 two-thirds of which from industry
        and one-third from academia. That proposal was a big black notebook sent in
        by a newly formed company called Thinking Machines Corporation (TMC) outlining
        their technical and business plans for a computer called the Connection Machine.</p><h1>Connection
        Machine\u2019s Operations</h1><p>The proposed machine was exactly what Squires
        was looking for. The machine the company proposed was an ambitious, truly
        parallel hardware architecture that seemed like it could scale up by simply
        adding more or better processors. In spite of the ambitious design, Squires
        still saw the approach as conservative \u2014 in a positive way. Not only
        had the Thinking Machines Corporation (TMC) already developed a workable VLSI
        design for the processor chips \u2014 which was one notable source of technological
        risk already mitigated \u2014 but the company also set out to construct the
        architecture using as many commonly used components as possible. In fact,
        in several areas they went as far as to use components that were far simpler
        than the ones commonly used in other machines at the time. One example is
        that, for its first machine, TMC planned to use the simplest 1-bit processors
        instead of the more powerful 4, 8, or 16-bit processors commonly in use at
        the time. In addition, the Connection Machine would not even be a stand-alone
        machine in the early iterations. The front end used to access the machine
        would be a more commonly used computer \u2014 a LISP machine in this case.
        In this early stage, SC\u2019s main goal was to demonstrate the feasibility
        of basic parallel processing concepts while not adding needless complications
        to projects that did not serve the main goal. TMC \u2014 maybe not surprisingly
        since its founder came from a lab that was quite used to working with DARPA
        PMs \u2014 clearly got the memo and submitted a proposal that perfectly complied
        with the gray coding approach while maintaining an ambitious vision.</p><p>After
        some meetings between DARPA staff and TMC executives, it seemed clear to DARPA
        that TMC as a venture was well thought out from both the business and technical
        perspectives. In early 1984, $3 million (~$9 million today) was approved to
        fund TMC\u2019s work on a small prototype of the machine containing 16k processors
        \u2014 with the option for an additional $1.65 million to scale up the prototype
        to 64k processors. Raising an additional $16 million in private investments
        around this time from private investors, TMC was well-capitalized and ready
        to begin work on its first machine.</p><p>Even having laid out a plan of action
        that complied with Squires\u2019 gray coding framework, the work was daunting
        for the company. Danny Hillis, the CEO of TMC who came up with the idea for
        the Connection Machine during his Ph.D. under Marvin Minksy at MIT, said the
        early work building the simplified version of the Connection Machine underway
        in 1984-1985 was \u201Coverwhelming.\u201D He continued, describing the early
        work of the company in a 1989 <em>Physics Today</em> article:</p><blockquote><p>We
        had to design our own silicon integrated circuits, with processors and a router.
        We also had to invent packaging and cooling mechanisms, write compilers and
        assemblers, devise ways of testing processors simultaneously, and so on. Even
        simple problems like wiring the boards together took on a whole new meaning
        when working with tens of thousands of processors. In retrospect, if we had
        had any understanding of how complicated the project was going to be, we never
        would have started.</p></blockquote><p>This reflection from Hillis probably
        lends some credibility to Squires\u2019 gray coding approach having been a
        good fit for this particular project. The design and need to develop new software
        approaches for making use of the novel design would also bring with it more
        difficulties. There was no need to compound on these difficulties by ensuring
        the initial prototype had the most cutting-edge component tech possible. Novel
        engineering development projects like this were sure to run into plenty of
        issues that required novel solutions to overcome. As Hillis stated, they ran
        into so many of these in the early days that they might not have even undertaken
        the project had they understood how painful it would be. As the project grew
        more complicated, Hillis seemed thankful to have Manhattan Project alum Richard
        Feynman around as an intern for some summers \u2014 Feynman\u2019s son Carl
        had worked with Hillis when Hillis was a grad student and Carl was an undergrad
        at MIT. Feynman seemed relatively influential in pushing the company to take
        a page out of the Manhattan Project\u2019s management playbook and designate
        a group lead in each area of technology \u2014 such as software, packaging,
        electronics, etc. Feynman \u2014 who was initially brought on to help brainstorm
        applications for the computer \u2014 also pushed the company to hold regular
        seminars of invited speakers, often scientists, who the company believed might
        have interesting use cases for the machine.</p><p>While the project did have
        its difficulties in the early years, in spite of Hillis\u2019s statement about
        the project being \u201Coverwhelming,\u201D the project was considered the
        gem of the architectures portfolio in the early years of SC. Squires\u2019
        program had also invested in other prototype-stage projects for at least a
        two year trial stage \u2014 which all had some positive developments as well
        as some downsides. Each of the general-purpose, prototype-stage projects was
        meant to help in the architectures program\u2019s larger goals of identifying
        promising approaches to parallel computing and providing experience working
        with the technology. Simultaneously, smaller projects called \u201Caccelerators\u201D
        were funded with the goal of doing development work to produce components
        that could later improve the performance of the general-purpose machines.
        Besides the Connection Machine, some of the architectures program\u2019s other
        general-purpose prototype projects included:</p><ul><li><p>BBN\u2019s Butterfly
        computer series</p><ul><li><p>The Butterfly computer had been in the DARPA
        funding pipeline since 1977, with funding going to one of DARPA\u2019s workhouse
        computing research contractors, BBN. At the start of the architectures program,
        a model of the Butterfly with 10 processors already existed. BBN received
        funding to continue to scale the prototype to 128 processors. The butterfly
        computer was a coarse-grained shared-memory machine \u2014 coarse-grained
        machines have fewer but more powerful processors capable of running entire
        programs in a moderately parallel but not truly parallel fashion. This technology
        was both further along in development than smooth-grained parallel computers
        \u2014 like the Connection Machine \u2014 were. Additionally, machines like
        the Butterfly could easily make use of traditional programming methodologies
        rather than developing new ones. For those reasons, the Butterfly was meant
        to serve as the benchmark against which progress on other machines in the
        portfolio would be measured.&nbsp;</p></li></ul></li><li><p>CMU\u2019s Warp</p><ul><li><p>The
        architectures program also contracted CMU to build Warp. Warp was a systolic
        array machine that sought to achieve more efficient performance using an approach
        in which processors are connected sequentially and data flows from one processor
        to the next with each performing a different operation \u2014 analogous to
        an assembly line. CMU had received this contract based on its prior demonstration
        of the systolic approach having built a programmable systolic chip.</p></li></ul></li><li><p>Columbia\u2019s
        DADO</p><ul><li><p>DADO was designed as a coarse-grained parallel machine.
        DADO was one of two tree-structured machines \u2014 in which a central processing
        unit was connected to two others, and each of those to two others, etc. \u2014
        started at Columbia University in 1981. DARPA discontinued funding for this
        project after the early prototype stage.</p></li></ul></li><li><p>Columbia\u2019s
        Non-Von</p><ul><li><p>The second of Columbia\u2019s two tree-structured machines,
        the Non-Von, was somewhere in between the DADO and the Connection Machine
        in terms of how fine-grained it was. A machine like this one would likely
        prove simpler to program than a Connection Machine, but would also not achieve
        quite the level of parallelization. The Non-Von sought to prove useful in
        managing and manipulating large databases. DARPA also discontinued funding
        for this project after the early prototype stage.</p></li></ul></li><li><p>TI\u2019s
        compact LISP machine</p><ul><li><p>TI sought DARPA funding to fund its development
        of a miniature version of a LISP machine. To do this, TI planned to implement
        LISP processors onto individual chips. TI hoped that the entire machine would
        fit on nine small printed circuit cards and, in the end, would be small enough
        to be embedded in computers of military systems such as within a pilot\u2019s
        cockpit to help in expert decision making. DARPA agreed to the $6 million
        in hardware costs and TI, sharing the costs, funded the $6 million in software
        development.</p></li></ul></li></ul><p>But none of these projects seemed to
        carry as much promise as the Connection Machine and its model of performing
        identical operations simultaneously across its many, many processors. However,
        each of the other machines did often have at least one piece of component
        technology in development that DARPA was intrigued to pursue further. For
        example, the Non-Von project developed a way of using multiple disk heads
        in operating its storage system. TMC later incorporated this idea into one
        of its later prototypes \u2014 all SC-funded developments were eligible to
        be used by other SC contractors. This is just one case of discontinued portfolio
        projects still finding help push the technology base forward in Squires\u2019
        portfolio.</p><p>As the Connection Machine project got underway, Hillis had
        arranged for DARPA funding to install a LISP machine at the chip foundry TMC
        worked with in California. With that machine installed, TMC could test chips
        at the foundry rather than having them shipped to Massachusetts to test them
        there. By mid-May 1985, a little over a year after TMC had received its first
        DARPA grant, the 16k processor prototype had been finished \u2014 more than
        a month ahead of schedule. After DARPA checked out the machine in the TMC
        offices, DARPA immediately invoked the option for the 64k processor machine.
        This scaled-up prototype of the machine, too, would be finished ahead of schedule
        \u2014 by the end of 1985. Hillis\u2019 design seemed to be scaling the way
        he and DARPA had hoped. And for all of the sub-problems that the initial design
        could not account for, the very talented (and growing) TMC research team \u2014
        largely drawn from the staff and students of MIT, CMU, Yale, Stanford, and
        other elite pools of computer engineering development talent \u2014 was proving
        more than up to the task of solving.&nbsp;</p><p>By the spring of 1986, the
        first Connection Machine \u2014 the CM1 \u2014 was being manufactured and
        was available for sale. The machine\u2019s chips each contained 16 customized
        1-bit VLSI processors. Thirty-two of these chips would be printed on a circuit
        board and mounted next to each other \u2014 eventually making up four separate
        quadrants of 16k processors. Each quadrant could stand alone as a cheaper,
        scaled-down model of the 64k processor machine. In spite of the novelty of
        the truly parallel design of the architecture, each of the components remained
        quite safe and reliable to use \u2014 as the gray coding approach intended.
        The 1-bit processors were basic and slow compared to the state-of-the-art
        processors at the time, but they also produced so little heat that the machine
        did not require much cooling. Installation of the machine on a buyer\u2019s
        site was easy and could be readily set up with a LISP machine on-site \u2014
        which customers knew how to use. However, customers did have to learn new,
        Connection Machines-workable, versions of C and LISP to make full use of the
        machine\u2019s capabilities while programming. The user did not need to fully
        understand how memory assignment and other aspects of the machines worked
        with these languages, but learning the languages was a necessity.</p><p>With
        the project going well, in the beginning of 1986 DARPA funded TMC for an additional
        $3.8 million over two years to find ways to exploit the CM1 and CM2 on various
        scientific and military applications. While the Connection Machines were still
        in the process of becoming truly useful machines, this particular generation
        of DARPA\u2019s computing programs carried a strong political emphasis on
        pushing successful research projects into application mode as early as possible.
        This iteration of DARPA funding for TMC included covering work on projects
        to further improve software development practices for the machine, develop
        the machine so it could be used as a network\u2019s server, increase storage
        capabilities to aid in work on data-intensive problems, and developing a training
        program to help others learn to use the machine. Beyond this $3.8 million,
        DARPA would also serve as TMC\u2019s biggest buyer in the coming years.&nbsp;</p><p>With
        this DARPA funding, the work continued on into more mature prototyping and
        scaling stages. As of 1987, around twelve CM1s had been purchased from TMC
        \u2014 about half of them purchased by DARPA for its contractor community.
        The machines were typically purchased for around $5 million a piece in 1987
        (~$14 million in 2023). As the TMC team sought new applications for its machines,
        under Hillis\u2019 leadership, they eagerly sought out projects with many
        in the research community. This included partnering with physicists, astronomers,
        geologists, biologists, and chemists to understand the details of their applications.
        Someone like Feynman worked in application areas at TMC such as problems in
        database searches, geophysical modeling, protein folding, analyzing images,
        and simulated evolution. Establishing partnerships with researchers proved,
        unsurprisingly, quite natural to the organization. It should be remembered
        that, during his graduate work, Hillis designed the machine to be something
        like an optimal tool for researchers studying artificial intelligence \u2014
        as well as other complex research problems. People like himself, his former
        lab mates in Minsky\u2019s lab, and other researchers from similar environments
        were the kinds of people who staffed the company as well as its initial, intended
        customers.</p><p>The company did not prove nearly as eager to establish profitable
        partnerships with industry as it did partnerships with researchers. Of course,
        not all of the problems the company explored applying the machine to were
        research related. For example, Feynman also worked on one application area
        related to reading insurance forms. However, this sort of application work
        did seem to be a less natural fit for Hillis and the company.&nbsp; Hillis,
        after all, saw the Connection Machine as a machine for science. In practice,
        TMC very clearly preferred not to make compromises to the goal of pushing
        science forward in search of increased revenue. For now, DARPA was TMC\u2019s
        guardian angel when it came to buying machines, helping broker sales with
        research consumers of the machines that DARPA didn\u2019t buy itself, and
        funding the company\u2019s R&amp;D work.</p><p>The CM1 \u2014 which TMC largely
        relied on DARPA to facilitate purchases for \u2014 did not only have limited
        market potential outside of the DARPA universe because of the price tag of
        its machines, but also, simply, because of what they just couldn\u2019t do.
        From a consumer standpoint, the first major deficiency was that the machines
        could not yet run the standard computer language of the scientific community:
        FORTRAN. The second major deficiency was even though the CM1 could perform
        ten times as many computations per second as many contemporary, commercial
        supercomputers \u2014 2.5 billion operations per second vs. a few hundred
        million \u2014 the 2.5 billion operations it could do per second were not
        floating-point operations. And floating-point operations were vital to most
        computationally intensive research problems at the time. Nevertheless, DARPA
        likely felt comfortable facilitating these purchases because much of the SC
        program emphasized folding other SC-developed component technologies into
        projects sooner rather than later. At the beginning of SC, DARPA had also
        facilitated the purchases of the young BBN Butterfly computer to some of its
        contractors in order to ensure they had the most up-to-date technology available
        to them and their development work. So, TMC was not the only contractor in
        the architectures portfolio to have received this sort of favorable treatment
        from DARPA.</p><p>With the deficiencies of the CM1 in mind, TMC got to work
        incorporating the clearly needed functionality into its CM2. Surely this lack
        of functionality, at this stage in the development process, was not considered
        a failure by any means. The first successful prototype \u2014 the CM1 \u2014
        lacking much needed functionality from the user\u2019s perspective was likely
        just a symptom of the gray coding approach. The project was likely unfolding
        as Squires hoped successful architectures projects would \u2014 in successive
        generations. By this time, the architectures program, headlined by TMC,&nbsp;was
        clearly proving that parallel computing could be very workable. From a technology
        development standpoint, that was no small thing, even if the tech was not
        the most powerful or user friendly yet.&nbsp;</p><p>The CM2 was brought to
        market in April 1986 and could run FORTRAN as well as do floating-point operations.
        This version of the machine was much more usable to the scientific community
        and the machine's very novel, massively parallel architecture still required
        its customers to learn to use special software and new programming techniques.
        Using these programming languages proved difficult for even the machine\u2019s
        quite savvy researcher-customers and far more trouble than it was worth for
        almost all commercial customers. An additional factor that made the machine
        more trouble than it was worth for many corporate customers was that the machine's
        rate of breakdowns and subsequent downtime, while not an issue to academic
        work schedules, was far more than corporate customers were comfortable with.</p><p>Some
        in the research community found useful applications for the new machines in
        areas such as fluid flow dynamics and large database searches. Yet, in spite
        of these successes, the machines were not yet considered very usable to their
        potential customers. Years later, Dave Waltz \u2014 the head of TMC\u2019s
        AI group \u2014 recounted that most of the CM2\u2019s early customers were
        not using the machine correctly. In his eyes, TMC had built a machine that
        worked far more like a human brain than a sequential computer like the Cray
        \u2014 the most successful commercial supercomputer company at the time \u2014
        but, he noted, that people actually knew how to write programs for a Cray!
        Many of the customers were running&nbsp; their computations in such a way
        that they used the floating point processors in the CM2s in a standard way
        and largely ignored the machine\u2019s novel, 64,000 single-bit processors.
        But, with the technology continually progressing, DARPA continued to help
        TMC facilitate sales. As one of TMC\u2019s research directors later recounted,
        <strong>&nbsp;</strong>\\\"Our charter wasn't to look at a machine and figure
        out the commercial profit. Our charter was to build an interesting machine.\\\"</p><p>As
        the Cold War waned and the acute need for military supercomputing and near-term
        AI applications lessened, the Bush administration and the president\u2019s
        main science advisor \u2014 nuclear physicist Allan Bromley \u2014 turned
        its eye towards helping solve \u201Cgrand science challenges.<a class=\\\"footnote-anchor\\\"
        data-component-name=\\\"FootnoteAnchorToDOM\\\" id=\\\"footnote-anchor-4\\\"
        href=\\\"#footnote-4\\\" target=\\\"_self\\\">4</a>\u201D These challenges
        included problems related to climate modeling, analyzing protein folding patterns,
        mapping the human genome, earthquake prediction, and learning more about the
        underpinnings of quantum mechanics. Bromley seemed to believe that these problems
        required enormous computing power rather than artificial intelligence. From
        the modern perspective, smart people could argue whether or not Bromley was
        right in that idea. But, surely, massive increases in computing power paved
        the way for specific solutions in those problem domains \u2014 even if novel
        AI methods were what pushed some of those things across the finish line. Regardless,
        that general ethos was what drove the government\u2019s new High Performance
        Computing and Communications (HPCC) program. DARPA was the lead agency in
        this work and received an additional budget of several billion dollars through
        1996 for the program. One of the primary goals of the new program was a computer
        that could achieve one trillion computations per second.</p><p>Naturally,
        DARPA turned to TMC and gave the company some of this funding to help the
        company expand on its work that was so successful thus far. DARPA granted
        the company a $12 million initial contract to produce a scaled-down machine
        that could theoretically hit the trillion computations per second benchmark
        by 1992. It was in this period when TMC was working on the CM5 \u2014 TMC
        CEO Sheryl Handler skipped \u201CCM3\u201D and \u201CCM4\u201D in the hopes
        that the naming curveball would make espionage harder for would-be infiltrators
        \u2014 that the music stopped for the young technology darling.</p><p>In this
        period, as TMC was hitting its objectives and in a great place from a technology
        development standpoint, they began to spend as if they were having equivalent
        levels of commercial success \u2014 which they were not. Company executive
        Sheryl Handler had been brought in to co-found the company to bring a level
        of business maturity to the operation, but that proved to be a mistake. Her
        corporate spending habits were, putting it charitably, a bit decadent. This
        habit showed itself early on in the company\u2019s lifespan and continued
        to worsen as the 1980s wore on. As early as 1984, the very young TMC moved
        into the top two floors of the expensive Carter Ink building and began spending
        money on modern, luxurious tech company-like amenities. A couple of these
        expenses included shelling out for a custom design plan and a gourmet chef.
        In 1989, as TMC\u2019s work on the CM5 was getting underway, Handler signed
        a whopping 10-year lease with the Carter Ink building which cost the company
        $6 million a year \u2014 the $37 per square foot for the lease was reportedly
        4.5 times higher than Lotus Development Corp. was paying just down the road.
        Additionally, management rapidly increased headcount by around 40% around
        this time \u2014 resulting in a staff of around 400. The company\u2019s spending
        was more in line with the company\u2019s growing status in the computing world
        than its actual financial standing. As Stephen Wolfram described the company\u2019s
        status at the time, the company was \u201Cthe place that foreign trade delegations
        would come to visit to see where American business was at these days.\u201D
        An IBM computer scientist put it a little differently when he described the
        company as having cornered the market \u201Con sex appeal in high-performance
        computing.\u201D</p><p>While TMC was in the process of developing machines
        that were on their way to doing things that no other computer could seemingly
        do at the time, they were not producing machines that the market wanted as
        measured in total dollars of actual market demand. With the launch of the
        government\u2019s HPCC initiative, the company\u2019s research emphasis shifted
        even more strongly into building the most powerful computer possible by 1992.
        As it did this, the company continued to put off making the developments it
        needed to make to find some level of product market fit. The company had produced
        a machine that cost millions, but was almost exclusively sourcing its customers
        from the research community. The number of customers and the budget per customer
        in this community was far more limited than it was in the corporate sphere.</p><p>As
        work on the CM2 and later CM5 progressed, it does seem that the company had
        at least some opportunity to slowly ramp up efforts in the commercial sector.
        For example, in the late 1980s TMC sold two Connection Machines to American
        Express as database mining for improved customer analytics was a growing trend
        at the time. There was some internal debate about whether or not to start
        a business supercomputing group at the company \u2014 which would be an obvious
        \u201Cyes\u201D for most young and growing companies with shareholders \u2014
        but nothing much came of it. The company, in practice, clearly preferred to
        serve the research vision that drove its founder and employees to pursue computing
        over maximizing shareholder value.</p><p>As the work got underway on the CM5,
        progress was not as smooth as it had been on the previous two generations
        of machines. The first CM5 would not be completed until October 1991 \u2014
        and even then it was not quite as powerful as promised because there was a
        delay in producing chips that fit the machine\u2019s design. However, throughout
        this 1988 to 1991 period, TMC was peaking from a revenue standpoint. In 1989,
        thanks to the company\u2019s good reputation within DARPA\u2019s walls, the
        company achieved its first profit \u2014 $700,000 profit on revenues of $45
        million. Then, in 1990 the company saw profits of $1 million on revenues of
        $65 million. This would be the peak of TMC\u2019s profits.</p><p>Sadly, for
        TMC and the world of early 1990s parallel computing, that was as good as things
        would ever get for the firm. The company\u2019s introduction of an early version
        of the CM5 in late 1991 was lauded by Hillis as possessing the highest theoretical
        peak performance if you added enough processors to it \u2014 but that particular
        version of the machine was less powerful than the CM2. It\u2019s very possible
        that with more DARPA funding and further work upgrading the prototype, this
        branch of the technology base would have continued to progress from within
        the expensive walls of the Carter Ink building, but that never came to be.</p><p>In
        August 1991, in the midst of the HPCC initiative\u2019s leaders beginning
        to make plans for how to deploy most of its sizable budget moving forward,
        the <em>Wall Street Journal</em> released a piece diving into DARPA\u2019s
        subsidies of at least two dozen Connection Machines in addition to the machines
        it had bought. While this in and of itself was not necessarily illegal or
        even nefarious, this particular political era was one in which anything that
        looked like \u201Cindustrial policy\u201D or government interference in markets
        was extremely scrutinized. DARPA buying its contractor pool BBN Butterfly
        machines and some machines that resulted from CMU\u2019s Warp project does
        not seem to have received negative press in the same way. This disparity in
        PR outcomes might be because buying machines that might soon become obsolete
        due to technology in development within your portfolio did not come off as
        bad as subsidizing machines that were not yet considered to be truly finished,
        market-competitive products.&nbsp;</p><p>Regardless of why the negative press
        came to be, the Bush administration made a point of ensuring that \u201Cindustrial
        policy\u201D-like help to TMC did not continue, and the company did not fare
        so well when it was abruptly thrust out into the competition of the commercial
        market. In 1992, TMC reported a $17 million loss. Massive layoffs and spending
        cuts began around this time. A new CEO was brought on who attempted to broker
        some kind of merger or partnership deal with Sun Microsystems and IBM, but
        TMC\u2019s balance sheet was more toxic than its tech was impressive. To give
        the reader an idea: TMC owed $36 million in rent to the Carter Ink Building
        over the next six years for its ill-advised ten-year lease. And that was just
        one of the apparently many dark spots on its balance sheet. A deal like the
        one the new CEO sought could surely have been brokered around 1989 or 1990,
        when the company still seemed several years ahead of the field in the rapidly
        developing field of parallel computation and was beginning to show profits.
        However, the company had rejected offers for mergers and acquisitions at that
        time.</p><p>TMC filed for Chapter 11 bankruptcy in late 1993 and re-emerged
        from this as a small software company attempting to sell programs to run on
        their former competitors' parallel computers.</p><div class=\\\"captioned-image-container\\\"><figure><a
        class=\\\"image-link is-viewable-img image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg\\\"
        width=\\\"384\\\" height=\\\"384\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:600,&quot;resizeWidth&quot;:384,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe65eb2d1-9b40-45a0-856a-47f49f905602_600x600.jpeg
        1456w\\\" sizes=\\\"100vw\\\" loading=\\\"lazy\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">Photo
        of the CM-1 in 1985. I usually share all the photos on this Substack in black
        and white, but I shared this one in color so maybe the reader could recognize
        the computer from the CM series\u2019 cameo in the Jurassic Park movie. Photo
        via the Computer History Museum. https://www.computerhistory.org/revolution/supercomputers/10/73/284</figcaption></figure></div><h1>Connection
        Machine\u2019s Results</h1><p>Thinking Machines had been capable of hanging
        onto the dream of building a machine that helped raise all boats in the research
        community when it was fueled by DARPA funding. With DARPA funding, it was
        able to largely ignore the commercial market and commit itself to pushing
        the technology base forward with DARPA\u2019s relatively rare form of risk-tolerant,
        farther-sighted capital. And they were doing a good job of meeting DARPA\u2019s
        needs. However, TMC was spending its privately raised capital as if they were
        doing an equally good job courting commercial customers as they were hitting
        computing benchmarks. And that was absolutely not the case. Of the 125 machines
        the company sold in its entire history, only ten percent were to commercial
        customers. So, when the DARPA relationship took a turn, the company inevitably
        had to file for Chapter 11 bankruptcy after some rushed attempts at solvency.</p><p>The
        company surely succeeded in its primary goal: producing a machine that proved
        truly parallel machines could work. Also, the firm did this while developing
        its technology, step-by-step, in a way that fit with DARPA\u2019s preferred
        approach to moving the architectures program\u2019s technology base along
        at the time: gray coding. From a technological standpoint, for the first five
        or six years it seems the company did an effective job of moving technological
        progress forward \u2014 in the form of a machine with steadily increasing
        functionality \u2014 on a consistent timeline that DARPA was very happy about.
        From a commercial perspective, however, the company was clearly a failure.
        Hillis himself admitted as much. At the time of the company\u2019s bankruptcy,
        Hillis was asked how he felt. He replied, \u201CSad, that the people who put
        so much in \u2014 the employees and the investors \u2014 won't necessarily
        be the ones who benefit. And sad that I haven't been able to build a technology
        success into a business success.\u201D DARPA surely got more out of the program
        for what it spent than the company\u2019s investors did.&nbsp;</p><p>In retrospect,
        DARPA\u2019s general thinking on the problem area seems to have proven quite
        prescient. A major application area like image processing was exactly what
        they believed building a truly parallel processing machine like the Connection
        Machine could do. In the seminar series that Feynman encouraged TMC to put
        on, they even had a speaker from a not-so-popular field called neural networks
        come and talk. This story is surely proof that the DARPA ecosystem was, in
        many ways, functioning quite well \u2014 even if no commercial success came
        from TMC. Hillis and individuals at DARPA were in contact, talking about securing
        money for the idea contained in his Ph.D. thesis, at least as early as 1983.
        DARPA\u2019s awareness of his idea and the young company almost from the moment
        of inception \u2014 as well as its recognition of Hillis\u2019 ability to
        lead such an operation from a technical perspective \u2014 should surely be
        seen as a feather in DARPA\u2019s cap. This awareness, of course, was possibly
        facilitated by Hillis\u2019 prior connection with Minsky\u2019s DARPA-funded
        lab at the heavily DARPA-funded MIT. But these kinds of interpersonal networks
        at key universities that were very good at certain sub-areas of research \u2014
        such as MIT and CMU in AI research \u2014 were what DARPA PMs made it their
        business to maintain. That sort of relationship management, first and foremost,
        will always be one of the primary pieces of the puzzle that determines whether
        or not ARPA-like organizations succeed or fail.</p><p>A final note on TMC\u2019s
        results: it is very possible that even a change as small as Hillis partnering
        with a much more responsible, business side co-founder than Handler \u2014
        who proved adept at fundraising, but deficient in many key areas that TMC
        required \u2014 could have been enough to shepherd the company to a longer
        lifespan. From a commercial strategy standpoint, an individual like this may
        have helped TMC find commercial uses that fit in with the company\u2019s goals
        and culture. From a financial standpoint, a better CEO might have ensured
        that the company either did not raise money from misaligned capital pools
        or did not spend the money it did raise as poorly as it did.&nbsp;</p><h1>Connection
        Machine\u2019s Lessons Learned (and Caveats)</h1><p>One quote that encapsulates
        several of the PM-relevant lessons from the Thinking Machines Corporation\u2019s
        work and death was said by Hillis as the company was filing for bankruptcy,
        \u201CThe real money is in handling Wal-Mart\u2019s inventory rather than
        searching for the origins of the universe.\u201D From the perspective of a
        pure venture capitalist, this quote might read as the complaint of an academic
        wishing that the financial markets rewarded what researchers found important.
        Of course, that is not the ideal perspective most venture capitalists would
        not want in their portfolio founders. But the quote can also be read as Hillis
        calling attention to what might be a very major market inefficiency. Research
        companies like Thinking Machines are great vehicles for building new, exploratory
        technology, but there is often a very salient tradeoff between a company like
        this moving the technology base forward and profit-maximizing for shareholders
        in the near term.</p><p>Let\u2019s not forget, the ILLIAC IV project did not
        only run into difficulties because its team was attempting to build a machine
        with too many novel components at once. Slotnick\u2019s \u201Cbattleship in
        a bathtub\u201D remark was explicitly describing how ill-suited modern universities
        had become to building big, new machines. Hillis did not make the identical
        mistake of trying to build the Connection Machine from within a university,
        but he still did make a mistake in choosing his organizational setup. TMC
        ran into the conundrum of raising money from private investors \u2014 who
        would naturally prefer the company do things like build a computer to handle
        Wal-Mart\u2019s inventory \u2014 as well as a funder (and initial customer)
        in DARPA that primarily cared about building a computer to move technology
        forward. In most cases, a company is going to make one of its two (rather
        different) funders unhappy in a situation where their incentives are only
        partially aligned.</p><p>Had Thinking Machines only pursued DARPA funding
        and spent more frugally \u2014 as an entity like ISI, which ran DARPA\u2019s
        MOSIS program, would have done \u2014 it is very possible the company would
        have continued to progress as the darling of DARPA\u2019s architectures program
        for the foreseeable future. If the company would have taken this approach,
        it could still have raised money from private markets at some point, just
        later on in its existence when it had a product that was closer to market-ready
        and had a more natural customer. Additionally, the company could have attempted
        to find a variety of customers so it would not have all its eggs in the DARPA
        basket the way BBN (covered at length in a coming piece) was known to have
        done in the 1960s and 1970s. BBN, in its early existence, did not seem to
        be purely profit-maximizing. Rather, it sought a variety of contracts \u2014
        from DARPA, private philanthropies, engineering industry, and more \u2014
        that its researchers found new and useful, but that still paid enough to facilitate
        a rather profitable operation.</p><p>Of course, none of this alternative history
        brings any assurance of success either. The political winds may have blown
        against TMC one way or another regardless. It should be noted, though, that
        Hillis\u2019 Wal-Mart quote was predictive in some sense. The company that
        did make possibly the biggest parallel computing breakthrough in the late
        1990s \u2014 NVIDIA with its new GPU \u2014 found a way to fund its breakthrough
        research by servicing a market that required high-tech solutions and was surprisingly
        large given how unimportant it was in the grand scheme of things: gaming graphics.&nbsp;</p><p>It
        is also important to note that, in the first five or six years of TMC\u2019s
        existence, it seems that the gray coding approach was working just as Squires
        had hoped. TMC was often putting out versions of Connection Machines that
        did not have the functionality or usability that many users would have wanted.
        But that could be considered a marketing issue as much as anything else. In
        bringing new functionality to the machines in this steady way, this gradual
        decrease in user dissatisfaction seems par for the course. The rate of progress
        of the Connection Machine seems, generally, far preferable to the messy process
        that was the ILLIAC IV development. Of course, individual PMs will know best
        when either approach is most suitable to a given project or portfolio. A variety
        of factors can impact the decision and there is no one-size-fits-all solution.
        One factor that could impact a decision like this, for example, is the higher
        the probability of failures or surprises in developing components in a given
        project, the more gray coding might be the clearly responsible approach. However,
        there are obviously also cases in which multiple components need to be changed
        between iterations of a machine for anything truly new to come of the process.
        When to apply either approach is, as always, best left up to a smart PM.</p><p>While
        the legacy of the Connection Machine is a complicated one, it was still the
        gem in an architectures portfolio that accomplished what it sought to do.
        As Roland and Shiman put it:</p><blockquote><p>The first stage of the SC architectures
        program proved unquestionably that parallelism was viable and did have promise
        in solving many real-world problems\u2026According to Professor John Hennessy
        of Stanford University, a respected authority on microelectronics and computer
        architectures, computer designers before SC, in their quest for greater speed
        and performance, kept running into the wall posed by the fundamental physical
        limits to the improvement of conventional systems and components. Each architect
        would try, and each would join the pile at the base of the wall. It was Squires,
        he said, who showed the way, and, through parallelism, led designers over
        the wall.</p></blockquote><p></p><p>Hopefully the collection of history and
        lessons in this post prove of some use to PMs thinking through decisions like
        those that arose in the ILLIAC IV and CM1-CM5 projects.</p><p class=\\\"button-wrapper\\\"
        data-attrs=\\\"{&quot;url&quot;:&quot;https://www.freaktakes.com/p/illiac-iv-and-the-connection-machine?utm_source=substack&utm_medium=email&utm_content=share&action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\\\"
        data-component-name=\\\"ButtonCreateButton\\\"><a class=\\\"button primary\\\"
        href=\\\"https://www.freaktakes.com/p/illiac-iv-and-the-connection-machine?utm_source=substack&utm_medium=email&utm_content=share&action=share\\\"><span>Share</span></a></p><p
        class=\\\"button-wrapper\\\" data-attrs=\\\"{&quot;url&quot;:&quot;https://www.freaktakes.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe
        now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\\\" data-component-name=\\\"ButtonCreateButton\\\"><a
        class=\\\"button primary\\\" href=\\\"https://www.freaktakes.com/subscribe?\\\"><span>Subscribe
        now</span></a></p><p><em>Stay tuned for next week\u2019s pieces! They explore
        DARPA\u2019s ~1980 work in speech and computer vision applications, expert
        systems, and its programs for moving the field of chip research forward. </em></p><ul><li><p><em>Coming
        Sunday</em></p><ul><li><p><em>MOSIS: The 1980s DARPA 'Silicon Broker'</em></p></li></ul></li><li><p><em>Coming
        Monday/Tuesday</em></p><ul><li><p>The Autonomous Land Vehicle, Pilot's Associate,
        and Battle Management: Three North Star Application Projects from DARPA's
        Strategic Computing Portfolio</p></li></ul></li></ul><h4><strong>Specific
        Links:</strong></h4><p><strong>ILLIAC IV</strong></p><ul><li><p>Slotnick\u2019s
        <a href=\\\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4392921\\\">The
        Conception and Development of Parallel Processors: A Personal Memoir</a></p><ul><li><p>An
        extremely useful source. Short, but quite thorough. Dives further into Slotnick\u2019s
        journey from grad student to eventually running the ILLIAC IV project and
        his perspectives and thoughts on the events as they happened.&nbsp;</p></li></ul></li><li><p>Chapter
        17 of Van Atta\u2019s <a href=\\\"https://apps.dtic.mil/sti/citations/ADA239925\\\">DARPA
        Technical Accomplishments: An Historical Review of Selected DARPA Projects,
        Volume 1</a></p><ul><li><p>A 1990, third party perspective on the planning,
        operations, and lessons learned from the ILLIAC IV project</p></li></ul></li><li><p>Chapter
        5, pages 160-164 of <a href=\\\"https://amzn.to/3QFebf3\\\">Strategic Computing:
        How DARPA Built the Computer Age</a>&nbsp;</p><ul><li><p>Particularly emphasizes
        the DARPA internal memory and different interpretations of the ILLIAC IV projects
        at DARPA during the 1980s.</p></li></ul></li><li><p>Chapter 6, pages 264-268
        of <a href=\\\"https://www.amazon.com/dp/0801851521?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details\\\">Transforming
        Computer Technology: Information Processing for the Pentagon, 1962-1986</a></p><ul><li><p>Particularly
        emphasizes how IPTO first got into parallel computing and the plans and early
        project planning of the ILLIAC IV project from a high level</p></li></ul></li><li><p>Chapter
        2 of <a href=\\\"https://www.amazon.com/dp/0801851521?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details\\\">Transforming
        Computer Technology: Information Processing for the Pentagon, 1962-1986</a></p><ul><li><p>Particularly
        dives into much of the early administrative history of IPTO \u2014 including
        things like budgets, norms for discussing and sourcing projects, etc.</p></li></ul></li><li><p>Slotnick\u2019s
        <em>Scientific American </em>article, <a href=\\\"https://www.jstor.org/stable/pdf/24927727.pdf?refreqid=excelsior%3A4f0e2e24a1a28f71d08d31ef99082736&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1\\\">The
        Fastest Computer</a></p><ul><li><p>A 1971 article outlining the project\u2019s
        goals and achievements for a public audience.&nbsp;</p></li><li><p>The article
        also contains the performance metrics and specs of various aspects of the
        machine as of 1971.&nbsp;</p></li><li><p>The article also spends several pages
        walking through an example problem from mathematical physics, walking the
        reader through how the ILLIAC IV \u2014 or any parallel processor for that
        matter \u2014 would perform its operations and solve the problem faster than
        a traditional computer at the time.&nbsp;</p></li></ul></li></ul><p><strong>Connection
        Machine</strong></p><ul><li><p>Chapter 5, pages 158-184 of <a href=\\\"https://amzn.to/3QFebf3\\\">Strategic
        Computing: How DARPA Built the Computer Age</a>&nbsp;</p><ul><li><p>Provides
        more detail on the history of Squires\u2019 approach to the architectures
        program and other projects that made up the portfolio. Some details about
        how the projects were run can also be found there.&nbsp;</p></li></ul></li><li><p><a
        href=\\\"http://www.bitsavers.org/pdf/thinkingMachines/The_Rise_and_Fall_of_Thinking_Machines_1995.pdf\\\">The
        Rise and Fall of Thinking Machines</a></p><ul><li><p>A thorough article diving
        into the lifespan of the company from a business and cultural perspective.
        The piece covers Thinking Machine\u2019s corporate decision-making and relationships
        from its early years until its bankruptcy.</p></li></ul></li><li><p><a href=\\\"https://longnow.org/essays/richard-feynman-connection-machine/\\\">Richard
        Feynman and the Connection Machine</a></p><ul><li><p>This <em>Physics Today
        </em>article, written by Hillis himself, not only discusses Feynman and his
        work with the company, but also paints a picture of what went on in the early
        days of the company when it was a bunch of technical researchers working out
        of an old mansion outside of Boston.</p></li></ul></li><li><p><a href=\\\"https://www.washingtonpost.com/archive/business/1994/08/28/thinking-machines-rather-than-markets/db29b5ee-0107-43e2-89b0-c22518a3a02c/\\\">Thinking
        Machines Rather Than Markets</a></p><ul><li><p>This <em>Washington Post </em>article,
        written in the midst of TMC\u2019s demise, provides additional facts about
        the situation, but will be primarily useful to readers of this piece in helping
        them understand how reasonable-seeming help provided by an actor like DARPA
        can be portrayed depending on the given political climate.</p></li></ul><p></p></li></ul><h4><strong>General
        Links:</strong></h4><ul><li><p><a href=\\\"https://amzn.to/3SqvwJu\\\">Strategic
        Computing: How DARPA Built the Computer Age</a></p></li><li><p><a href=\\\"https://amzn.to/3tSafxW\\\">Transforming
        Computer Technology: Information Processing for the Pentagon, 1962-1986</a></p></li></ul><div
        class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a id=\\\"footnote-1\\\"
        href=\\\"#footnote-anchor-1\\\" class=\\\"footnote-number\\\" contenteditable=\\\"false\\\"
        target=\\\"_self\\\">1</a><div class=\\\"footnote-content\\\"><p>In 1962,
        IPTO's $9 million budget was entirely earmarked for category 6.1 research.</p></div></div><div
        class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a id=\\\"footnote-2\\\"
        href=\\\"#footnote-anchor-2\\\" class=\\\"footnote-number\\\" contenteditable=\\\"false\\\"
        target=\\\"_self\\\">2</a><div class=\\\"footnote-content\\\"><p>For the rest
        of the interpersonal aspects of the accounts in the next few paragraphs, I\u2019ll
        rely on Slotnick\u2019s memoir since he seemed to have a more clear recollection
        of the order in which things happened than Sutherland. </p></div></div><div
        class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a id=\\\"footnote-3\\\"
        href=\\\"#footnote-anchor-3\\\" class=\\\"footnote-number\\\" contenteditable=\\\"false\\\"
        target=\\\"_self\\\">3</a><div class=\\\"footnote-content\\\"><p>Another FreakTakes
        piece dives into the analogous case of early molecular biology researchers
        \u2014 many of whom would eventually win Nobel Prizes \u2014 taking risks
        and changing multiple parameters between experiments. The piece explores what
        they did and why they thought that was an optimal strategy for a field as
        young as molecular biology.</p></div></div><div class=\\\"footnote\\\" data-component-name=\\\"FootnoteToDOM\\\"><a
        id=\\\"footnote-4\\\" href=\\\"#footnote-anchor-4\\\" class=\\\"footnote-number\\\"
        contenteditable=\\\"false\\\" target=\\\"_self\\\">4</a><div class=\\\"footnote-content\\\"><p>For
        those curious about what \\\"AI applications\\\" looked like in the mid-1980s,
        check out next week's post on the Autonomous Land Vehicle, Pilot's Associate,
        and Battle Management</p><p></p></div></div>\",\"doi\":\"https://doi.org/10.59350/ewpnk-67998\",\"reference\":[],\"summary\":\"DARPA's
        varied approaches to developing early parallel computers\",\"tags\":[],\"title\":\"ILLIAC
        IV and the Connection Machine\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"archive_url\":\"https://wayback.archive-it.org/22096/20231101172748/https://blog.front-matter.io/posts/rogue-scholar-blogs-are-now-searchable\",\"authors\":[{\"name\":\"Martin
        Fenner\",\"url\":\"https://orcid.org/0000-0003-1419-2405\"}],\"blog_id\":\"f0m0e38\",\"blog_name\":\"Front
        Matter\",\"blog_slug\":\"front_matter\",\"content_html\":\"<p>With an update
        launched yesterday, all blogs that participate in the <a href=\\\"https://rogue-scholar.org\\\"
        rel=\\\"noreferrer\\\">Rogue Scholar</a> science blog archive are now searchable
        by blog title, description, language, blogging platform, category (scientific
        field using the <a href=\\\"https://www.oecd.org/science/inno/38235147.pdf\\\"
        rel=\\\"noreferrer\\\">OECD Fields of Science and Technology</a>), and ISSN.
        With the growing number of participating blogs (66 as of today) the new search
        functionality makes it easier to find a specific blog compared to the previous
        single page listing all blogs in alphabetical order.</p><figure class=\\\"kg-card
        kg-image-card\\\"><img src=\\\"https://blog.front-matter.io/content/images/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png\\\"
        class=\\\"kg-image\\\" alt=\\\"\\\" loading=\\\"lazy\\\" width=\\\"1864\\\"
        height=\\\"1048\\\" srcset=\\\"https://blog.front-matter.io/content/images/size/w600/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        600w, https://blog.front-matter.io/content/images/size/w1000/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        1000w, https://blog.front-matter.io/content/images/size/w1600/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        1600w, https://blog.front-matter.io/content/images/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        1864w\\\" sizes=\\\"(min-width: 720px) 720px\\\"></figure><p>The search functionality
        uses the same user interface as the blog posts search. In addition to searching
        for specific strings, you can also click on the tags associated with each
        blog to find all blogs covering a specific subject area and/or blogs hosted
        by a specific blogging platform, or filter blogs by language (using the language
        set it in the upper right corner of all Rogue Scholar pages). The blog title
        and description comes from the blog authors and is currently not curated by
        Rogue Scholar staff. Several blogs have an ISSN, and you can of course also
        <a href=\\\"https://rogue-scholar.org/blogs?query=2051-8188\\\" rel=\\\"noreferrer\\\">search
        for it</a>.</p><p>The above changes have also been implemented in the <a href=\\\"https://api.rogue-scholar.org/blogs\\\"
        rel=\\\"noreferrer\\\">Rogue Scholar API</a> if you prefer to interact with
        Rogue Scholar programmatically.</p><p>Together with the blog search functionality,
        I made a small change to the Rogue Scholar navigation. The default page for
        Rogue Scholar is now the blog posts search, showing the most recent posts
        unless you search for something specific. You now find information about Rogue
        Scholar (description, FAQ, pricing, and stats) on the <a href=\\\"https://rogue-scholar.org/about\\\"
        rel=\\\"noreferrer\\\">About</a> page.</p>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.53731/j4m27-bdw69\",\"guid\":\"6544c3a43d863800019a5eec\",\"id\":\"db56ed1f-6430-4418-b4a5-afa6338cf318\",\"image\":\"https://images.unsplash.com/photo-1595412134077-515f3c30b2fb?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDQwfHxzZWFyY2h8ZW58MHx8fHwxNjk5MDA1MzY4fDA&ixlib=rb-4.0.3&q=80&w=2000\",\"language\":\"en\",\"published_at\":1699009062,\"reference\":[],\"relationships\":[],\"summary\":\"With
        an update launched yesterday, all blogs that participate in the Rogue Scholar
        science blog archive are now searchable by blog title, description, language,
        blogging platform, category (scientific field using the OECD Fields of Science
        and Technology), and ISSN.\",\"tags\":[\"News\"],\"title\":\"Rogue Scholar
        blogs are now searchable\",\"updated_at\":1699010328,\"url\":\"https://blog.front-matter.io/posts/rogue-scholar-blogs-are-now-searchable\"},\"highlight\":{\"authors\":[{\"name\":\"Martin
        Fenner\",\"url\":\"https://orcid.org/0000-0003-1419-2405\"}],\"content_html\":\"<p>With
        an update launched yesterday, all blogs that participate in the <a href=\\\"https://rogue-scholar.org\\\"
        rel=\\\"noreferrer\\\">Rogue Scholar</a> science blog archive are now searchable
        by blog title, description, language, blogging platform, category (scientific
        field using the <a href=\\\"https://www.oecd.org/science/inno/38235147.pdf\\\"
        rel=\\\"noreferrer\\\">OECD Fields of Science and Technology</a>), and ISSN.
        With the growing number of participating blogs (66 as of today) the new search
        functionality makes it easier to find a specific blog compared to the previous
        single page listing all blogs in alphabetical order.</p><figure class=\\\"kg-card
        kg-image-card\\\"><img src=\\\"https://blog.front-matter.io/content/images/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png\\\"
        class=\\\"kg-image\\\" alt=\\\"\\\" loading=\\\"lazy\\\" width=\\\"1864\\\"
        height=\\\"1048\\\" srcset=\\\"https://blog.front-matter.io/content/images/size/w600/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        600w, https://blog.front-matter.io/content/images/size/w1000/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        1000w, https://blog.front-matter.io/content/images/size/w1600/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        1600w, https://blog.front-matter.io/content/images/2023/11/Bildschirmfoto-2023-11-03-um-11.03.12.png
        1864w\\\" sizes=\\\"(min-width: 720px) 720px\\\"></figure><p>The search functionality
        uses the same user interface as the blog posts search. In addition to searching
        for specific strings, you can also click on the tags associated with each
        blog to find all blogs covering a specific subject area and/or blogs hosted
        by a specific blogging platform, or filter blogs by language (using the language
        set it in the upper right corner of all Rogue Scholar pages). The blog title
        and description comes from the blog authors and is currently not curated by
        Rogue Scholar staff. Several blogs have an ISSN, and you can of course also
        <a href=\\\"https://rogue-scholar.org/blogs?query=2051-8188\\\" rel=\\\"noreferrer\\\">search
        for it</a>.</p><p>The above changes have also been implemented in the <a href=\\\"https://api.rogue-scholar.org/blogs\\\"
        rel=\\\"noreferrer\\\">Rogue Scholar API</a> if you prefer to interact with
        Rogue Scholar programmatically.</p><p>Together with the blog search functionality,
        I made a small change to the Rogue Scholar navigation. The default page for
        Rogue Scholar is now the blog posts search, showing the most recent posts
        unless you search for something specific. You now find information about Rogue
        Scholar (description, FAQ, pricing, and stats) on the <a href=\\\"https://rogue-scholar.org/about\\\"
        rel=\\\"noreferrer\\\">About</a> page.</p>\",\"doi\":\"https://doi.org/10.53731/j4m27-bdw69\",\"reference\":[],\"summary\":\"With
        an update launched yesterday, all blogs that participate in the Rogue Scholar
        science blog archive are now searchable by blog title, description, language,
        blogging platform, category (scientific field using the OECD Fields of Science
        and Technology), and ISSN.\",\"tags\":[\"News\"],\"title\":\"Rogue Scholar
        blogs are now searchable\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Norbisley
        Fern\xE1ndez\",\"url\":\"https://orcid.org/0000-0002-9373-4622\"}],\"blog_id\":\"6aswq28\",\"blog_name\":\"Edici\xF3n
        y comunicaci\xF3n de la Ciencia\",\"blog_slug\":\"norbisley\",\"content_html\":\"\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"538\\\"
        data-permalink=\\\"https://norbisley.wordpress.com/2023/11/03/potencial-de-rogue-scholar-para-la-visibilidad-de-la-ciencia-latinoamericana/blog/\\\"
        data-orig-file=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg\\\"
        data-orig-size=\\\"4831,3221\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"blog\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=300\\\"
        data-large-file=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=1024\\\"
        loading=\\\"lazy\\\" width=\\\"1024\\\" height=\\\"682\\\" src=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=1024\\\"
        alt=\\\"\\\" class=\\\"wp-image-538\\\" srcset=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=1024
        1024w, https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=2046 2046w,
        https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=150 150w, https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=300
        300w, https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=768 768w\\\"
        sizes=\\\"(max-width: 1024px) 100vw, 1024px\\\" /></figure>\\n\\n\\n\\n<p
        class=\\\"has-text-align-right\\\">Yo creo que la sobrevivencia de los blogs
        depender\xE1 de la capacidad de hacerlos realmente colaborativos<br> y de
        volver al concepto primigenio de entradas cortas.</p>\\n\\n\\n\\n<p class=\\\"has-text-align-right\\\">Monsalve,
        Valencia y Franco, 2019</p>\\n\\n\\n\\n<p>Para nadie es un secreto que la
        ciencia en pa\xEDses subdesarrollados es menos visible en las grandes plataformas
        de indexaci\xF3n de contenido cient\xEDfico, sobre todo aquella en idioma
        espa\xF1ol, ya que \u201Cel ingl\xE9s es la lengua franca de la ciencia.\u201D
        A esto sumemos la poca posibilidad de pagar por publicar de un cient\xEDfico
        latinoamericano, pues en palabras de Ganga, Paredes y Pedraja (2015) \u2014citado
        por Artigas (2023)\u2014 \u201Clos investigadores de pa\xEDses desarrollados
        normalmente publican en revistas reconocidas y cuentan con altos presupuestos
        de investigaci\xF3n, los de pa\xEDses con pocos recursos no logran publicar
        m\xE1s all\xE1 de sus fronteras, por tanto, no podemos hablar en general,
        ni esperar los mismos resultados\u201D (p.2). Es por eso que ser\xEDa provechoso
        agregar blogs de ciencia en idioma espa\xF1ol a Rogue Scholar a fin de que
        puedan obtener un DOI y hacer m\xE1s visible la ciencia de su pa\xEDs.</p>\\n\\n\\n\\n<p>Empec\xE9
        a buscar en internet y encontr\xE9 varios portales que agregaban blogs pero
        ninguno exhib\xEDa requisitos, solo ResearchBlogging ped\xEDa el env\xEDo
        de un post con una referencia a fuente arbitrada.</p>\\n\\n\\n\\n<p>Pens\xE9
        entonces en el proyecto de Martin Fenner y en c\xF3mo podr\xEDa ayudar a que
        mejores blogs fueran indexados. Como la navaja de Occam: lo m\xE1s simple
        es lo m\xE1s indicado. Aqu\xED resumo un art\xEDculo que pronto ser\xE1 publicado
        sobre calidad de los blogs y su papel en el ecosistema digital de la publicaci\xF3n
        acad\xE9mica.</p>\\n\\n\\n\\n<ol>\\n<li>Los blogs son aconsejados por muchos
        investigadores como un medio para obtener visibilidad (Aviles, 2023; Ruiz,
        L\xF3pez, Arteaga y Gonz\xE1lez, 2020, Cabezas, Torres y Delgado, 2009; Portuguez,
        Rey y G\xF3mez, 2019; D\xEDaz y Vit\xF3n, 2020) su contenido no siempre es
        validado (Rodr\xEDguez, 2019), y no puede validarse por el arbitraje tradicional
        pues tardar\xEDa mucho, por tanto lo primero que debe presentar un blog de
        ciencia es citaci\xF3n a fuentes arbitradas. Analic\xE9 muchos y muy pocos
        \u2014incluso algunos con muy buenos art\xEDculos\u2014 refieren fuentes.
        Un blog es un formato citable, reconocido por las normas de citaci\xF3n y
        que tiene su misi\xF3n como producto informativo dentro de la divulgaci\xF3n
        cient\xEDfica \xBFPor qu\xE9 no referenciar tambi\xE9n?</li>\\n\\n\\n\\n<li>El
        lector de ciencia ya est\xE1 familiarizado con el uso de referencias, ya que
        es, o un par intelectual, o un estudiante o alguien interesado en un tema
        con capacidad de investigaci\xF3n y ansias de conocimientos (y un lector ansioso,
        es un lector atento) Por tanto, independientemente de que un blog se escribe
        por pasi\xF3n, ha de tener un objetivo claro y una audiencia bien identificada.</li>\\n\\n\\n\\n<li>El
        acceso abierto es la v\xEDa m\xE1s expedita para visibilizar la ciencia (Hern\xE1ndez,
        Ramos y Fl\xF3rez, 2020; Fajardo, 2019 y Garc\xEDa, 2022, entre otros) entonces
        los blogs pueden promover el acceso abierto y la colaboraci\xF3n cient\xEDfica.Los
        servicios de muchos blogs podr\xEDan a\xF1adirle valor a la plataforma y retroalimentar
        la comunidad de bloggers y lectores.</li>\\n\\n\\n\\n<li>Un recurso importante
        para la confiabilidad del contenido compartido, adem\xE1s de la citaci\xF3n
        de fuentes arbitradas, es la informaci\xF3n del autor, verificable a trav\xE9s
        de su ORCID y perfiles de redes acad\xE9micas y profesionales.</li>\\n</ol>\\n\\n\\n\\n<p>Rogue
        Scholar es una gran oportunidad para la ciencia latinoamericana y del Sur,
        en general. Podemos visibilizar la ciencia de pa\xEDses en v\xEDas de desarrollo
        a fin de promover el trabajo colaborativo y el acceso abierto a informaci\xF3n
        confiable. Animo a aquellos docentes que tienen un blog a  presentarslo, siempre
        y cuando observen los requisitos mencionados. No hay restricci\xF3n tem\xE1tica,
        Rogue da cobertura a cualquier \xE1rea cient\xEDfica que aborde soluciones
        cient\xEDficas a problemas de la sociedad o divulgue el quehacer de una instituci\xF3n
        investigativa.</p>\\n\\n\\n\\n<p><strong>Referencias</strong></p>\\n\\n\\n\\n<p>Artigas
        (2023). El arduo camino de la gesti\xF3n editorial Latinoamericana: Comentarios
        sobre una realidad actual. Journal of the Academy. N\xFAm. 8. Enero-Junio
        2023. <a href=\\\"/Users/PC/AppData/Local/Temp/MicrosoftEdgeDownloads/cee0a1ba-5ea5-43bb-ba4f-780613e676aa/157-Texto%20del%20art%C3%ADculo-978-1-10-20230106.pdf\\\">157-Texto
        del art\xEDculo-978-1-10-20230106.pdf</a></p>\\n\\n\\n\\n<p>Avilez, A. (2023).
        Mejora de la visibilidad de la Unidad M\xE9dica Vascular SAS mediante estrategias
        innovadoras de marketing en salud.</p>\\n\\n\\n\\n<p>Cabezas-Clavijo, \xC1.,
        Torres-Salinas, D., &amp; Delgado-L\xF3pez-C\xF3zar, E. (2009). Ciencia 2.0:
        cat\xE1logo de herramientas e implicaciones para la actividad investigadora.&nbsp;<em>Profesional
        de la Informaci\xF3n</em>,&nbsp;<em>18</em>(1), 72-80.</p>\\n\\n\\n\\n<p>D\xEDaz-Samada,
        R. E., &amp; Vit\xF3n-Castillo, A. A. (2020) \xBF C\xF3mo aumentar la visibilidad
        de las publicaciones cient\xEDficas?.&nbsp;<em>Revista Cubana de Medicina
        Militar</em>,&nbsp;<em>49</em>(2).</p>\\n\\n\\n\\n<p>Esteve-Guillen, A. (2022).
        Blogs y lectura: Un an\xE1lisis cr\xEDtico de los art\xEDculos de investigaci\xF3n.&nbsp;<em>Ocnos.
        Revista De Estudios Sobre Lectura</em>,&nbsp;<em>21</em>(1). <a href=\\\"https://doi.org/10.18239/ocnos_2022.21.1.2739\\\">https://doi.org/10.18239/ocnos_2022.21.1.2739</a></p>\\n\\n\\n\\n<p>Fajardo,
        J. L. C. (2019). Acceso abierto v\xEDa diamante en revistas cient\xEDficas
        latinoamericanas.&nbsp;<em>Tlatemoani: revista acad\xE9mica de investigaci\xF3n</em>,&nbsp;<em>10</em>(30),
        170-187.</p>\\n\\n\\n\\n<p>Garc\xEDa, A. B. (2022). Favorecer los canales
        de publicaci\xF3n y distribuci\xF3n inclusivos de manera que nunca se excluya
        a los autores por motivos econ\xF3micos: el Acceso Abierto \u201Cverde\u201D
        y \u201Cdiamante\u201D en Am\xE9rica Latina en el marco de BOAI20.&nbsp;<em>Tramas
        y Redes</em>, (3), 327-337.</p>\\n\\n\\n\\n<p>Hern\xE1ndez, D. Y. H., Ramos,
        D. P. L., &amp; Fl\xF3rez, D. T. (2020). Importancia de las revistas de acceso
        abierto: la indizaci\xF3n como meta fundamental.&nbsp;<em>Dictamen Libre</em>,
        (26), 81-98.</p>\\n\\n\\n\\n<p>Portuguez Castro, M., Rey Castillo, M., &amp;
        G\xF3mez Zerme\xF1o, M. G. (2019). Estrategias de visibilidad para la producci\xF3n
        cient\xEDfica en revistas electr\xF3nicas de acceso abierto: revisi\xF3n sistem\xE1tica
        de literatura.&nbsp;<em>Education in the knowledge society: EKS</em>.</p>\\n\\n\\n\\n<p>Rodr\xEDguez
        Camacaro, J. E. (2019). Visibilidad de las publicaciones cient\xEDficas: un
        modo de fortalecerlas.&nbsp;<em>Suma de Negocios</em>,&nbsp;<em>10</em>(21),
        63-69.</p>\\n\\n\\n\\n<p>Ruiz Corbella, M., L\xF3pez G\xF3mez, E., Arteaga
        Mart\xEDnez, B. P., &amp; Gonz\xE1lez Gal\xE1n, A. (2020). Visibilidad, impacto
        y transferencia del conocimiento en revistas cient\xEDficas de educaci\xF3n:
        la experiencia de Aula Magna 2.0.&nbsp;<em>Revista electr\xF3nica de investigaci\xF3n
        y evaluaci\xF3n educativa</em>.</p>\\n\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/w0dk4-rw897\",\"guid\":\"https://norbisley.wordpress.com/?p=536\",\"id\":\"4a360b87-6b0c-4362-9bfb-6bff531a69cd\",\"image\":\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=1024\",\"language\":\"es\",\"published_at\":1699007953,\"reference\":[],\"relationships\":[],\"summary\":\"Yo
        creo que la sobrevivencia de los blogs depender\xE1 de la capacidad de hacerlos
        realmente colaborativos y de volver al concepto primigenio de entradas cortas.\",\"tags\":[\"Sin
        Categor\xEDa\"],\"title\":\"Potencial de Rogue Scholar para la visibilidad
        de la ciencia latinoamericana\",\"updated_at\":1699014419,\"url\":\"https://norbisley.wordpress.com/2023/11/03/potencial-de-rogue-scholar-para-la-visibilidad-de-la-ciencia-latinoamericana\"},\"highlight\":{\"authors\":[{\"name\":\"Norbisley
        Fern\xE1ndez\",\"url\":\"https://orcid.org/0000-0002-9373-4622\"}],\"content_html\":\"\\n<figure
        class=\\\"wp-block-image size-large\\\"><img data-attachment-id=\\\"538\\\"
        data-permalink=\\\"https://norbisley.wordpress.com/2023/11/03/potencial-de-rogue-scholar-para-la-visibilidad-de-la-ciencia-latinoamericana/blog/\\\"
        data-orig-file=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg\\\"
        data-orig-size=\\\"4831,3221\\\" data-comments-opened=\\\"1\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"blog\\\" data-image-description=\\\"\\\" data-image-caption=\\\"\\\"
        data-medium-file=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=300\\\"
        data-large-file=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=1024\\\"
        loading=\\\"lazy\\\" width=\\\"1024\\\" height=\\\"682\\\" src=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=1024\\\"
        alt=\\\"\\\" class=\\\"wp-image-538\\\" srcset=\\\"https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=1024
        1024w, https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=2046 2046w,
        https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=150 150w, https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=300
        300w, https://norbisley.files.wordpress.com/2023/11/blog.jpg?w=768 768w\\\"
        sizes=\\\"(max-width: 1024px) 100vw, 1024px\\\" /></figure>\\n\\n\\n\\n<p
        class=\\\"has-text-align-right\\\">Yo creo que la sobrevivencia de los blogs
        depender\xE1 de la capacidad de hacerlos realmente colaborativos<br> y de
        volver al concepto primigenio de entradas cortas.</p>\\n\\n\\n\\n<p class=\\\"has-text-align-right\\\">Monsalve,
        Valencia y Franco, 2019</p>\\n\\n\\n\\n<p>Para nadie es un secreto que la
        ciencia en pa\xEDses subdesarrollados es menos visible en las grandes plataformas
        de indexaci\xF3n de contenido cient\xEDfico, sobre todo aquella en idioma
        espa\xF1ol, ya que \u201Cel ingl\xE9s es la lengua franca de la ciencia.\u201D
        A esto sumemos la poca posibilidad de pagar por publicar de un cient\xEDfico
        latinoamericano, pues en palabras de Ganga, Paredes y Pedraja (2015) \u2014citado
        por Artigas (2023)\u2014 \u201Clos investigadores de pa\xEDses desarrollados
        normalmente publican en revistas reconocidas y cuentan con altos presupuestos
        de investigaci\xF3n, los de pa\xEDses con pocos recursos no logran publicar
        m\xE1s all\xE1 de sus fronteras, por tanto, no podemos hablar en general,
        ni esperar los mismos resultados\u201D (p.2). Es por eso que ser\xEDa provechoso
        agregar blogs de ciencia en idioma espa\xF1ol a Rogue Scholar a fin de que
        puedan obtener un DOI y hacer m\xE1s visible la ciencia de su pa\xEDs.</p>\\n\\n\\n\\n<p>Empec\xE9
        a buscar en internet y encontr\xE9 varios portales que agregaban blogs pero
        ninguno exhib\xEDa requisitos, solo ResearchBlogging ped\xEDa el env\xEDo
        de un post con una referencia a fuente arbitrada.</p>\\n\\n\\n\\n<p>Pens\xE9
        entonces en el proyecto de Martin Fenner y en c\xF3mo podr\xEDa ayudar a que
        mejores blogs fueran indexados. Como la navaja de Occam: lo m\xE1s simple
        es lo m\xE1s indicado. Aqu\xED resumo un art\xEDculo que pronto ser\xE1 publicado
        sobre calidad de los blogs y su papel en el ecosistema digital de la publicaci\xF3n
        acad\xE9mica.</p>\\n\\n\\n\\n<ol>\\n<li>Los blogs son aconsejados por muchos
        investigadores como un medio para obtener visibilidad (Aviles, 2023; Ruiz,
        L\xF3pez, Arteaga y Gonz\xE1lez, 2020, Cabezas, Torres y Delgado, 2009; Portuguez,
        Rey y G\xF3mez, 2019; D\xEDaz y Vit\xF3n, 2020) su contenido no siempre es
        validado (Rodr\xEDguez, 2019), y no puede validarse por el arbitraje tradicional
        pues tardar\xEDa mucho, por tanto lo primero que debe presentar un blog de
        ciencia es citaci\xF3n a fuentes arbitradas. Analic\xE9 muchos y muy pocos
        \u2014incluso algunos con muy buenos art\xEDculos\u2014 refieren fuentes.
        Un blog es un formato citable, reconocido por las normas de citaci\xF3n y
        que tiene su misi\xF3n como producto informativo dentro de la divulgaci\xF3n
        cient\xEDfica \xBFPor qu\xE9 no referenciar tambi\xE9n?</li>\\n\\n\\n\\n<li>El
        lector de ciencia ya est\xE1 familiarizado con el uso de referencias, ya que
        es, o un par intelectual, o un estudiante o alguien interesado en un tema
        con capacidad de investigaci\xF3n y ansias de conocimientos (y un lector ansioso,
        es un lector atento) Por tanto, independientemente de que un blog se escribe
        por pasi\xF3n, ha de tener un objetivo claro y una audiencia bien identificada.</li>\\n\\n\\n\\n<li>El
        acceso abierto es la v\xEDa m\xE1s expedita para visibilizar la ciencia (Hern\xE1ndez,
        Ramos y Fl\xF3rez, 2020; Fajardo, 2019 y Garc\xEDa, 2022, entre otros) entonces
        los blogs pueden promover el acceso abierto y la colaboraci\xF3n cient\xEDfica.Los
        servicios de muchos blogs podr\xEDan a\xF1adirle valor a la plataforma y retroalimentar
        la comunidad de bloggers y lectores.</li>\\n\\n\\n\\n<li>Un recurso importante
        para la confiabilidad del contenido compartido, adem\xE1s de la citaci\xF3n
        de fuentes arbitradas, es la informaci\xF3n del autor, verificable a trav\xE9s
        de su ORCID y perfiles de redes acad\xE9micas y profesionales.</li>\\n</ol>\\n\\n\\n\\n<p>Rogue
        Scholar es una gran oportunidad para la ciencia latinoamericana y del Sur,
        en general. Podemos visibilizar la ciencia de pa\xEDses en v\xEDas de desarrollo
        a fin de promover el trabajo colaborativo y el acceso abierto a informaci\xF3n
        confiable. Animo a aquellos docentes que tienen un blog a  presentarslo, siempre
        y cuando observen los requisitos mencionados. No hay restricci\xF3n tem\xE1tica,
        Rogue da cobertura a cualquier \xE1rea cient\xEDfica que aborde soluciones
        cient\xEDficas a problemas de la sociedad o divulgue el quehacer de una instituci\xF3n
        investigativa.</p>\\n\\n\\n\\n<p><strong>Referencias</strong></p>\\n\\n\\n\\n<p>Artigas
        (2023). El arduo camino de la gesti\xF3n editorial Latinoamericana: Comentarios
        sobre una realidad actual. Journal of the Academy. N\xFAm. 8. Enero-Junio
        2023. <a href=\\\"/Users/PC/AppData/Local/Temp/MicrosoftEdgeDownloads/cee0a1ba-5ea5-43bb-ba4f-780613e676aa/157-Texto%20del%20art%C3%ADculo-978-1-10-20230106.pdf\\\">157-Texto
        del art\xEDculo-978-1-10-20230106.pdf</a></p>\\n\\n\\n\\n<p>Avilez, A. (2023).
        Mejora de la visibilidad de la Unidad M\xE9dica Vascular SAS mediante estrategias
        innovadoras de marketing en salud.</p>\\n\\n\\n\\n<p>Cabezas-Clavijo, \xC1.,
        Torres-Salinas, D., &amp; Delgado-L\xF3pez-C\xF3zar, E. (2009). Ciencia 2.0:
        cat\xE1logo de herramientas e implicaciones para la actividad investigadora.&nbsp;<em>Profesional
        de la Informaci\xF3n</em>,&nbsp;<em>18</em>(1), 72-80.</p>\\n\\n\\n\\n<p>D\xEDaz-Samada,
        R. E., &amp; Vit\xF3n-Castillo, A. A. (2020) \xBF C\xF3mo aumentar la visibilidad
        de las publicaciones cient\xEDficas?.&nbsp;<em>Revista Cubana de Medicina
        Militar</em>,&nbsp;<em>49</em>(2).</p>\\n\\n\\n\\n<p>Esteve-Guillen, A. (2022).
        Blogs y lectura: Un an\xE1lisis cr\xEDtico de los art\xEDculos de investigaci\xF3n.&nbsp;<em>Ocnos.
        Revista De Estudios Sobre Lectura</em>,&nbsp;<em>21</em>(1). <a href=\\\"https://doi.org/10.18239/ocnos_2022.21.1.2739\\\">https://doi.org/10.18239/ocnos_2022.21.1.2739</a></p>\\n\\n\\n\\n<p>Fajardo,
        J. L. C. (2019). Acceso abierto v\xEDa diamante en revistas cient\xEDficas
        latinoamericanas.&nbsp;<em>Tlatemoani: revista acad\xE9mica de investigaci\xF3n</em>,&nbsp;<em>10</em>(30),
        170-187.</p>\\n\\n\\n\\n<p>Garc\xEDa, A. B. (2022). Favorecer los canales
        de publicaci\xF3n y distribuci\xF3n inclusivos de manera que nunca se excluya
        a los autores por motivos econ\xF3micos: el Acceso Abierto \u201Cverde\u201D
        y \u201Cdiamante\u201D en Am\xE9rica Latina en el marco de BOAI20.&nbsp;<em>Tramas
        y Redes</em>, (3), 327-337.</p>\\n\\n\\n\\n<p>Hern\xE1ndez, D. Y. H., Ramos,
        D. P. L., &amp; Fl\xF3rez, D. T. (2020). Importancia de las revistas de acceso
        abierto: la indizaci\xF3n como meta fundamental.&nbsp;<em>Dictamen Libre</em>,
        (26), 81-98.</p>\\n\\n\\n\\n<p>Portuguez Castro, M., Rey Castillo, M., &amp;
        G\xF3mez Zerme\xF1o, M. G. (2019). Estrategias de visibilidad para la producci\xF3n
        cient\xEDfica en revistas electr\xF3nicas de acceso abierto: revisi\xF3n sistem\xE1tica
        de literatura.&nbsp;<em>Education in the knowledge society: EKS</em>.</p>\\n\\n\\n\\n<p>Rodr\xEDguez
        Camacaro, J. E. (2019). Visibilidad de las publicaciones cient\xEDficas: un
        modo de fortalecerlas.&nbsp;<em>Suma de Negocios</em>,&nbsp;<em>10</em>(21),
        63-69.</p>\\n\\n\\n\\n<p>Ruiz Corbella, M., L\xF3pez G\xF3mez, E., Arteaga
        Mart\xEDnez, B. P., &amp; Gonz\xE1lez Gal\xE1n, A. (2020). Visibilidad, impacto
        y transferencia del conocimiento en revistas cient\xEDficas de educaci\xF3n:
        la experiencia de Aula Magna 2.0.&nbsp;<em>Revista electr\xF3nica de investigaci\xF3n
        y evaluaci\xF3n educativa</em>.</p>\\n\",\"doi\":\"https://doi.org/10.59350/w0dk4-rw897\",\"reference\":[],\"summary\":\"Yo
        creo que la sobrevivencia de los blogs depender\xE1 de la capacidad de hacerlos
        realmente colaborativos y de volver al concepto primigenio de entradas cortas.\",\"tags\":[\"Sin
        Categor\xEDa\"],\"title\":\"Potencial de Rogue Scholar para la visibilidad
        de la ciencia latinoamericana\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Ernesto
        Priego\"}],\"blog_id\":\"tnxpa35\",\"blog_name\":\"Everything is Connected\",\"blog_slug\":\"ernestopriego\",\"content_html\":\"\\n<figure
        class=\\\"wp-block-image size-large\\\"><a href=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg\\\"><img
        data-attachment-id=\\\"9727\\\" data-permalink=\\\"http://ernestopriego.com/2023/11/02/haunted-by-juan-rulfo-on-books-as-inheritance-and-sites-for-remembrance/juan-rulfo-llano-llamas-pedro-paramo-fce-priego/\\\"
        data-orig-file=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg\\\"
        data-orig-size=\\\"2048,1536\\\" data-comments-opened=\\\"0\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"juan-rulfo-llano-llamas-pedro-paramo-fce-priego\\\" data-image-description=\\\"\\\"
        data-image-caption=\\\"\\\" data-medium-file=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=300\\\"
        data-large-file=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=1024\\\"
        loading=\\\"lazy\\\" width=\\\"1024\\\" height=\\\"768\\\" src=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=1024\\\"
        alt=\\\"Photo of two paperbacks wrapped in plastic bags. Left, Juan Rulfo,
        El llano en llamas, right Juan Rulfo, Pedro P\xE1ramo, both Colecci\xF3n Popular,
        FCE, M\xE9xico. One is a fourth edition, the other a sixth edition. Both copies
        show signs of usage and time passing. \\\" class=\\\"wp-image-9727\\\" srcset=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=1024
        1024w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg
        2048w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=150
        150w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=300
        300w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=768
        768w\\\" sizes=\\\"(max-width: 1024px) 100vw, 1024px\\\" /></a></figure>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p><strong>T</strong>oday
        is 2 de noviembre, and as such it is and always will be <a href=\\\"https://en.wikipedia.org/wiki/Day_of_the_Dead\\\">d\xEDa
        de muertos</a><sup data-fn=\\\"5b7e4bec-7597-4124-a5db-7c356a656f49\\\" class=\\\"fn\\\"><a
        href=\\\"#5b7e4bec-7597-4124-a5db-7c356a656f49\\\" id=\\\"5b7e4bec-7597-4124-a5db-7c356a656f49-link\\\">1</a></sup>.
        Often an excuse not to write more is having &#8216;big&#8217; topics that
        will become too complex and for which I won&#8217;t have the time to discuss
        as they deserve. Thought I could keep trying to write about some of the items
        in my personal library. As always, I am not promising anything. </p>\\n\\n\\n\\n<p>Anyway,
        as I started saying, &#8220;today is 2 de noviembre, and as such it is and
        always will be d\xEDa de muertos&#8221;. It&#8217;s strange to think that
        today is not day of the dead for everyone everywhere, when the dead belong
        to all of us, when they are also everywhere, and continue to make the world
        go round and round. The dead propell us towards the future. Today, like many
        previous 2 de noviembre, I grabbed these two copies from my shelves, Juan
        Rulfo&#8217;s short story collection, <em><a href=\\\"https://en.wikipedia.org/wiki/El_Llano_en_llamas\\\"
        target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\">El llano en llamas</a></em>,
        and his short novel <em><a href=\\\"https://en.wikipedia.org/wiki/Pedro_P%C3%A1ramo\\\"
        target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\">Pedro P\xE1ramo</a></em>.
        </p>\\n\\n\\n\\n<p>These are two of the most treasured items in my collection.
        These are not first editions, but a fourth and a sixth, respectively. These
        are the copies in which I read Rulfo first as a teenager, as both were prescribed
        to all of us in middle school, and after that I re-read them at university
        as an undergraduate, and then for sheer pleasure several times over the years.
        I go back to them like some people go back to their Bible or their Shakespeare-
        they are for me akin to reference books, sites of reassurance and temporary
        relief, nostalgia and inspiration. These are books that, as objects, carry
        magical powers, and as such are to be treated with respect. I did not always
        know that, but over the years these books and the stories within them became
        more and more powerful. They were always haunted, full of ghosts, but as time
        passed and I got older and things happened they became doubly, triply haunted.
        </p>\\n\\n\\n\\n<p>The <em>Pedro P\xE1ramo</em> now in my possession used
        to belong to my father (RIP), and it still bears his signature on the first
        page. It is possible he bought it on the same year this sixth edition came
        out  (1964). It was a rarity in his collection because he did not date it
        nor added any other notes; he used to write down exact dates and places of
        acquisition, and used to dedicate most of his books to my mother, even if
        he had originally bought them for himself, really. (Eventually, my mother
        would read nearly every non-technical book my father ever bought). Given I
        have siblings, I am not entirely sure I am the one who should have this copy
        right now, but the thing is I do, having brought it with me to the UK from
        Mexico City several years ago, as a piece of my family and my home and my
        country and my culture. Along <em>El llano en llamas</em>, it it truly is
        a desert island book.</p>\\n\\n\\n\\n<p><em>El llano en llamas</em> is a fourth
        edition (Colecci\xF3n Popular) from 1959, and I cannot fully recall how it
        came to my hands. This one does not have my father&#8217;s signature inside,
        nor a dedication (books he got us were always dated and dedicated). Instead
        it&#8217;s got my signature, very faint in pencil, and it&#8217;s the signature
        I used to do around the time I was 15 or so. I must have bought it second
        hand. I remember being quite snobbish at school when we were asked to read
        classics and fellow students brought brand-new editions of what to me seemed
        <em>ancient</em> books, so if my parents did not already have them I looked
        for them second hand. I felt that old books needed to be read in old editions,
        an affectation that I happily left behind many years ago. </p>\\n\\n\\n\\n<p>Carefully
        flipping through these books&#8217; yellowed pages, feeling their brittle
        spines that somehow still hold the pages together, it&#8217;s impossible not
        to be always amazed at Rulfo&#8217;s masterful craft. A perfect mash-up of
        the Mexican oral tradition and precise, expertly-balanced rhythm and tone.
        Punctuation, paragraph breaks, sentence length, word order and the rich referentiality
        of people&#8217;s and places&#8217; names compose a rich universe of feeling
        and imagery. Of all of Mexico&#8217;s greatest writers, there is no doubt
        to me there&#8217;s no one like Rulfo. All of that is common place, of course.
        </p>\\n\\n\\n\\n<p>There are writers whose works we go back to in certain
        times of the year: Dickens for Christmas, for example. For November, and particularly
        d\xEDa de muertos, the go to is Rulfo. His books are part of my inheritance
        (the one I received; the one I hope to be able to pass on) and my heritage.
        </p>\\n\\n\\n\\n<p>Rulfo&#8217;s literature is like an ofrenda, a celebration
        of life and time and place and the fading faces of those we knew. These paperbacks
        are my personal treasure and work like amulets and time machines. I open these
        books and here they are, Natalia, Camilo, Remigio, Odil\xF3n, Anacleto, Macario,
        la Pancha, Pedro Zamora, Comala, Luvina, Zapotl\xE1n, Talpa. But also my father,
        my brother in law, friends my age who&#8217;ve already departed, the places
        where I have lived, the old haunts. There&#8217;s no doubt in my mind that
        books have that power: sites of remembrance, beyond the page, beyond words
        and fiction, towards the present and the future. </p>\\n\\n\\n\",\"content_text\":\"content_text\",\"guid\":\"https://ernestopriego.com/?p=9725\",\"id\":\"a37fe5f9-a9ee-4de7-9a74-f2e5314f4c97\",\"image\":\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=1024\",\"language\":\"en\",\"published_at\":1698920265,\"reference\":[],\"relationships\":[],\"summary\":\"Where
        I write, being d\xEDa de muertos, about my copies of Rulfo\u2019s El llano
        en llamas and Pedro P\xE1ramo.\",\"tags\":[\"Scraps\"],\"title\":\"Haunted
        by Juan Rulfo: On Books as Inheritance and Sites for Remembrance\",\"updated_at\":1698925726,\"url\":\"https://ernestopriego.com/2023/11/02/haunted-by-juan-rulfo-on-books-as-inheritance-and-sites-for-remembrance\"},\"highlight\":{\"authors\":[{\"name\":\"Ernesto
        Priego\"}],\"content_html\":\"\\n<figure class=\\\"wp-block-image size-large\\\"><a
        href=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg\\\"><img
        data-attachment-id=\\\"9727\\\" data-permalink=\\\"http://ernestopriego.com/2023/11/02/haunted-by-juan-rulfo-on-books-as-inheritance-and-sites-for-remembrance/juan-rulfo-llano-llamas-pedro-paramo-fce-priego/\\\"
        data-orig-file=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg\\\"
        data-orig-size=\\\"2048,1536\\\" data-comments-opened=\\\"0\\\" data-image-meta=\\\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\\\"
        data-image-title=\\\"juan-rulfo-llano-llamas-pedro-paramo-fce-priego\\\" data-image-description=\\\"\\\"
        data-image-caption=\\\"\\\" data-medium-file=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=300\\\"
        data-large-file=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=1024\\\"
        loading=\\\"lazy\\\" width=\\\"1024\\\" height=\\\"768\\\" src=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=1024\\\"
        alt=\\\"Photo of two paperbacks wrapped in plastic bags. Left, Juan Rulfo,
        El llano en llamas, right Juan Rulfo, Pedro P\xE1ramo, both Colecci\xF3n Popular,
        FCE, M\xE9xico. One is a fourth edition, the other a sixth edition. Both copies
        show signs of usage and time passing. \\\" class=\\\"wp-image-9727\\\" srcset=\\\"https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=1024
        1024w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg
        2048w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=150
        150w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=300
        300w, https://epriego.files.wordpress.com/2023/11/juan-rulfo-llano-llamas-pedro-paramo-fce-priego.jpeg?w=768
        768w\\\" sizes=\\\"(max-width: 1024px) 100vw, 1024px\\\" /></a></figure>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p><strong>T</strong>oday
        is 2 de noviembre, and as such it is and always will be <a href=\\\"https://en.wikipedia.org/wiki/Day_of_the_Dead\\\">d\xEDa
        de muertos</a><sup data-fn=\\\"5b7e4bec-7597-4124-a5db-7c356a656f49\\\" class=\\\"fn\\\"><a
        href=\\\"#5b7e4bec-7597-4124-a5db-7c356a656f49\\\" id=\\\"5b7e4bec-7597-4124-a5db-7c356a656f49-link\\\">1</a></sup>.
        Often an excuse not to write more is having &#8216;big&#8217; topics that
        will become too complex and for which I won&#8217;t have the time to discuss
        as they deserve. Thought I could keep trying to write about some of the items
        in my personal library. As always, I am not promising anything. </p>\\n\\n\\n\\n<p>Anyway,
        as I started saying, &#8220;today is 2 de noviembre, and as such it is and
        always will be d\xEDa de muertos&#8221;. It&#8217;s strange to think that
        today is not day of the dead for everyone everywhere, when the dead belong
        to all of us, when they are also everywhere, and continue to make the world
        go round and round. The dead propell us towards the future. Today, like many
        previous 2 de noviembre, I grabbed these two copies from my shelves, Juan
        Rulfo&#8217;s short story collection, <em><a href=\\\"https://en.wikipedia.org/wiki/El_Llano_en_llamas\\\"
        target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\">El llano en llamas</a></em>,
        and his short novel <em><a href=\\\"https://en.wikipedia.org/wiki/Pedro_P%C3%A1ramo\\\"
        target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\">Pedro P\xE1ramo</a></em>.
        </p>\\n\\n\\n\\n<p>These are two of the most treasured items in my collection.
        These are not first editions, but a fourth and a sixth, respectively. These
        are the copies in which I read Rulfo first as a teenager, as both were prescribed
        to all of us in middle school, and after that I re-read them at university
        as an undergraduate, and then for sheer pleasure several times over the years.
        I go back to them like some people go back to their Bible or their Shakespeare-
        they are for me akin to reference books, sites of reassurance and temporary
        relief, nostalgia and inspiration. These are books that, as objects, carry
        magical powers, and as such are to be treated with respect. I did not always
        know that, but over the years these books and the stories within them became
        more and more powerful. They were always haunted, full of ghosts, but as time
        passed and I got older and things happened they became doubly, triply haunted.
        </p>\\n\\n\\n\\n<p>The <em>Pedro P\xE1ramo</em> now in my possession used
        to belong to my father (RIP), and it still bears his signature on the first
        page. It is possible he bought it on the same year this sixth edition came
        out  (1964). It was a rarity in his collection because he did not date it
        nor added any other notes; he used to write down exact dates and places of
        acquisition, and used to dedicate most of his books to my mother, even if
        he had originally bought them for himself, really. (Eventually, my mother
        would read nearly every non-technical book my father ever bought). Given I
        have siblings, I am not entirely sure I am the one who should have this copy
        right now, but the thing is I do, having brought it with me to the UK from
        Mexico City several years ago, as a piece of my family and my home and my
        country and my culture. Along <em>El llano en llamas</em>, it it truly is
        a desert island book.</p>\\n\\n\\n\\n<p><em>El llano en llamas</em> is a fourth
        edition (Colecci\xF3n Popular) from 1959, and I cannot fully recall how it
        came to my hands. This one does not have my father&#8217;s signature inside,
        nor a dedication (books he got us were always dated and dedicated). Instead
        it&#8217;s got my signature, very faint in pencil, and it&#8217;s the signature
        I used to do around the time I was 15 or so. I must have bought it second
        hand. I remember being quite snobbish at school when we were asked to read
        classics and fellow students brought brand-new editions of what to me seemed
        <em>ancient</em> books, so if my parents did not already have them I looked
        for them second hand. I felt that old books needed to be read in old editions,
        an affectation that I happily left behind many years ago. </p>\\n\\n\\n\\n<p>Carefully
        flipping through these books&#8217; yellowed pages, feeling their brittle
        spines that somehow still hold the pages together, it&#8217;s impossible not
        to be always amazed at Rulfo&#8217;s masterful craft. A perfect mash-up of
        the Mexican oral tradition and precise, expertly-balanced rhythm and tone.
        Punctuation, paragraph breaks, sentence length, word order and the rich referentiality
        of people&#8217;s and places&#8217; names compose a rich universe of feeling
        and imagery. Of all of Mexico&#8217;s greatest writers, there is no doubt
        to me there&#8217;s no one like Rulfo. All of that is common place, of course.
        </p>\\n\\n\\n\\n<p>There are writers whose works we go back to in certain
        times of the year: Dickens for Christmas, for example. For November, and particularly
        d\xEDa de muertos, the go to is Rulfo. His books are part of my inheritance
        (the one I received; the one I hope to be able to pass on) and my heritage.
        </p>\\n\\n\\n\\n<p>Rulfo&#8217;s literature is like an ofrenda, a celebration
        of life and time and place and the fading faces of those we knew. These paperbacks
        are my personal treasure and work like amulets and time machines. I open these
        books and here they are, Natalia, Camilo, Remigio, Odil\xF3n, Anacleto, Macario,
        la Pancha, Pedro Zamora, Comala, Luvina, Zapotl\xE1n, Talpa. But also my father,
        my brother in law, friends my age who&#8217;ve already departed, the places
        where I have lived, the old haunts. There&#8217;s no doubt in my mind that
        books have that power: sites of remembrance, beyond the page, beyond words
        and fiction, towards the present and the future. </p>\\n\\n\\n\",\"reference\":[],\"summary\":\"Where
        I write, being d\xEDa de muertos, about my copies of Rulfo\u2019s El llano
        en llamas and Pedro P\xE1ramo.\",\"tags\":[\"Scraps\"],\"title\":\"Haunted
        by Juan Rulfo: On Books as Inheritance and Sites for Remembrance\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Georg
        Fischer\"}],\"blog_id\":\"15san30\",\"blog_name\":\"iRights.info\",\"blog_slug\":\"irights\",\"content_html\":\"<p>Open
        Access f\xFCr \xF6ffentliche Forschung und Kultur: Zwei Jahrzehnte ist es
        schon her, dass sich zahlreiche Organisationen in der <em>Berliner Erkl\xE4rung</em>
        daf\xFCr aussprachen. Wie weit ist die \xD6ffnung tats\xE4chlich gekommen?
        Und was gilt es weiterhin zu tun? Spannende Fragen, die Berliner und Brandenburger
        Vertreter*innen aus Wissenschaft und Kultur letzte Woche diskutierten.</p>\\n<p><span
        id=\\\"more-32078\\\"></span></p>\\n<p>Am 24. Oktober 2023 luden das\_<a href=\\\"https://www.ibi.hu-berlin.de/de\\\">Institut
        f\xFCr Bibliotheks- und Informationswissenschaft</a>\_(IBI) der Humboldt-Universit\xE4t
        zu Berlin (HU), der\_<a href=\\\"https://www.kobv.de/\\\">Kooperative Bibliotheksverbund
        Berlin-Brandenburg</a>\_(KOBV), das\_<a href=\\\"http://www.open-access-berlin.de/index.html\\\">Open-Access-B\xFCro
        Berlin</a>\_(OABB) und die\_<a href=\\\"https://open-access-brandenburg.de/\\\">Vernetzungs-
        und Kompetenzstelle Open Access Brandenburg</a>\_(VuK) dazu ein, im\_<a href=\\\"https://www.zib.de/\\\">Zuse
        Institut Berlin</a>\_(ZIB) den Status von Open Access in der Region Berlin-Brandenburg
        zu diskutieren. Unter dem Motto \u201E<a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\">Was
        wurde erreicht und wo geht es hin?</a>\u201D sa\xDFen insgesamt sechs Vertreter*innen
        der Berliner und Brandenburger Open-Access-Community auf dem Podium.</p>\\n<div
        id=\\\"attachment_32081\\\" style=\\\"width: 500px\\\" class=\\\"wp-caption
        alignnone\\\"><a href=\\\"https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-scaled.jpg\\\"><img
        aria-describedby=\\\"caption-attachment-32081\\\" loading=\\\"lazy\\\" class=\\\"size-content
        wp-image-32081\\\" src=\\\"https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-490x254.jpg\\\"
        alt=\\\"\\\" width=\\\"490\\\" height=\\\"254\\\" srcset=\\\"https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-490x254.jpg
        490w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-300x156.jpg
        300w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-900x467.jpg
        900w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-768x399.jpg
        768w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-1536x797.jpg
        1536w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-2048x1063.jpg
        2048w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-225x117.jpg
        225w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-150x78.jpg
        150w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-1320x685.jpg
        1320w\\\" sizes=\\\"(max-width: 490px) 100vw, 490px\\\" /></a><p id=\\\"caption-attachment-32081\\\"
        class=\\\"wp-caption-text\\\">Foto: Georg Fischer unter <a href=\\\"https://creativecommons.org/licenses/by/4.0/legalcode\\\">CC
        BY 4.0</a></p></div>\\n<p>Von links nach rechts auf dem Podium:</p>\\n<ul>\\n<li><a
        href=\\\"https://www.bbaw.de/die-akademie/bbaw-mitglieder/mitglied-martin-groetschel\\\">Martin
        Gr\xF6tschel</a>\_(eh. Berlin-Brandenburgische Akademie der Wissenschaften)</li>\\n<li><a
        href=\\\"https://www.smb.museum/museen-einrichtungen/institut-fuer-museumsforschung/ueber-uns/mitarbeiterinnen/detail/kathrin-grotz/\\\">Kathrin
        Grotz</a>\_(Staatliche Museen Berlin, SPK)</li>\\n<li><a href=\\\"https://www.tu.berlin/ub/ueber-uns/kontakt/ansprechpartnerinnen\\\">J\xFCrgen
        Christof</a>, (Universit\xE4tsbibliothek Technische Universit\xE4t Berlin)</li>\\n<li><a
        href=\\\"https://www.udk-berlin.de/person/ariane-jessulat/\\\">Ariane Je\xDFulat</a>\_(Universit\xE4t
        der K\xFCnste Berlin)</li>\\n<li><a href=\\\"https://www.uni-potsdam.de/de/cio/index\\\">Peter
        Kost\xE4dt</a>\_(Universit\xE4t Potsdam)</li>\\n<li><a href=\\\"https://www.ibi.hu-berlin.de/de/institut/personen/pampel\\\">Heinz
        Pampel</a>\_(Humboldt-Universit\xE4t zu Berlin, Helmholtz Open Science Office)</li>\\n</ul>\\n<p>Ganz
        rechts am Pult:\_<a href=\\\"http://www.open-access-berlin.de/\\\">Maxi Kindling</a>\_(Open-Access-B\xFCro
        Berlin)</p>\\n<p>Ausf\xFChrliche Informationen zu den Diskutant*innen und
        ihren fachlichen Hintergr\xFCnden finden sich\_<a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\">hier</a>.</p>\\n<h2>20
        Jahre Berliner Erkl\xE4rung</h2>\\n<p><span style=\\\"font-weight: 400;\\\">Nach
        einer kurzen Begr\xFC\xDFung durch die Leiterin des OABB Maxi Kindling f\xFChrte
        </span><a href=\\\"https://www.fu-berlin.de/sites/ub/ueber-uns/team/brandtner/index.html\\\"><span
        style=\\\"font-weight: 400;\\\">Andreas Brandtner</span></a><span style=\\\"font-weight:
        400;\\\">, Direktor der Universit\xE4tsbibliothek der Freien Universit\xE4t
        Berlin und Ko-Leiter der </span><a href=\\\"http://www.open-access-berlin.de/aktivitaeten/index.html\\\"><span
        style=\\\"font-weight: 400;\\\">Arbeitsgruppe Open-Access-Strategie Berlin</span></a><span
        style=\\\"font-weight: 400;\\\">, als Moderator durch die Diskussion. Heimlicher
        Stargast, so der allgemeine Tenor des Nachmittags, war die</span><i><span
        style=\\\"font-weight: 400;\\\"> Berliner Erkl\xE4rung \xFCber offenen Zugang
        zu wissenschaftlichem Wissen </span></i><span style=\\\"font-weight: 400;\\\">aus
        dem Jahr 2003, </span><a href=\\\"https://openaccess.mpg.de/68053/Berliner_Erklaerung_dt_Version_07-2006.pdf\\\"><span
        style=\\\"font-weight: 400;\\\">deren Erscheinen sich fast auf den Tag genau
        zum 20. Mal j\xE4hrte</span></a><span style=\\\"font-weight: 400;\\\">. Ein
        guter Zeitpunkt, um zur\xFCckzublicken, kritisch den Status Quo zu evaluieren
        und Akzente f\xFCr die zuk\xFCnftige Entwicklung zu setzen. Die vielf\xE4ltigen
        Aspekte der Diskussion in G\xE4nze darzustellen ist schwerlich umzusetzen,
        daher beschr\xE4nkt sich der Veranstaltungsbericht auf die gro\xDFen Linien
        der Diskussion.</span></p>\\n<h2>Open-Research-Praktiken in Wissenschaft und
        Kulturerbe</h2>\\n<p><span style=\\\"font-weight: 400;\\\">Schnell wurde klar:
        Die Open-Access-Transformation ist noch lange nicht abgeschlossen, im Gegenteil:
        Sie ist ein Prozess mit offenem Ende und \u2013 obgleich schon viel erreicht
        wurde \u2013 weiterhin Herausforderung und Chance zugleich f\xFCr die Wissenschaft
        in der Region Berlin-Brandenburg wie auch weltweit.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Die Forderung, Open Access zu publizieren,
        ist mittlerweile selbst zum Standard geworden und auch die weitere \xD6ffnung
        des Forschungsprozesses im Sinne von Open Research l\xE4sst sich erfreulicherweise
        vielerorts beobachten. In diesem Sinne betonten sowohl Kathrin Grotz als auch
        Ariane Je\xDFulat diverse Vorz\xFCge, die frei verf\xFCgbare und offen lizenzierte
        Texte, Daten und andere Materialien f\xFCr Wissenschaft und genauso f\xFCr
        die Kunst und die k\xFCnstlerische Forschung br\xE4chten: So einfach wie heute
        sei es noch nie gewesen, auf digitalem Wege an Quellen zu kommen. Der digitale
        Fernzugriff habe den Forschenden wie auch der interessierten \xD6ffentlichkeit
        in den letzten 20 Jahren enorme Ersparnisse an Zeit und Ressourcen gebracht,
        so Je\xDFulat. Gleichzeitig erinnerte die UdK-Professorin aber auch an die
        in der </span><i><span style=\\\"font-weight: 400;\\\">Berliner Erkl\xE4rung</span></i><span
        style=\\\"font-weight: 400;\\\"> angelegte Trennung zwischen wissenschaftlichen
        Forschungs- und k\xFCnstlerischen Kulturdaten. Eine solche Trennung halte
        sie inzwischen f\xFCr \xFCberholt \u2013 tats\xE4chlich sei ein hybrider Datenraum
        zwischen Wissenschaft und Kulturerbe entstanden, der f\xFCr Offene Forschung
        (Open Research) von herausragender Bedeutung sei.</span></p>\\n<p><span style=\\\"font-weight:
        400;\\\">Indirekt best\xE4tigte Kathrin Grotz den Befund ihrer Kollegin, als
        sie die zahlreichen Praktiken erl\xE4uterte, mit denen moderne Museen heutzutage
        Daten verarbeiten, vernetzen und auswerten w\xFCrden. Und sie wies darauf
        hin, dass aufgrund der technologischen Entwicklungen weitere wichtige Anwendungen
        entst\xFCnden, die sich heute noch gar nicht oder nur in Umrissen erkennen
        lie\xDFen. Man m\xFCsse daher strategisch und in langfristigen Zeithorizonten
        denken und \u2013 so paradox es zun\xE4chst klingen mag \u2013 dem Unvorhergesehenen
        Raum lassen. Manchmal, so unterstrich die Stellvertretende Direktorin des
        Instituts f\xFCr Museumsforschung, w\xFCrden bestimmte Daten erst in 20 Jahren
        wertvoll werden.</span></p>\\n<h2>Open Access zwischen Community und Kommerz</h2>\\n<p><span
        style=\\\"font-weight: 400;\\\">Neben der Einheit von Wissenschaft und Kunst
        hinsichtlich Open Research zog sich das Spannungsfeld von \u201ECommunity
        und Kommerzialisierung\u201D, angelehnt an das diesj\xE4hrige Motto der </span><a
        href=\\\"https://www.openaccessweek.org/\\\"><span style=\\\"font-weight:
        400;\\\">internationalen Open-Access-Week</span></a><span style=\\\"font-weight:
        400;\\\">, wie ein roter Faden durch die Diskussion.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Einerseits war man sich auf dem Panel durchaus
        einig in der Frage, dass die Open-Access-Community ihre vielf\xE4ltigen Kooperations-
        und Vernetzungsarbeiten weiterhin so engagiert wie bisher f\xFChren und weiter
        ausbauen sollte, gerade in Zeiten allgemein schrumpfender Etats. Das sei neben
        ad\xE4quater politischer F\xF6rderstrukturen ein Schl\xFCssel zum Erfolg,
        wie Heinz Pampel betonte. Andererseits zeigte man sich auf dem Podium auch
        etwas zerknirscht, als die Sprache auf die </span><a href=\\\"https://www.forschung-und-lehre.de/forschung/open-acess-vereinbarung-fuer-deutschsprachige-forschung-5889\\\"><span
        style=\\\"font-weight: 400;\\\">k\xFCrzlich abgeschlossenen Verhandlungen
        zum DEAL-Vertrag</span></a><span style=\\\"font-weight: 400;\\\"> zwischen
        der Allianz der deutschen Wissenschaftsorganisationen und dem international
        agierenden Gro\xDFverlag Elsevier kam. J\xFCrgen Christof etwa merkte sehr
        deutlich an, dass die Finanzstr\xF6me, die das wissenschaftliche Publizieren
        sicherten, den Forschenden oftmals verborgen blieben. Das aber \xE4ndere nichts
        an den erheblichen Betr\xE4gen, die vorrangig an die Gro\xDFverlage flie\xDFen
        und die sich so in den letzten Jahrzehnten eine ebenfalls erhebliche Marktmacht
        aufbauen konnten \u2013 nicht trotz, sondern gerade aufgrund von Open Access
        und den damit von den Verlagen aufgerufenen Geb\xFChren.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Dass die DEAL-Vertr\xE4ge mit Springer Nature,
        Wiley und nun auch Elsevier eigentlich als Transformationsvertr\xE4ge gedacht
        waren, also das Flipping der Zeitschriften zum Ziel hatten, kann man gerade
        in der 20-j\xE4hrigen R\xFCckschau nach der </span><i><span style=\\\"font-weight:
        400;\\\">Berliner Erkl\xE4rung</span></i><span style=\\\"font-weight: 400;\\\">
        getrost als Flop bezeichnen. </span><span style=\\\"font-weight: 400;\\\">Dazu
        kommt das \u2013 innerhalb der Diskussion nur am Rande besprochene \u2013
        digitale Tracking von Forschenden und ihrem Nutzungsverhalten. Mithilfe der
        Aggregation und Analyse von Nutzungsdaten konnten sich wissenschaftliche Gro\xDFverlage
        wie Elsevier in den vergangenen Jahren u.a. dank enormer Open-Access-Geb\xFChren
        ein neues, durchaus lukratives Gesch\xE4ftsmodell aufbauen. Mittlerweile wird
        es von vielen Stellen als </span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/tag/datentracking/\\\"><span
        style=\\\"font-weight: 400;\\\">Gefahr f\xFCr die freie Wissenschaft</span></a><span
        style=\\\"font-weight: 400;\\\"> gesehen.</span></p>\\n<h2>Informiert publizieren,
        juristisch beraten, nachhaltig f\xF6rdern</h2>\\n<p><a href=\\\"https://www.hiig.de/publication/wissenschaftsgeleitetes-publizieren/\\\"><span
        style=\\\"font-weight: 400;\\\">Wissenschaftsgeleitete Publikationsformate</span></a><span
        style=\\\"font-weight: 400;\\\"> und </span><a href=\\\"https://en.wikipedia.org/wiki/Diamond_open_access\\\"><span
        style=\\\"font-weight: 400;\\\">Diamond Open Access</span></a><span style=\\\"font-weight:
        400;\\\"> seien darauf die richtige Antwort und auch auf dem Vormarsch, diagnostizierte
        Peter Kost\xE4dt unter zustimmendem allgemeinem Nicken auf dem Podium. Gleichzeitig
        f\xFChre das zum n\xE4chsten Problem, n\xE4mlich den jahrzehntelang gewachsenen
        Reputationsstrukturen, die in der Transformation sichtbar w\xFCrden. Hier
        seien innere Widerst\xE4nde seitens der Wissenschaft oder \u2013 positiv ausgedr\xFCckt
        \u2013 nicht ausreichend Anreize vorhanden, die Forschende dazu bringen w\xFCrden,
        von den Gro\xDFverlagen und ihren etablierten Zeitschriften Abstand zu nehmen.
        Publikationsentscheidungen, so Pampel, sollten \u201Einformierte Entscheidungen\u201D
        der Forschenden sein und die Bibliotheken h\xE4tten die verantwortungsvolle
        Aufgabe, hierbei zu beraten und zu unterst\xFCtzen.</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Informiert m\xFCssten wissenschaftlich und
        k\xFCnstlerisch Forschende aber nicht nur in ihrem eigenen Fachgebiet und
        den dortigen Publikationsstrukturen sein. Kathrin Grotz zufolge dr\xE4ngten
        sich den Forschenden im Alltag auch vielf\xE4ltige juristische und ethische
        Fragen auf. Solche Fragen h\xE4tten sich im Zuge der Digitalisierung erheblich
        vermehrt und auch in ihrer Komplexit\xE4t verst\xE4rkt, wie sie am Beispiel
        der Forschungs- und Kulturdaten untermauerte. Ariane Je\xDFulat hob die Rechtsberatung
        durch den </span><a href=\\\"https://nfdi4culture.de/de/helpdesk.html\\\"><span
        style=\\\"font-weight: 400;\\\">Legal Helpdesk der Initiative NFDI4Culture</span></a><span
        style=\\\"font-weight: 400;\\\"> als wegweisendes Beispiel hervor. Auch pl\xE4dierte
        sie f\xFCr die vielf\xE4ltigen Potentiale, die im Management von Forschungs-
        und Kulturdaten steckten, die sich ohne juristische Beratung aber nur unzureichend
        heben lie\xDFen. Und schob hinterher, dass gerade bei Sammlungen und Kulturdaten
        die langfristige Perspektive von gro\xDFer Bedeutung sei.</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Dem st\xFCnde, wie mehrere Teilnehmer*innen
        auf dem Podium auch in anderen Kontexten klarstellten, in zu vielen F\xE4llen
        die Logik der zeitlich befristeten Projekte entgegen. J\xFCrgen Christof formulierte
        hier deutlich, wie notwendig ein ordentlich ausfinanzierter Regelbetrieb sei,
        \xFCber den sich Infrastrukturen aufbauen und Ideen \xFCber Projektende hinaus
        realisieren lie\xDFen. Die \xDCberf\xFChrung von Drittmittel-finanzierten
        Projekten in einen nachhaltig und institutionell langfristig gesicherten Regelbetrieb
        m\xFCsse daher st\xE4rker politisch gef\xF6rdert werden.</span></p>\\n<h2>Das
        harte Brett der Open-Access-Transformation</h2>\\n<p><span style=\\\"font-weight:
        400;\\\">Die Zw\xE4nge, die sich aus F\xF6rderlogik und dem f\xF6deralen System
        nach wie vor ergeben w\xFCrden, monierte gegen Ende der Diskussion auch Martin
        Gr\xF6tschel, der die Open-Access-Transformation langj\xE4hrig begleitet und
        vorangetrieben hatte. Seiner Meinung nach habe deswegen die Entwicklung bisher
        \u201Edoppelt bis dreifach so lange gedauert\u201D als urspr\xFCnglich erwartet.
        Auch betonte er die zahlreichen, teils in verschiedenen Geschwindigkeiten
        laufenden Parallel-Prozesse. J\xFCrgen Christof wiederum w\xFCnschte sich
        eine Halbierung der wissenschaftlichen Publikationen bis zum Jahre 2040.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Am Ende zeigt sich deutlich, dass Open Access
        und Open Research keine vereinzelten Themen der Wissenschaft sind, sondern
        an der Transformation eines ganzen Wissenschaftssystems h\xE4ngen. Daraus
        ergeben sich zwei wichtige Perspektiven f\xFCr die Zukunft: Die Zusammenarbeit
        und Kooperation unter verschiedenen Interessengruppen werden noch wichtiger
        werden als bisher. Und gleichzeitig werden der Aufbau und die Finanzierung
        offener Infrastrukturen dar\xFCber entscheiden, ob diese Bem\xFChungen auch
        dauerhaft der Kommerzialisierung von Wissen etwas entgegenhalten k\xF6nnen.
        </span><span style=\\\"font-weight: 400;\\\">Denn offene, selbst getragene
        Infrastrukturen k\xF6nnen auch f\xFCr andere Probleme, insbesondere das oft
        kritisierte Datentracking von Verlagen, eine geeignete Antwort bieten, insofern
        sich die Wissenschaft das Publizieren der eigenen Forschung wieder st\xE4rker
        in die eigenen H\xE4nde holen k\xF6nnte.</span></p>\\n<p style=\\\"text-align:
        right;\\\"><em>Urspr\xFCnglich ver\xF6ffentlicht <a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/10/31/20-jahre-berliner-erklaerung-open-access-berlin-brandenburg/\\\">im
        Blog des Open-Access-B\xFCros Berlin</a>.</em></p>\\n<p>&nbsp;</p>\\n<div
        class=\\\"merksatz\\\">\\n<h2>Sie m\xF6chten iRights.info unterst\xFCtzen?</h2>\\n<p><strong><a
        href=\\\"https://irights.info/\\\">iRights.info</a>\_informiert und erkl\xE4rt
        rund um das Thema \u201EUrheberrecht und Kreativit\xE4t in der digitalen Welt\u201C.
        Alle Texte erscheinen kostenlos und offen lizenziert.</strong></p>\\n<p><strong>Wenn
        Sie m\xF6gen, k\xF6nnen Sie uns \xFCber die gemeinn\xFCtzige\_<a href=\\\"https://www.betterplace.org/de/projects/120241-irights-info-informationsplattform-zum-urheberrecht-in-der-digitalen-welt\\\">Spendenplattform
        Betterplace</a>\_unterst\xFCtzen und daf\xFCr eine Spendenbescheinigung erhalten.
        Betterplace akzeptiert PayPal, Bankeinzug, Kreditkarte, paydirekt oder \xDCberweisung.</strong></p>\\n<p><strong>Besonders
        freuen wir uns \xFCber einen regelm\xE4\xDFigen Beitrag, beispielsweise als
        monatlicher Dauerauftrag.\_F\xFCr Ihre Unterst\xFCtzung dankt Ihnen herzlich
        der\_<a href=\\\"https://irights.info/was-ist-irightsinfo-projekttrger\\\">gemeinn\xFCtzige
        iRights e.V.</a>!</strong></p>\\n</div>\\n<p><script src=\\\"https://www.betterplace.org/de/widgets/overlays/EjCxZ8kpYxhZeyTSTKxRZ33M.js\\\"
        async=\\\"async\\\" type=\\\"text/javascript\\\"></script></p>\\n<p>The post
        <a rel=\\\"nofollow\\\" href=\\\"https://irights.info/artikel/20-jahre-berliner-erklaerung-open-access-berlin-brandenburg/32078\\\">20
        Jahre Berliner Erkl\xE4rung f\xFCr Open Access: Wie ist die Lage in Berlin
        und Brandenburg?</a> appeared first on <a rel=\\\"nofollow\\\" href=\\\"https://irights.info\\\">iRights.info</a>.</p>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/6mhvf-28f30\",\"guid\":\"https://irights.info/?post_type=custom_artikel&p=32078\",\"id\":\"c85a507c-9125-40a1-9dc2-da10129b57c6\",\"image\":\"https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-490x254.jpg\",\"language\":\"de\",\"published_at\":1698908451,\"reference\":[],\"relationships\":[],\"summary\":\"Open
        Access f\xFCr \xF6ffentliche Forschung und Kultur: Zwei Jahrzehnte ist es
        schon her, dass sich zahlreiche Organisationen in der <em>Berliner Erkl\xE4rung</em>
        daf\xFCr aussprachen. Wie weit ist die \xD6ffnung tats\xE4chlich gekommen?
        Und was gilt es weiterhin zu tun? Spannende Fragen, die Berliner und Brandenburger
        Vertreter*innen aus Wissenschaft und Kultur letzte Woche diskutierten.\",\"tags\":[\"Gesellschaft
        + Kunst\",\"Museen + Archive\",\"Urheberrecht\",\"Wissen + Open Access\",\"Wissenschaft\"],\"title\":\"20
        Jahre Berliner Erkl\xE4rung f\xFCr Open Access: Wie ist die Lage in Berlin
        und Brandenburg?\",\"updated_at\":1698917891,\"url\":\"https://irights.info/artikel/20-jahre-berliner-erklaerung-open-access-berlin-brandenburg/32078\"},\"highlight\":{\"authors\":[{\"name\":\"Georg
        Fischer\"}],\"content_html\":\"<p>Open Access f\xFCr \xF6ffentliche Forschung
        und Kultur: Zwei Jahrzehnte ist es schon her, dass sich zahlreiche Organisationen
        in der <em>Berliner Erkl\xE4rung</em> daf\xFCr aussprachen. Wie weit ist die
        \xD6ffnung tats\xE4chlich gekommen? Und was gilt es weiterhin zu tun? Spannende
        Fragen, die Berliner und Brandenburger Vertreter*innen aus Wissenschaft und
        Kultur letzte Woche diskutierten.</p>\\n<p><span id=\\\"more-32078\\\"></span></p>\\n<p>Am
        24. Oktober 2023 luden das\_<a href=\\\"https://www.ibi.hu-berlin.de/de\\\">Institut
        f\xFCr Bibliotheks- und Informationswissenschaft</a>\_(IBI) der Humboldt-Universit\xE4t
        zu Berlin (HU), der\_<a href=\\\"https://www.kobv.de/\\\">Kooperative Bibliotheksverbund
        Berlin-Brandenburg</a>\_(KOBV), das\_<a href=\\\"http://www.open-access-berlin.de/index.html\\\">Open-Access-B\xFCro
        Berlin</a>\_(OABB) und die\_<a href=\\\"https://open-access-brandenburg.de/\\\">Vernetzungs-
        und Kompetenzstelle Open Access Brandenburg</a>\_(VuK) dazu ein, im\_<a href=\\\"https://www.zib.de/\\\">Zuse
        Institut Berlin</a>\_(ZIB) den Status von Open Access in der Region Berlin-Brandenburg
        zu diskutieren. Unter dem Motto \u201E<a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\">Was
        wurde erreicht und wo geht es hin?</a>\u201D sa\xDFen insgesamt sechs Vertreter*innen
        der Berliner und Brandenburger Open-Access-Community auf dem Podium.</p>\\n<div
        id=\\\"attachment_32081\\\" style=\\\"width: 500px\\\" class=\\\"wp-caption
        alignnone\\\"><a href=\\\"https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-scaled.jpg\\\"><img
        aria-describedby=\\\"caption-attachment-32081\\\" loading=\\\"lazy\\\" class=\\\"size-content
        wp-image-32081\\\" src=\\\"https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-490x254.jpg\\\"
        alt=\\\"\\\" width=\\\"490\\\" height=\\\"254\\\" srcset=\\\"https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-490x254.jpg
        490w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-300x156.jpg
        300w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-900x467.jpg
        900w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-768x399.jpg
        768w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-1536x797.jpg
        1536w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-2048x1063.jpg
        2048w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-225x117.jpg
        225w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-150x78.jpg
        150w, https://irights.info/wp-content/uploads/2023/11/IMG_20231024_1601265362-1320x685.jpg
        1320w\\\" sizes=\\\"(max-width: 490px) 100vw, 490px\\\" /></a><p id=\\\"caption-attachment-32081\\\"
        class=\\\"wp-caption-text\\\">Foto: Georg Fischer unter <a href=\\\"https://creativecommons.org/licenses/by/4.0/legalcode\\\">CC
        BY 4.0</a></p></div>\\n<p>Von links nach rechts auf dem Podium:</p>\\n<ul>\\n<li><a
        href=\\\"https://www.bbaw.de/die-akademie/bbaw-mitglieder/mitglied-martin-groetschel\\\">Martin
        Gr\xF6tschel</a>\_(eh. Berlin-Brandenburgische Akademie der Wissenschaften)</li>\\n<li><a
        href=\\\"https://www.smb.museum/museen-einrichtungen/institut-fuer-museumsforschung/ueber-uns/mitarbeiterinnen/detail/kathrin-grotz/\\\">Kathrin
        Grotz</a>\_(Staatliche Museen Berlin, SPK)</li>\\n<li><a href=\\\"https://www.tu.berlin/ub/ueber-uns/kontakt/ansprechpartnerinnen\\\">J\xFCrgen
        Christof</a>, (Universit\xE4tsbibliothek Technische Universit\xE4t Berlin)</li>\\n<li><a
        href=\\\"https://www.udk-berlin.de/person/ariane-jessulat/\\\">Ariane Je\xDFulat</a>\_(Universit\xE4t
        der K\xFCnste Berlin)</li>\\n<li><a href=\\\"https://www.uni-potsdam.de/de/cio/index\\\">Peter
        Kost\xE4dt</a>\_(Universit\xE4t Potsdam)</li>\\n<li><a href=\\\"https://www.ibi.hu-berlin.de/de/institut/personen/pampel\\\">Heinz
        Pampel</a>\_(Humboldt-Universit\xE4t zu Berlin, Helmholtz Open Science Office)</li>\\n</ul>\\n<p>Ganz
        rechts am Pult:\_<a href=\\\"http://www.open-access-berlin.de/\\\">Maxi Kindling</a>\_(Open-Access-B\xFCro
        Berlin)</p>\\n<p>Ausf\xFChrliche Informationen zu den Diskutant*innen und
        ihren fachlichen Hintergr\xFCnden finden sich\_<a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\">hier</a>.</p>\\n<h2>20
        Jahre Berliner Erkl\xE4rung</h2>\\n<p><span style=\\\"font-weight: 400;\\\">Nach
        einer kurzen Begr\xFC\xDFung durch die Leiterin des OABB Maxi Kindling f\xFChrte
        </span><a href=\\\"https://www.fu-berlin.de/sites/ub/ueber-uns/team/brandtner/index.html\\\"><span
        style=\\\"font-weight: 400;\\\">Andreas Brandtner</span></a><span style=\\\"font-weight:
        400;\\\">, Direktor der Universit\xE4tsbibliothek der Freien Universit\xE4t
        Berlin und Ko-Leiter der </span><a href=\\\"http://www.open-access-berlin.de/aktivitaeten/index.html\\\"><span
        style=\\\"font-weight: 400;\\\">Arbeitsgruppe Open-Access-Strategie Berlin</span></a><span
        style=\\\"font-weight: 400;\\\">, als Moderator durch die Diskussion. Heimlicher
        Stargast, so der allgemeine Tenor des Nachmittags, war die</span><i><span
        style=\\\"font-weight: 400;\\\"> Berliner Erkl\xE4rung \xFCber offenen Zugang
        zu wissenschaftlichem Wissen </span></i><span style=\\\"font-weight: 400;\\\">aus
        dem Jahr 2003, </span><a href=\\\"https://openaccess.mpg.de/68053/Berliner_Erklaerung_dt_Version_07-2006.pdf\\\"><span
        style=\\\"font-weight: 400;\\\">deren Erscheinen sich fast auf den Tag genau
        zum 20. Mal j\xE4hrte</span></a><span style=\\\"font-weight: 400;\\\">. Ein
        guter Zeitpunkt, um zur\xFCckzublicken, kritisch den Status Quo zu evaluieren
        und Akzente f\xFCr die zuk\xFCnftige Entwicklung zu setzen. Die vielf\xE4ltigen
        Aspekte der Diskussion in G\xE4nze darzustellen ist schwerlich umzusetzen,
        daher beschr\xE4nkt sich der Veranstaltungsbericht auf die gro\xDFen Linien
        der Diskussion.</span></p>\\n<h2>Open-Research-Praktiken in Wissenschaft und
        Kulturerbe</h2>\\n<p><span style=\\\"font-weight: 400;\\\">Schnell wurde klar:
        Die Open-Access-Transformation ist noch lange nicht abgeschlossen, im Gegenteil:
        Sie ist ein Prozess mit offenem Ende und \u2013 obgleich schon viel erreicht
        wurde \u2013 weiterhin Herausforderung und Chance zugleich f\xFCr die Wissenschaft
        in der Region Berlin-Brandenburg wie auch weltweit.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Die Forderung, Open Access zu publizieren,
        ist mittlerweile selbst zum Standard geworden und auch die weitere \xD6ffnung
        des Forschungsprozesses im Sinne von Open Research l\xE4sst sich erfreulicherweise
        vielerorts beobachten. In diesem Sinne betonten sowohl Kathrin Grotz als auch
        Ariane Je\xDFulat diverse Vorz\xFCge, die frei verf\xFCgbare und offen lizenzierte
        Texte, Daten und andere Materialien f\xFCr Wissenschaft und genauso f\xFCr
        die Kunst und die k\xFCnstlerische Forschung br\xE4chten: So einfach wie heute
        sei es noch nie gewesen, auf digitalem Wege an Quellen zu kommen. Der digitale
        Fernzugriff habe den Forschenden wie auch der interessierten \xD6ffentlichkeit
        in den letzten 20 Jahren enorme Ersparnisse an Zeit und Ressourcen gebracht,
        so Je\xDFulat. Gleichzeitig erinnerte die UdK-Professorin aber auch an die
        in der </span><i><span style=\\\"font-weight: 400;\\\">Berliner Erkl\xE4rung</span></i><span
        style=\\\"font-weight: 400;\\\"> angelegte Trennung zwischen wissenschaftlichen
        Forschungs- und k\xFCnstlerischen Kulturdaten. Eine solche Trennung halte
        sie inzwischen f\xFCr \xFCberholt \u2013 tats\xE4chlich sei ein hybrider Datenraum
        zwischen Wissenschaft und Kulturerbe entstanden, der f\xFCr Offene Forschung
        (Open Research) von herausragender Bedeutung sei.</span></p>\\n<p><span style=\\\"font-weight:
        400;\\\">Indirekt best\xE4tigte Kathrin Grotz den Befund ihrer Kollegin, als
        sie die zahlreichen Praktiken erl\xE4uterte, mit denen moderne Museen heutzutage
        Daten verarbeiten, vernetzen und auswerten w\xFCrden. Und sie wies darauf
        hin, dass aufgrund der technologischen Entwicklungen weitere wichtige Anwendungen
        entst\xFCnden, die sich heute noch gar nicht oder nur in Umrissen erkennen
        lie\xDFen. Man m\xFCsse daher strategisch und in langfristigen Zeithorizonten
        denken und \u2013 so paradox es zun\xE4chst klingen mag \u2013 dem Unvorhergesehenen
        Raum lassen. Manchmal, so unterstrich die Stellvertretende Direktorin des
        Instituts f\xFCr Museumsforschung, w\xFCrden bestimmte Daten erst in 20 Jahren
        wertvoll werden.</span></p>\\n<h2>Open Access zwischen Community und Kommerz</h2>\\n<p><span
        style=\\\"font-weight: 400;\\\">Neben der Einheit von Wissenschaft und Kunst
        hinsichtlich Open Research zog sich das Spannungsfeld von \u201ECommunity
        und Kommerzialisierung\u201D, angelehnt an das diesj\xE4hrige Motto der </span><a
        href=\\\"https://www.openaccessweek.org/\\\"><span style=\\\"font-weight:
        400;\\\">internationalen Open-Access-Week</span></a><span style=\\\"font-weight:
        400;\\\">, wie ein roter Faden durch die Diskussion.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Einerseits war man sich auf dem Panel durchaus
        einig in der Frage, dass die Open-Access-Community ihre vielf\xE4ltigen Kooperations-
        und Vernetzungsarbeiten weiterhin so engagiert wie bisher f\xFChren und weiter
        ausbauen sollte, gerade in Zeiten allgemein schrumpfender Etats. Das sei neben
        ad\xE4quater politischer F\xF6rderstrukturen ein Schl\xFCssel zum Erfolg,
        wie Heinz Pampel betonte. Andererseits zeigte man sich auf dem Podium auch
        etwas zerknirscht, als die Sprache auf die </span><a href=\\\"https://www.forschung-und-lehre.de/forschung/open-acess-vereinbarung-fuer-deutschsprachige-forschung-5889\\\"><span
        style=\\\"font-weight: 400;\\\">k\xFCrzlich abgeschlossenen Verhandlungen
        zum DEAL-Vertrag</span></a><span style=\\\"font-weight: 400;\\\"> zwischen
        der Allianz der deutschen Wissenschaftsorganisationen und dem international
        agierenden Gro\xDFverlag Elsevier kam. J\xFCrgen Christof etwa merkte sehr
        deutlich an, dass die Finanzstr\xF6me, die das wissenschaftliche Publizieren
        sicherten, den Forschenden oftmals verborgen blieben. Das aber \xE4ndere nichts
        an den erheblichen Betr\xE4gen, die vorrangig an die Gro\xDFverlage flie\xDFen
        und die sich so in den letzten Jahrzehnten eine ebenfalls erhebliche Marktmacht
        aufbauen konnten \u2013 nicht trotz, sondern gerade aufgrund von Open Access
        und den damit von den Verlagen aufgerufenen Geb\xFChren.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Dass die DEAL-Vertr\xE4ge mit Springer Nature,
        Wiley und nun auch Elsevier eigentlich als Transformationsvertr\xE4ge gedacht
        waren, also das Flipping der Zeitschriften zum Ziel hatten, kann man gerade
        in der 20-j\xE4hrigen R\xFCckschau nach der </span><i><span style=\\\"font-weight:
        400;\\\">Berliner Erkl\xE4rung</span></i><span style=\\\"font-weight: 400;\\\">
        getrost als Flop bezeichnen. </span><span style=\\\"font-weight: 400;\\\">Dazu
        kommt das \u2013 innerhalb der Diskussion nur am Rande besprochene \u2013
        digitale Tracking von Forschenden und ihrem Nutzungsverhalten. Mithilfe der
        Aggregation und Analyse von Nutzungsdaten konnten sich wissenschaftliche Gro\xDFverlage
        wie Elsevier in den vergangenen Jahren u.a. dank enormer Open-Access-Geb\xFChren
        ein neues, durchaus lukratives Gesch\xE4ftsmodell aufbauen. Mittlerweile wird
        es von vielen Stellen als </span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/tag/datentracking/\\\"><span
        style=\\\"font-weight: 400;\\\">Gefahr f\xFCr die freie Wissenschaft</span></a><span
        style=\\\"font-weight: 400;\\\"> gesehen.</span></p>\\n<h2>Informiert publizieren,
        juristisch beraten, nachhaltig f\xF6rdern</h2>\\n<p><a href=\\\"https://www.hiig.de/publication/wissenschaftsgeleitetes-publizieren/\\\"><span
        style=\\\"font-weight: 400;\\\">Wissenschaftsgeleitete Publikationsformate</span></a><span
        style=\\\"font-weight: 400;\\\"> und </span><a href=\\\"https://en.wikipedia.org/wiki/Diamond_open_access\\\"><span
        style=\\\"font-weight: 400;\\\">Diamond Open Access</span></a><span style=\\\"font-weight:
        400;\\\"> seien darauf die richtige Antwort und auch auf dem Vormarsch, diagnostizierte
        Peter Kost\xE4dt unter zustimmendem allgemeinem Nicken auf dem Podium. Gleichzeitig
        f\xFChre das zum n\xE4chsten Problem, n\xE4mlich den jahrzehntelang gewachsenen
        Reputationsstrukturen, die in der Transformation sichtbar w\xFCrden. Hier
        seien innere Widerst\xE4nde seitens der Wissenschaft oder \u2013 positiv ausgedr\xFCckt
        \u2013 nicht ausreichend Anreize vorhanden, die Forschende dazu bringen w\xFCrden,
        von den Gro\xDFverlagen und ihren etablierten Zeitschriften Abstand zu nehmen.
        Publikationsentscheidungen, so Pampel, sollten \u201Einformierte Entscheidungen\u201D
        der Forschenden sein und die Bibliotheken h\xE4tten die verantwortungsvolle
        Aufgabe, hierbei zu beraten und zu unterst\xFCtzen.</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Informiert m\xFCssten wissenschaftlich und
        k\xFCnstlerisch Forschende aber nicht nur in ihrem eigenen Fachgebiet und
        den dortigen Publikationsstrukturen sein. Kathrin Grotz zufolge dr\xE4ngten
        sich den Forschenden im Alltag auch vielf\xE4ltige juristische und ethische
        Fragen auf. Solche Fragen h\xE4tten sich im Zuge der Digitalisierung erheblich
        vermehrt und auch in ihrer Komplexit\xE4t verst\xE4rkt, wie sie am Beispiel
        der Forschungs- und Kulturdaten untermauerte. Ariane Je\xDFulat hob die Rechtsberatung
        durch den </span><a href=\\\"https://nfdi4culture.de/de/helpdesk.html\\\"><span
        style=\\\"font-weight: 400;\\\">Legal Helpdesk der Initiative NFDI4Culture</span></a><span
        style=\\\"font-weight: 400;\\\"> als wegweisendes Beispiel hervor. Auch pl\xE4dierte
        sie f\xFCr die vielf\xE4ltigen Potentiale, die im Management von Forschungs-
        und Kulturdaten steckten, die sich ohne juristische Beratung aber nur unzureichend
        heben lie\xDFen. Und schob hinterher, dass gerade bei Sammlungen und Kulturdaten
        die langfristige Perspektive von gro\xDFer Bedeutung sei.</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Dem st\xFCnde, wie mehrere Teilnehmer*innen
        auf dem Podium auch in anderen Kontexten klarstellten, in zu vielen F\xE4llen
        die Logik der zeitlich befristeten Projekte entgegen. J\xFCrgen Christof formulierte
        hier deutlich, wie notwendig ein ordentlich ausfinanzierter Regelbetrieb sei,
        \xFCber den sich Infrastrukturen aufbauen und Ideen \xFCber Projektende hinaus
        realisieren lie\xDFen. Die \xDCberf\xFChrung von Drittmittel-finanzierten
        Projekten in einen nachhaltig und institutionell langfristig gesicherten Regelbetrieb
        m\xFCsse daher st\xE4rker politisch gef\xF6rdert werden.</span></p>\\n<h2>Das
        harte Brett der Open-Access-Transformation</h2>\\n<p><span style=\\\"font-weight:
        400;\\\">Die Zw\xE4nge, die sich aus F\xF6rderlogik und dem f\xF6deralen System
        nach wie vor ergeben w\xFCrden, monierte gegen Ende der Diskussion auch Martin
        Gr\xF6tschel, der die Open-Access-Transformation langj\xE4hrig begleitet und
        vorangetrieben hatte. Seiner Meinung nach habe deswegen die Entwicklung bisher
        \u201Edoppelt bis dreifach so lange gedauert\u201D als urspr\xFCnglich erwartet.
        Auch betonte er die zahlreichen, teils in verschiedenen Geschwindigkeiten
        laufenden Parallel-Prozesse. J\xFCrgen Christof wiederum w\xFCnschte sich
        eine Halbierung der wissenschaftlichen Publikationen bis zum Jahre 2040.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400;\\\">Am Ende zeigt sich deutlich, dass Open Access
        und Open Research keine vereinzelten Themen der Wissenschaft sind, sondern
        an der Transformation eines ganzen Wissenschaftssystems h\xE4ngen. Daraus
        ergeben sich zwei wichtige Perspektiven f\xFCr die Zukunft: Die Zusammenarbeit
        und Kooperation unter verschiedenen Interessengruppen werden noch wichtiger
        werden als bisher. Und gleichzeitig werden der Aufbau und die Finanzierung
        offener Infrastrukturen dar\xFCber entscheiden, ob diese Bem\xFChungen auch
        dauerhaft der Kommerzialisierung von Wissen etwas entgegenhalten k\xF6nnen.
        </span><span style=\\\"font-weight: 400;\\\">Denn offene, selbst getragene
        Infrastrukturen k\xF6nnen auch f\xFCr andere Probleme, insbesondere das oft
        kritisierte Datentracking von Verlagen, eine geeignete Antwort bieten, insofern
        sich die Wissenschaft das Publizieren der eigenen Forschung wieder st\xE4rker
        in die eigenen H\xE4nde holen k\xF6nnte.</span></p>\\n<p style=\\\"text-align:
        right;\\\"><em>Urspr\xFCnglich ver\xF6ffentlicht <a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/10/31/20-jahre-berliner-erklaerung-open-access-berlin-brandenburg/\\\">im
        Blog des Open-Access-B\xFCros Berlin</a>.</em></p>\\n<p>&nbsp;</p>\\n<div
        class=\\\"merksatz\\\">\\n<h2>Sie m\xF6chten iRights.info unterst\xFCtzen?</h2>\\n<p><strong><a
        href=\\\"https://irights.info/\\\">iRights.info</a>\_informiert und erkl\xE4rt
        rund um das Thema \u201EUrheberrecht und Kreativit\xE4t in der digitalen Welt\u201C.
        Alle Texte erscheinen kostenlos und offen lizenziert.</strong></p>\\n<p><strong>Wenn
        Sie m\xF6gen, k\xF6nnen Sie uns \xFCber die gemeinn\xFCtzige\_<a href=\\\"https://www.betterplace.org/de/projects/120241-irights-info-informationsplattform-zum-urheberrecht-in-der-digitalen-welt\\\">Spendenplattform
        Betterplace</a>\_unterst\xFCtzen und daf\xFCr eine Spendenbescheinigung erhalten.
        Betterplace akzeptiert PayPal, Bankeinzug, Kreditkarte, paydirekt oder \xDCberweisung.</strong></p>\\n<p><strong>Besonders
        freuen wir uns \xFCber einen regelm\xE4\xDFigen Beitrag, beispielsweise als
        monatlicher Dauerauftrag.\_F\xFCr Ihre Unterst\xFCtzung dankt Ihnen herzlich
        der\_<a href=\\\"https://irights.info/was-ist-irightsinfo-projekttrger\\\">gemeinn\xFCtzige
        iRights e.V.</a>!</strong></p>\\n</div>\\n<p><script src=\\\"https://www.betterplace.org/de/widgets/overlays/EjCxZ8kpYxhZeyTSTKxRZ33M.js\\\"
        async=\\\"async\\\" type=\\\"text/javascript\\\"></script></p>\\n<p>The post
        <a rel=\\\"nofollow\\\" href=\\\"https://irights.info/artikel/20-jahre-berliner-erklaerung-open-access-berlin-brandenburg/32078\\\">20
        Jahre Berliner Erkl\xE4rung f\xFCr Open Access: Wie ist die Lage in Berlin
        und Brandenburg?</a> appeared first on <a rel=\\\"nofollow\\\" href=\\\"https://irights.info\\\">iRights.info</a>.</p>\",\"doi\":\"https://doi.org/10.59350/6mhvf-28f30\",\"reference\":[],\"summary\":\"Open
        Access f\xFCr \xF6ffentliche Forschung und Kultur: Zwei Jahrzehnte ist es
        schon her, dass sich zahlreiche Organisationen in der <em>Berliner Erkl\xE4rung</em>
        daf\xFCr aussprachen. Wie weit ist die \xD6ffnung tats\xE4chlich gekommen?
        Und was gilt es weiterhin zu tun? Spannende Fragen, die Berliner und Brandenburger
        Vertreter*innen aus Wissenschaft und Kultur letzte Woche diskutierten.\",\"tags\":[\"Gesellschaft
        + Kunst\",\"Museen + Archive\",\"Urheberrecht\",\"Wissen + Open Access\",\"Wissenschaft\"],\"title\":\"20
        Jahre Berliner Erkl\xE4rung f\xFCr Open Access: Wie ist die Lage in Berlin
        und Brandenburg?\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Georg
        Fischer\"}],\"blog_id\":\"3r60t96\",\"blog_name\":\"Open Access Blog Berlin\",\"blog_slug\":\"oaberlin\",\"content_html\":\"<p><span
        style=\\\"font-weight: 400\\\">Am 24. Oktober 2023 luden das </span><a href=\\\"https://www.ibi.hu-berlin.de/de\\\"><span
        style=\\\"font-weight: 400\\\">Institut f\xFCr Bibliotheks- und Informationswissenschaft</span></a><span
        style=\\\"font-weight: 400\\\"> (IBI) der Humboldt-Universit\xE4t zu Berlin
        (HU), der </span><a href=\\\"https://www.kobv.de/\\\"><span style=\\\"font-weight:
        400\\\">Kooperative Bibliotheksverbund Berlin-Brandenburg</span></a><span
        style=\\\"font-weight: 400\\\"> (KOBV), das </span><a href=\\\"http://www.open-access-berlin.de/index.html\\\"><span
        style=\\\"font-weight: 400\\\">Open-Access-B\xFCro Berlin</span></a><span
        style=\\\"font-weight: 400\\\"> (OABB) und die </span><a href=\\\"https://open-access-brandenburg.de/\\\"><span
        style=\\\"font-weight: 400\\\">Vernetzungs- und Kompetenzstelle Open Access
        Brandenburg</span></a><span style=\\\"font-weight: 400\\\"> (VuK) dazu ein,
        im </span><a href=\\\"https://www.zib.de\\\"><span style=\\\"font-weight:
        400\\\">Zuse Institut Berlin</span></a><span style=\\\"font-weight: 400\\\">
        (ZIB) den Status von Open Access in der Region Berlin-Brandenburg zu diskutieren.
        Unter dem Motto \u201E</span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\"><span
        style=\\\"font-weight: 400\\\">Was wurde erreicht und wo geht es hin?</span></a><span
        style=\\\"font-weight: 400\\\">\u201D sa\xDFen insgesamt sechs Vertreter*innen
        der Berliner und Brandenburger Open-Access-Community auf dem Podium.</span></p>\\n<figure
        id=\\\"attachment_2093\\\" aria-describedby=\\\"caption-attachment-2093\\\"
        style=\\\"width: 840px\\\" class=\\\"wp-caption alignnone\\\"><img loading=\\\"lazy\\\"
        class=\\\"wp-image-2093 size-wcfixedheight\\\" src=\\\"https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-963x500.jpg\\\"
        alt=\\\"\\\" width=\\\"840\\\" height=\\\"436\\\" srcset=\\\"https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-963x500.jpg
        963w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-300x156.jpg
        300w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-1024x531.jpg
        1024w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-768x399.jpg
        768w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-1536x797.jpg
        1536w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-2048x1063.jpg
        2048w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-1200x623.jpg
        1200w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-250x130.jpg
        250w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-550x285.jpg
        550w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-800x415.jpg
        800w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-347x180.jpg
        347w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-578x300.jpg
        578w\\\" sizes=\\\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width:
        1362px) 62vw, 840px\\\" /><figcaption id=\\\"caption-attachment-2093\\\" class=\\\"wp-caption-text\\\">Foto:
        Georg Fischer unter <a href=\\\"https://creativecommons.org/licenses/by/4.0/legalcode\\\">CC
        BY 4.0</a></figcaption></figure>\\n<p><!--more--></p>\\n<p><span style=\\\"font-weight:
        400\\\">Von links nach rechts auf dem Podium:</span></p>\\n<ul>\\n<li><a href=\\\"https://www.bbaw.de/die-akademie/bbaw-mitglieder/mitglied-martin-groetschel\\\"><span
        style=\\\"font-weight: 400\\\">Martin Gr\xF6tschel</span></a><span style=\\\"font-weight:
        400\\\"> (eh. Berlin-Brandenburgische Akademie der Wissenschaften)</span></li>\\n<li><a
        href=\\\"https://www.smb.museum/museen-einrichtungen/institut-fuer-museumsforschung/ueber-uns/mitarbeiterinnen/detail/kathrin-grotz/\\\"><span
        style=\\\"font-weight: 400\\\">Kathrin Grotz</span></a><span style=\\\"font-weight:
        400\\\"> (Staatliche Museen Berlin, SPK)</span></li>\\n<li><a href=\\\"https://www.tu.berlin/ub/ueber-uns/kontakt/ansprechpartnerinnen\\\"><span
        style=\\\"font-weight: 400\\\">J\xFCrgen Christof</span></a><span style=\\\"font-weight:
        400\\\">, (Universit\xE4tsbibliothek Technische Universit\xE4t Berlin)</span></li>\\n<li><a
        href=\\\"https://www.udk-berlin.de/person/ariane-jessulat/\\\"><span style=\\\"font-weight:
        400\\\">Ariane Je\xDFulat</span></a><span style=\\\"font-weight: 400\\\">
        (Universit\xE4t der K\xFCnste Berlin)</span></li>\\n<li><a href=\\\"https://www.uni-potsdam.de/de/cio/index\\\"><span
        style=\\\"font-weight: 400\\\">Peter Kost\xE4dt</span></a><span style=\\\"font-weight:
        400\\\"> (Universit\xE4t Potsdam)</span></li>\\n<li><a href=\\\"https://www.ibi.hu-berlin.de/de/institut/personen/pampel\\\"><span
        style=\\\"font-weight: 400\\\">Heinz Pampel</span></a><span style=\\\"font-weight:
        400\\\"> (Humboldt-Universit\xE4t zu Berlin, Helmholtz Open Science Office)</span></li>\\n</ul>\\n<p><span
        style=\\\"font-weight: 400\\\">Ganz rechts am Pult: </span><a href=\\\"http://www.open-access-berlin.de/\\\"><span
        style=\\\"font-weight: 400\\\">Maxi Kindling</span></a><span style=\\\"font-weight:
        400\\\"> (Open-Access-B\xFCro Berlin)</span></p>\\n<p><span style=\\\"font-weight:
        400\\\">Ausf\xFChrliche Informationen zu den Diskutant*innen und ihren fachlichen
        Hintergr\xFCnden finden sich </span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\"><span
        style=\\\"font-weight: 400\\\">hier</span></a><span style=\\\"font-weight:
        400\\\">.</span></p>\\n<pre>Zitiervorschlag: Fischer, G. (2023). 20 Jahre
        Berliner Erkl\xE4rung f\xFCr Open Access: Wie ist die Lage in Berlin und Brandenburg?,
        Open Access Blog Berlin. <a href=\\\"https://doi.org/10.59350/yy5kk-3tz25\\\">https://doi.org/10.59350/yy5kk-3tz25</a>.</pre>\\n<h1><span
        style=\\\"font-weight: 400\\\">20 Jahre Berliner Erkl\xE4rung</span></h1>\\n<p><span
        style=\\\"font-weight: 400\\\">Nach einer kurzen Begr\xFC\xDFung durch die
        Leiterin des OABB Maxi Kindling f\xFChrte </span><a href=\\\"https://www.fu-berlin.de/sites/ub/ueber-uns/team/brandtner/index.html\\\"><span
        style=\\\"font-weight: 400\\\">Andreas Brandtner</span></a><span style=\\\"font-weight:
        400\\\">, Direktor der Universit\xE4tsbibliothek der Freien Universit\xE4t
        Berlin und Ko-Leiter der </span><a href=\\\"http://www.open-access-berlin.de/aktivitaeten/index.html\\\"><span
        style=\\\"font-weight: 400\\\">Arbeitsgruppe Open-Access-Strategie Berlin</span></a><span
        style=\\\"font-weight: 400\\\">, als Moderator durch die Diskussion. Heimlicher
        Stargast, so der allgemeine Tenor des Nachmittags, war die</span><i><span
        style=\\\"font-weight: 400\\\"> Berliner Erkl\xE4rung \xFCber offenen Zugang
        zu wissenschaftlichem Wissen </span></i><span style=\\\"font-weight: 400\\\">aus
        dem Jahr 2003, </span><a href=\\\"https://openaccess.mpg.de/68053/Berliner_Erklaerung_dt_Version_07-2006.pdf\\\"><span
        style=\\\"font-weight: 400\\\">deren Erscheinen sich fast auf den Tag genau
        zum 20. Mal j\xE4hrte</span></a><span style=\\\"font-weight: 400\\\">. Ein
        guter Zeitpunkt, um zur\xFCckzublicken, kritisch den Status Quo zu evaluieren
        und Akzente f\xFCr die zuk\xFCnftige Entwicklung zu setzen. Die vielf\xE4ltigen
        Aspekte der Diskussion in G\xE4nze darzustellen ist schwerlich umzusetzen,
        daher beschr\xE4nkt sich der Veranstaltungsbericht auf die gro\xDFen Linien
        der Diskussion.</span></p>\\n<h1><span style=\\\"font-weight: 400\\\">Open-Research-Praktiken
        in Wissenschaft und Kulturerbe\_</span></h1>\\n<p><span style=\\\"font-weight:
        400\\\">Schnell wurde klar: Die Open-Access-Transformation ist noch lange
        nicht abgeschlossen, im Gegenteil: Sie ist ein Prozess mit offenem Ende und
        \u2013 obgleich schon viel erreicht wurde \u2013 weiterhin Herausforderung
        und Chance zugleich f\xFCr die Wissenschaft in der Region Berlin-Brandenburg
        wie auch weltweit.\_</span></p>\\n<p><span style=\\\"font-weight: 400\\\">Die
        Forderung, Open Access zu publizieren, ist mittlerweile selbst zum Standard
        geworden und auch die weitere \xD6ffnung des Forschungsprozesses im Sinne
        von Open Research l\xE4sst sich erfreulicherweise vielerorts beobachten. In
        diesem Sinne betonten sowohl Kathrin Grotz als auch Ariane Je\xDFulat diverse
        Vorz\xFCge, die frei verf\xFCgbare und offen lizenzierte Texte, Daten und
        andere Materialien f\xFCr Wissenschaft und genauso f\xFCr die Kunst und die
        k\xFCnstlerische Forschung br\xE4chten: So einfach wie heute sei es noch nie
        gewesen, auf digitalem Wege an Quellen zu kommen. Der digitale Fernzugriff
        habe den Forschenden wie auch der interessierten \xD6ffentlichkeit in den
        letzten 20 Jahren enorme Ersparnisse an Zeit und Ressourcen gebracht, so Je\xDFulat.
        Gleichzeitig erinnerte die UdK-Professorin aber auch an die in der </span><i><span
        style=\\\"font-weight: 400\\\">Berliner Erkl\xE4rung</span></i><span style=\\\"font-weight:
        400\\\"> angelegte Trennung zwischen wissenschaftlichen Forschungs- und k\xFCnstlerischen
        Kulturdaten. Eine solche Trennung halte sie inzwischen f\xFCr \xFCberholt
        \u2013 tats\xE4chlich sei ein hybrider Datenraum zwischen Wissenschaft und
        Kulturerbe entstanden, der f\xFCr Offene Forschung (Open Research) von herausragender
        Bedeutung sei.</span></p>\\n<p><span style=\\\"font-weight: 400\\\">Indirekt
        best\xE4tigte Kathrin Grotz den Befund ihrer Kollegin, als sie die zahlreichen
        Praktiken erl\xE4uterte, mit denen moderne Museen heutzutage Daten verarbeiten,
        vernetzen und auswerten w\xFCrden. Und sie wies darauf hin, dass aufgrund
        der technologischen Entwicklungen weitere wichtige Anwendungen entst\xFCnden,
        die sich heute noch gar nicht oder nur in Umrissen erkennen lie\xDFen. Man
        m\xFCsse daher strategisch und in langfristigen Zeithorizonten denken und
        \u2013 so paradox es zun\xE4chst klingen mag \u2013 dem Unvorhergesehenen
        Raum lassen. Manchmal, so unterstrich die Stellvertretende Direktorin des
        Instituts f\xFCr Museumsforschung, w\xFCrden bestimmte Daten erst in 20 Jahren
        wertvoll werden.</span></p>\\n<h1><span style=\\\"font-weight: 400\\\">Open
        Access zwischen Community und Kommerz</span></h1>\\n<p><span style=\\\"font-weight:
        400\\\">Neben der Einheit von Wissenschaft und Kunst hinsichtlich Open Research
        zog sich das Spannungsfeld von \u201ECommunity und Kommerzialisierung\u201D,
        angelehnt an das diesj\xE4hrige Motto der </span><a href=\\\"https://www.openaccessweek.org/\\\"><span
        style=\\\"font-weight: 400\\\">internationalen Open-Access-Week</span></a><span
        style=\\\"font-weight: 400\\\">, wie ein roter Faden durch die Diskussion.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Einerseits war man sich auf dem Panel durchaus
        einig in der Frage, dass die Open-Access-Community ihre vielf\xE4ltigen Kooperations-
        und Vernetzungsarbeiten weiterhin so engagiert wie bisher f\xFChren und weiter
        ausbauen sollte, gerade in Zeiten allgemein schrumpfender Etats. Das sei neben
        ad\xE4quater politischer F\xF6rderstrukturen ein Schl\xFCssel zum Erfolg,
        wie Heinz Pampel betonte. Andererseits zeigte man sich auf dem Podium auch
        etwas zerknirscht, als die Sprache auf die </span><a href=\\\"https://www.forschung-und-lehre.de/forschung/open-acess-vereinbarung-fuer-deutschsprachige-forschung-5889\\\"><span
        style=\\\"font-weight: 400\\\">k\xFCrzlich abgeschlossenen Verhandlungen zum
        DEAL-Vertrag</span></a><span style=\\\"font-weight: 400\\\"> zwischen der
        Allianz der deutschen Wissenschaftsorganisationen und dem international agierenden
        Gro\xDFverlag Elsevier kam. J\xFCrgen Christof etwa merkte sehr deutlich an,
        dass die Finanzstr\xF6me, die das wissenschaftliche Publizieren sicherten,
        den Forschenden oftmals verborgen blieben. Das aber \xE4ndere nichts an den
        erheblichen Betr\xE4gen, die vorrangig an die Gro\xDFverlage flie\xDFen und
        die sich so in den letzten Jahrzehnten eine ebenfalls erhebliche Marktmacht
        aufbauen konnten \u2013 nicht trotz, sondern gerade aufgrund von Open Access
        und den damit von den Verlagen aufgerufenen Geb\xFChren.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Dass die DEAL-Vertr\xE4ge mit Springer Nature,
        Wiley und nun auch Elsevier eigentlich als Transformationsvertr\xE4ge gedacht
        waren, also das Flipping der Zeitschriften zum Ziel hatten, kann man gerade
        in der 20-j\xE4hrigen R\xFCckschau nach der </span><i><span style=\\\"font-weight:
        400\\\">Berliner Erkl\xE4rung</span></i><span style=\\\"font-weight: 400\\\">
        getrost als Flop bezeichnen. </span><span style=\\\"font-weight: 400\\\">Dazu
        kommt das \u2013 innerhalb der Diskussion nur am Rande besprochene \u2013
        digitale Tracking von Forschenden und ihrem Nutzungsverhalten. Mithilfe der
        Aggregation und Analyse von Nutzungsdaten konnten sich wissenschaftliche Gro\xDFverlage
        wie Elsevier in den vergangenen Jahren u.a. dank enormer Open-Access-Geb\xFChren
        ein neues, durchaus lukratives Gesch\xE4ftsmodell aufbauen. Mittlerweile wird
        es von vielen Stellen als </span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/tag/datentracking/\\\"><span
        style=\\\"font-weight: 400\\\">Gefahr f\xFCr die freie Wissenschaft</span></a><span
        style=\\\"font-weight: 400\\\"> gesehen.</span></p>\\n<h1><span style=\\\"font-weight:
        400\\\">Informiert publizieren, juristisch beraten, nachhaltig f\xF6rdern</span></h1>\\n<p><a
        href=\\\"https://www.hiig.de/publication/wissenschaftsgeleitetes-publizieren/\\\"><span
        style=\\\"font-weight: 400\\\">Wissenschaftsgeleitete Publikationsformate</span></a><span
        style=\\\"font-weight: 400\\\"> und </span><a href=\\\"https://en.wikipedia.org/wiki/Diamond_open_access\\\"><span
        style=\\\"font-weight: 400\\\">Diamond Open Access</span></a><span style=\\\"font-weight:
        400\\\"> seien darauf die richtige Antwort und auch auf dem Vormarsch, diagnostizierte
        Peter Kost\xE4dt unter zustimmendem allgemeinem Nicken auf dem Podium. Gleichzeitig
        f\xFChre das zum n\xE4chsten Problem, n\xE4mlich den jahrzehntelang gewachsenen
        Reputationsstrukturen, die in der Transformation sichtbar w\xFCrden. Hier
        seien innere Widerst\xE4nde seitens der Wissenschaft oder \u2013 positiv ausgedr\xFCckt
        \u2013 nicht ausreichend Anreize vorhanden, die Forschende dazu bringen w\xFCrden,
        von den Gro\xDFverlagen und ihren etablierten Zeitschriften Abstand zu nehmen.
        Publikationsentscheidungen, so Pampel, sollten \u201Einformierte Entscheidungen\u201D
        der Forschenden sein und die Bibliotheken h\xE4tten die verantwortungsvolle
        Aufgabe, hierbei zu beraten und zu unterst\xFCtzen.</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Informiert m\xFCssten wissenschaftlich und
        k\xFCnstlerisch Forschende aber nicht nur in ihrem eigenen Fachgebiet und
        den dortigen Publikationsstrukturen sein. Kathrin Grotz zufolge dr\xE4ngten
        sich den Forschenden im Alltag auch vielf\xE4ltige juristische und ethische
        Fragen auf. Solche Fragen h\xE4tten sich im Zuge der Digitalisierung erheblich
        vermehrt und auch in ihrer Komplexit\xE4t verst\xE4rkt, wie sie am Beispiel
        der Forschungs- und Kulturdaten untermauerte. Ariane Je\xDFulat hob die Rechtsberatung
        durch den </span><a href=\\\"https://nfdi4culture.de/de/helpdesk.html\\\"><span
        style=\\\"font-weight: 400\\\">Legal Helpdesk der Initiative NFDI4Culture</span></a><span
        style=\\\"font-weight: 400\\\"> als wegweisendes Beispiel hervor. Auch pl\xE4dierte
        sie f\xFCr die vielf\xE4ltigen Potentiale, die im Management von Forschungs-
        und Kulturdaten steckten, die sich ohne juristische Beratung aber nur unzureichend
        heben lie\xDFen. Und schob hinterher, dass gerade bei Sammlungen und Kulturdaten
        die langfristige Perspektive von gro\xDFer Bedeutung sei.</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Dem st\xFCnde, wie mehrere Teilnehmer*innen
        auf dem Podium auch in anderen Kontexten klarstellten, in zu vielen F\xE4llen
        die Logik der zeitlich befristeten Projekte entgegen. J\xFCrgen Christof formulierte
        hier deutlich, wie notwendig ein ordentlich ausfinanzierter Regelbetrieb sei,
        \xFCber den sich Infrastrukturen aufbauen und Ideen \xFCber Projektende hinaus
        realisieren lie\xDFen. Die \xDCberf\xFChrung von Drittmittel-finanzierten
        Projekten in einen nachhaltig und institutionell langfristig gesicherten Regelbetrieb
        m\xFCsse daher st\xE4rker politisch gef\xF6rdert werden.</span></p>\\n<h1><span
        style=\\\"font-weight: 400\\\">Das harte Brett der Open-Access-Transformation</span></h1>\\n<p><span
        style=\\\"font-weight: 400\\\">Die Zw\xE4nge, die sich aus F\xF6rderlogik
        und dem f\xF6deralen System nach wie vor ergeben w\xFCrden, monierte gegen
        Ende der Diskussion auch Martin Gr\xF6tschel, der die Open-Access-Transformation
        langj\xE4hrig begleitet und vorangetrieben hatte. Seiner Meinung nach habe
        deswegen die Entwicklung bisher \u201Edoppelt bis dreifach so lange gedauert\u201D
        als urspr\xFCnglich erwartet. Auch betonte er die zahlreichen, teils in verschiedenen
        Geschwindigkeiten laufenden Parallel-Prozesse. J\xFCrgen Christof wiederum
        w\xFCnschte sich eine Halbierung der wissenschaftlichen Publikationen bis
        zum Jahre 2040.\_</span></p>\\n<p><span style=\\\"font-weight: 400\\\">Am
        Ende zeigt sich deutlich, dass Open Access und Open Research keine vereinzelten
        Themen der Wissenschaft sind, sondern an der Transformation eines ganzen Wissenschaftssystems
        h\xE4ngen. Daraus ergeben sich zwei wichtige Perspektiven f\xFCr die Zukunft:
        Die Zusammenarbeit und Kooperation unter verschiedenen Interessengruppen werden
        noch wichtiger werden als bisher. Und gleichzeitig werden der Aufbau und die
        Finanzierung offener Infrastrukturen dar\xFCber entscheiden, ob diese Bem\xFChungen
        auch dauerhaft der Kommerzialisierung von Wissen etwas entgegenhalten k\xF6nnen.
        </span><span style=\\\"font-weight: 400\\\">Denn offene, selbst getragene
        Infrastrukturen k\xF6nnen auch f\xFCr andere Probleme, insbesondere das oft
        kritisierte Datentracking von Verlagen, eine geeignete Antwort bieten, insofern
        sich die Wissenschaft das Publizieren der eigenen Forschung wieder st\xE4rker
        in die eigenen H\xE4nde holen k\xF6nnte.</span></p>\\n<p>&nbsp;</p>\\n<h3><i><span
        style=\\\"font-weight: 400\\\">Weitere Berichte zur Podiumsdiskussion:</span></i></h3>\\n<ul>\\n<li
        style=\\\"font-weight: 400\\\"><i><span style=\\\"font-weight: 400\\\">Vernetzungs-
        und Kompetenzstelle Open Access Brandenburg (VuK): <a href=\\\"https://open-access-brandenburg.de/oa-takeaways-open-access-berlin-brandenburg/\\\">Takeaways
        zur Podiumsdiskussion</a></span></i></li>\\n</ul>\\n\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/yy5kk-3tz25\",\"guid\":\"https://blogs.fu-berlin.de/open-access-berlin/?p=2091\",\"id\":\"7fd600b7-d7e9-4424-9909-80f35d6f1d9a\",\"image\":\"https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-963x500.jpg\",\"language\":\"de\",\"published_at\":1698765480,\"reference\":[],\"relationships\":[],\"summary\":\"Am
        24. Oktober 2023 luden das Institut f\xFCr Bibliotheks- und Informationswissenschaft
        (IBI) der Humboldt-Universit\xE4t zu Berlin (HU), der Kooperative Bibliotheksverbund
        Berlin-Brandenburg (KOBV), das Open-Access-B\xFCro Berlin (OABB) und die Vernetzungs-
        und Kompetenzstelle Open Access Brandenburg (VuK) dazu ein, im Zuse Institut
        Berlin (ZIB) den Status von Open Access in der Region Berlin-Brandenburg zu
        diskutieren.\",\"tags\":[\"Allgemein\"],\"title\":\"20 Jahre Berliner Erkl\xE4rung
        f\xFCr Open Access: Wie ist die Lage in Berlin und Brandenburg?\",\"updated_at\":1698850758,\"url\":\"https://blogs.fu-berlin.de/open-access-berlin/2023/10/31/20-jahre-berliner-erklaerung-open-access-berlin-brandenburg\"},\"highlight\":{\"authors\":[{\"name\":\"Georg
        Fischer\"}],\"content_html\":\"<p><span style=\\\"font-weight: 400\\\">Am
        24. Oktober 2023 luden das </span><a href=\\\"https://www.ibi.hu-berlin.de/de\\\"><span
        style=\\\"font-weight: 400\\\">Institut f\xFCr Bibliotheks- und Informationswissenschaft</span></a><span
        style=\\\"font-weight: 400\\\"> (IBI) der Humboldt-Universit\xE4t zu Berlin
        (HU), der </span><a href=\\\"https://www.kobv.de/\\\"><span style=\\\"font-weight:
        400\\\">Kooperative Bibliotheksverbund Berlin-Brandenburg</span></a><span
        style=\\\"font-weight: 400\\\"> (KOBV), das </span><a href=\\\"http://www.open-access-berlin.de/index.html\\\"><span
        style=\\\"font-weight: 400\\\">Open-Access-B\xFCro Berlin</span></a><span
        style=\\\"font-weight: 400\\\"> (OABB) und die </span><a href=\\\"https://open-access-brandenburg.de/\\\"><span
        style=\\\"font-weight: 400\\\">Vernetzungs- und Kompetenzstelle Open Access
        Brandenburg</span></a><span style=\\\"font-weight: 400\\\"> (VuK) dazu ein,
        im </span><a href=\\\"https://www.zib.de\\\"><span style=\\\"font-weight:
        400\\\">Zuse Institut Berlin</span></a><span style=\\\"font-weight: 400\\\">
        (ZIB) den Status von Open Access in der Region Berlin-Brandenburg zu diskutieren.
        Unter dem Motto \u201E</span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\"><span
        style=\\\"font-weight: 400\\\">Was wurde erreicht und wo geht es hin?</span></a><span
        style=\\\"font-weight: 400\\\">\u201D sa\xDFen insgesamt sechs Vertreter*innen
        der Berliner und Brandenburger Open-Access-Community auf dem Podium.</span></p>\\n<figure
        id=\\\"attachment_2093\\\" aria-describedby=\\\"caption-attachment-2093\\\"
        style=\\\"width: 840px\\\" class=\\\"wp-caption alignnone\\\"><img loading=\\\"lazy\\\"
        class=\\\"wp-image-2093 size-wcfixedheight\\\" src=\\\"https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-963x500.jpg\\\"
        alt=\\\"\\\" width=\\\"840\\\" height=\\\"436\\\" srcset=\\\"https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-963x500.jpg
        963w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-300x156.jpg
        300w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-1024x531.jpg
        1024w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-768x399.jpg
        768w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-1536x797.jpg
        1536w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-2048x1063.jpg
        2048w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-1200x623.jpg
        1200w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-250x130.jpg
        250w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-550x285.jpg
        550w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-800x415.jpg
        800w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-347x180.jpg
        347w, https://blogs.fu-berlin.de/open-access-berlin/files/2023/10/IMG_20231024_1601265362-578x300.jpg
        578w\\\" sizes=\\\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width:
        1362px) 62vw, 840px\\\" /><figcaption id=\\\"caption-attachment-2093\\\" class=\\\"wp-caption-text\\\">Foto:
        Georg Fischer unter <a href=\\\"https://creativecommons.org/licenses/by/4.0/legalcode\\\">CC
        BY 4.0</a></figcaption></figure>\\n<p><!--more--></p>\\n<p><span style=\\\"font-weight:
        400\\\">Von links nach rechts auf dem Podium:</span></p>\\n<ul>\\n<li><a href=\\\"https://www.bbaw.de/die-akademie/bbaw-mitglieder/mitglied-martin-groetschel\\\"><span
        style=\\\"font-weight: 400\\\">Martin Gr\xF6tschel</span></a><span style=\\\"font-weight:
        400\\\"> (eh. Berlin-Brandenburgische Akademie der Wissenschaften)</span></li>\\n<li><a
        href=\\\"https://www.smb.museum/museen-einrichtungen/institut-fuer-museumsforschung/ueber-uns/mitarbeiterinnen/detail/kathrin-grotz/\\\"><span
        style=\\\"font-weight: 400\\\">Kathrin Grotz</span></a><span style=\\\"font-weight:
        400\\\"> (Staatliche Museen Berlin, SPK)</span></li>\\n<li><a href=\\\"https://www.tu.berlin/ub/ueber-uns/kontakt/ansprechpartnerinnen\\\"><span
        style=\\\"font-weight: 400\\\">J\xFCrgen Christof</span></a><span style=\\\"font-weight:
        400\\\">, (Universit\xE4tsbibliothek Technische Universit\xE4t Berlin)</span></li>\\n<li><a
        href=\\\"https://www.udk-berlin.de/person/ariane-jessulat/\\\"><span style=\\\"font-weight:
        400\\\">Ariane Je\xDFulat</span></a><span style=\\\"font-weight: 400\\\">
        (Universit\xE4t der K\xFCnste Berlin)</span></li>\\n<li><a href=\\\"https://www.uni-potsdam.de/de/cio/index\\\"><span
        style=\\\"font-weight: 400\\\">Peter Kost\xE4dt</span></a><span style=\\\"font-weight:
        400\\\"> (Universit\xE4t Potsdam)</span></li>\\n<li><a href=\\\"https://www.ibi.hu-berlin.de/de/institut/personen/pampel\\\"><span
        style=\\\"font-weight: 400\\\">Heinz Pampel</span></a><span style=\\\"font-weight:
        400\\\"> (Humboldt-Universit\xE4t zu Berlin, Helmholtz Open Science Office)</span></li>\\n</ul>\\n<p><span
        style=\\\"font-weight: 400\\\">Ganz rechts am Pult: </span><a href=\\\"http://www.open-access-berlin.de/\\\"><span
        style=\\\"font-weight: 400\\\">Maxi Kindling</span></a><span style=\\\"font-weight:
        400\\\"> (Open-Access-B\xFCro Berlin)</span></p>\\n<p><span style=\\\"font-weight:
        400\\\">Ausf\xFChrliche Informationen zu den Diskutant*innen und ihren fachlichen
        Hintergr\xFCnden finden sich </span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/2023/06/01/oaweekbbb-auftakt/\\\"><span
        style=\\\"font-weight: 400\\\">hier</span></a><span style=\\\"font-weight:
        400\\\">.</span></p>\\n<pre>Zitiervorschlag: Fischer, G. (2023). 20 Jahre
        Berliner Erkl\xE4rung f\xFCr Open Access: Wie ist die Lage in Berlin und Brandenburg?,
        Open Access Blog Berlin. <a href=\\\"https://doi.org/10.59350/yy5kk-3tz25\\\">https://doi.org/10.59350/yy5kk-3tz25</a>.</pre>\\n<h1><span
        style=\\\"font-weight: 400\\\">20 Jahre Berliner Erkl\xE4rung</span></h1>\\n<p><span
        style=\\\"font-weight: 400\\\">Nach einer kurzen Begr\xFC\xDFung durch die
        Leiterin des OABB Maxi Kindling f\xFChrte </span><a href=\\\"https://www.fu-berlin.de/sites/ub/ueber-uns/team/brandtner/index.html\\\"><span
        style=\\\"font-weight: 400\\\">Andreas Brandtner</span></a><span style=\\\"font-weight:
        400\\\">, Direktor der Universit\xE4tsbibliothek der Freien Universit\xE4t
        Berlin und Ko-Leiter der </span><a href=\\\"http://www.open-access-berlin.de/aktivitaeten/index.html\\\"><span
        style=\\\"font-weight: 400\\\">Arbeitsgruppe Open-Access-Strategie Berlin</span></a><span
        style=\\\"font-weight: 400\\\">, als Moderator durch die Diskussion. Heimlicher
        Stargast, so der allgemeine Tenor des Nachmittags, war die</span><i><span
        style=\\\"font-weight: 400\\\"> Berliner Erkl\xE4rung \xFCber offenen Zugang
        zu wissenschaftlichem Wissen </span></i><span style=\\\"font-weight: 400\\\">aus
        dem Jahr 2003, </span><a href=\\\"https://openaccess.mpg.de/68053/Berliner_Erklaerung_dt_Version_07-2006.pdf\\\"><span
        style=\\\"font-weight: 400\\\">deren Erscheinen sich fast auf den Tag genau
        zum 20. Mal j\xE4hrte</span></a><span style=\\\"font-weight: 400\\\">. Ein
        guter Zeitpunkt, um zur\xFCckzublicken, kritisch den Status Quo zu evaluieren
        und Akzente f\xFCr die zuk\xFCnftige Entwicklung zu setzen. Die vielf\xE4ltigen
        Aspekte der Diskussion in G\xE4nze darzustellen ist schwerlich umzusetzen,
        daher beschr\xE4nkt sich der Veranstaltungsbericht auf die gro\xDFen Linien
        der Diskussion.</span></p>\\n<h1><span style=\\\"font-weight: 400\\\">Open-Research-Praktiken
        in Wissenschaft und Kulturerbe\_</span></h1>\\n<p><span style=\\\"font-weight:
        400\\\">Schnell wurde klar: Die Open-Access-Transformation ist noch lange
        nicht abgeschlossen, im Gegenteil: Sie ist ein Prozess mit offenem Ende und
        \u2013 obgleich schon viel erreicht wurde \u2013 weiterhin Herausforderung
        und Chance zugleich f\xFCr die Wissenschaft in der Region Berlin-Brandenburg
        wie auch weltweit.\_</span></p>\\n<p><span style=\\\"font-weight: 400\\\">Die
        Forderung, Open Access zu publizieren, ist mittlerweile selbst zum Standard
        geworden und auch die weitere \xD6ffnung des Forschungsprozesses im Sinne
        von Open Research l\xE4sst sich erfreulicherweise vielerorts beobachten. In
        diesem Sinne betonten sowohl Kathrin Grotz als auch Ariane Je\xDFulat diverse
        Vorz\xFCge, die frei verf\xFCgbare und offen lizenzierte Texte, Daten und
        andere Materialien f\xFCr Wissenschaft und genauso f\xFCr die Kunst und die
        k\xFCnstlerische Forschung br\xE4chten: So einfach wie heute sei es noch nie
        gewesen, auf digitalem Wege an Quellen zu kommen. Der digitale Fernzugriff
        habe den Forschenden wie auch der interessierten \xD6ffentlichkeit in den
        letzten 20 Jahren enorme Ersparnisse an Zeit und Ressourcen gebracht, so Je\xDFulat.
        Gleichzeitig erinnerte die UdK-Professorin aber auch an die in der </span><i><span
        style=\\\"font-weight: 400\\\">Berliner Erkl\xE4rung</span></i><span style=\\\"font-weight:
        400\\\"> angelegte Trennung zwischen wissenschaftlichen Forschungs- und k\xFCnstlerischen
        Kulturdaten. Eine solche Trennung halte sie inzwischen f\xFCr \xFCberholt
        \u2013 tats\xE4chlich sei ein hybrider Datenraum zwischen Wissenschaft und
        Kulturerbe entstanden, der f\xFCr Offene Forschung (Open Research) von herausragender
        Bedeutung sei.</span></p>\\n<p><span style=\\\"font-weight: 400\\\">Indirekt
        best\xE4tigte Kathrin Grotz den Befund ihrer Kollegin, als sie die zahlreichen
        Praktiken erl\xE4uterte, mit denen moderne Museen heutzutage Daten verarbeiten,
        vernetzen und auswerten w\xFCrden. Und sie wies darauf hin, dass aufgrund
        der technologischen Entwicklungen weitere wichtige Anwendungen entst\xFCnden,
        die sich heute noch gar nicht oder nur in Umrissen erkennen lie\xDFen. Man
        m\xFCsse daher strategisch und in langfristigen Zeithorizonten denken und
        \u2013 so paradox es zun\xE4chst klingen mag \u2013 dem Unvorhergesehenen
        Raum lassen. Manchmal, so unterstrich die Stellvertretende Direktorin des
        Instituts f\xFCr Museumsforschung, w\xFCrden bestimmte Daten erst in 20 Jahren
        wertvoll werden.</span></p>\\n<h1><span style=\\\"font-weight: 400\\\">Open
        Access zwischen Community und Kommerz</span></h1>\\n<p><span style=\\\"font-weight:
        400\\\">Neben der Einheit von Wissenschaft und Kunst hinsichtlich Open Research
        zog sich das Spannungsfeld von \u201ECommunity und Kommerzialisierung\u201D,
        angelehnt an das diesj\xE4hrige Motto der </span><a href=\\\"https://www.openaccessweek.org/\\\"><span
        style=\\\"font-weight: 400\\\">internationalen Open-Access-Week</span></a><span
        style=\\\"font-weight: 400\\\">, wie ein roter Faden durch die Diskussion.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Einerseits war man sich auf dem Panel durchaus
        einig in der Frage, dass die Open-Access-Community ihre vielf\xE4ltigen Kooperations-
        und Vernetzungsarbeiten weiterhin so engagiert wie bisher f\xFChren und weiter
        ausbauen sollte, gerade in Zeiten allgemein schrumpfender Etats. Das sei neben
        ad\xE4quater politischer F\xF6rderstrukturen ein Schl\xFCssel zum Erfolg,
        wie Heinz Pampel betonte. Andererseits zeigte man sich auf dem Podium auch
        etwas zerknirscht, als die Sprache auf die </span><a href=\\\"https://www.forschung-und-lehre.de/forschung/open-acess-vereinbarung-fuer-deutschsprachige-forschung-5889\\\"><span
        style=\\\"font-weight: 400\\\">k\xFCrzlich abgeschlossenen Verhandlungen zum
        DEAL-Vertrag</span></a><span style=\\\"font-weight: 400\\\"> zwischen der
        Allianz der deutschen Wissenschaftsorganisationen und dem international agierenden
        Gro\xDFverlag Elsevier kam. J\xFCrgen Christof etwa merkte sehr deutlich an,
        dass die Finanzstr\xF6me, die das wissenschaftliche Publizieren sicherten,
        den Forschenden oftmals verborgen blieben. Das aber \xE4ndere nichts an den
        erheblichen Betr\xE4gen, die vorrangig an die Gro\xDFverlage flie\xDFen und
        die sich so in den letzten Jahrzehnten eine ebenfalls erhebliche Marktmacht
        aufbauen konnten \u2013 nicht trotz, sondern gerade aufgrund von Open Access
        und den damit von den Verlagen aufgerufenen Geb\xFChren.\_</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Dass die DEAL-Vertr\xE4ge mit Springer Nature,
        Wiley und nun auch Elsevier eigentlich als Transformationsvertr\xE4ge gedacht
        waren, also das Flipping der Zeitschriften zum Ziel hatten, kann man gerade
        in der 20-j\xE4hrigen R\xFCckschau nach der </span><i><span style=\\\"font-weight:
        400\\\">Berliner Erkl\xE4rung</span></i><span style=\\\"font-weight: 400\\\">
        getrost als Flop bezeichnen. </span><span style=\\\"font-weight: 400\\\">Dazu
        kommt das \u2013 innerhalb der Diskussion nur am Rande besprochene \u2013
        digitale Tracking von Forschenden und ihrem Nutzungsverhalten. Mithilfe der
        Aggregation und Analyse von Nutzungsdaten konnten sich wissenschaftliche Gro\xDFverlage
        wie Elsevier in den vergangenen Jahren u.a. dank enormer Open-Access-Geb\xFChren
        ein neues, durchaus lukratives Gesch\xE4ftsmodell aufbauen. Mittlerweile wird
        es von vielen Stellen als </span><a href=\\\"https://blogs.fu-berlin.de/open-access-berlin/tag/datentracking/\\\"><span
        style=\\\"font-weight: 400\\\">Gefahr f\xFCr die freie Wissenschaft</span></a><span
        style=\\\"font-weight: 400\\\"> gesehen.</span></p>\\n<h1><span style=\\\"font-weight:
        400\\\">Informiert publizieren, juristisch beraten, nachhaltig f\xF6rdern</span></h1>\\n<p><a
        href=\\\"https://www.hiig.de/publication/wissenschaftsgeleitetes-publizieren/\\\"><span
        style=\\\"font-weight: 400\\\">Wissenschaftsgeleitete Publikationsformate</span></a><span
        style=\\\"font-weight: 400\\\"> und </span><a href=\\\"https://en.wikipedia.org/wiki/Diamond_open_access\\\"><span
        style=\\\"font-weight: 400\\\">Diamond Open Access</span></a><span style=\\\"font-weight:
        400\\\"> seien darauf die richtige Antwort und auch auf dem Vormarsch, diagnostizierte
        Peter Kost\xE4dt unter zustimmendem allgemeinem Nicken auf dem Podium. Gleichzeitig
        f\xFChre das zum n\xE4chsten Problem, n\xE4mlich den jahrzehntelang gewachsenen
        Reputationsstrukturen, die in der Transformation sichtbar w\xFCrden. Hier
        seien innere Widerst\xE4nde seitens der Wissenschaft oder \u2013 positiv ausgedr\xFCckt
        \u2013 nicht ausreichend Anreize vorhanden, die Forschende dazu bringen w\xFCrden,
        von den Gro\xDFverlagen und ihren etablierten Zeitschriften Abstand zu nehmen.
        Publikationsentscheidungen, so Pampel, sollten \u201Einformierte Entscheidungen\u201D
        der Forschenden sein und die Bibliotheken h\xE4tten die verantwortungsvolle
        Aufgabe, hierbei zu beraten und zu unterst\xFCtzen.</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Informiert m\xFCssten wissenschaftlich und
        k\xFCnstlerisch Forschende aber nicht nur in ihrem eigenen Fachgebiet und
        den dortigen Publikationsstrukturen sein. Kathrin Grotz zufolge dr\xE4ngten
        sich den Forschenden im Alltag auch vielf\xE4ltige juristische und ethische
        Fragen auf. Solche Fragen h\xE4tten sich im Zuge der Digitalisierung erheblich
        vermehrt und auch in ihrer Komplexit\xE4t verst\xE4rkt, wie sie am Beispiel
        der Forschungs- und Kulturdaten untermauerte. Ariane Je\xDFulat hob die Rechtsberatung
        durch den </span><a href=\\\"https://nfdi4culture.de/de/helpdesk.html\\\"><span
        style=\\\"font-weight: 400\\\">Legal Helpdesk der Initiative NFDI4Culture</span></a><span
        style=\\\"font-weight: 400\\\"> als wegweisendes Beispiel hervor. Auch pl\xE4dierte
        sie f\xFCr die vielf\xE4ltigen Potentiale, die im Management von Forschungs-
        und Kulturdaten steckten, die sich ohne juristische Beratung aber nur unzureichend
        heben lie\xDFen. Und schob hinterher, dass gerade bei Sammlungen und Kulturdaten
        die langfristige Perspektive von gro\xDFer Bedeutung sei.</span></p>\\n<p><span
        style=\\\"font-weight: 400\\\">Dem st\xFCnde, wie mehrere Teilnehmer*innen
        auf dem Podium auch in anderen Kontexten klarstellten, in zu vielen F\xE4llen
        die Logik der zeitlich befristeten Projekte entgegen. J\xFCrgen Christof formulierte
        hier deutlich, wie notwendig ein ordentlich ausfinanzierter Regelbetrieb sei,
        \xFCber den sich Infrastrukturen aufbauen und Ideen \xFCber Projektende hinaus
        realisieren lie\xDFen. Die \xDCberf\xFChrung von Drittmittel-finanzierten
        Projekten in einen nachhaltig und institutionell langfristig gesicherten Regelbetrieb
        m\xFCsse daher st\xE4rker politisch gef\xF6rdert werden.</span></p>\\n<h1><span
        style=\\\"font-weight: 400\\\">Das harte Brett der Open-Access-Transformation</span></h1>\\n<p><span
        style=\\\"font-weight: 400\\\">Die Zw\xE4nge, die sich aus F\xF6rderlogik
        und dem f\xF6deralen System nach wie vor ergeben w\xFCrden, monierte gegen
        Ende der Diskussion auch Martin Gr\xF6tschel, der die Open-Access-Transformation
        langj\xE4hrig begleitet und vorangetrieben hatte. Seiner Meinung nach habe
        deswegen die Entwicklung bisher \u201Edoppelt bis dreifach so lange gedauert\u201D
        als urspr\xFCnglich erwartet. Auch betonte er die zahlreichen, teils in verschiedenen
        Geschwindigkeiten laufenden Parallel-Prozesse. J\xFCrgen Christof wiederum
        w\xFCnschte sich eine Halbierung der wissenschaftlichen Publikationen bis
        zum Jahre 2040.\_</span></p>\\n<p><span style=\\\"font-weight: 400\\\">Am
        Ende zeigt sich deutlich, dass Open Access und Open Research keine vereinzelten
        Themen der Wissenschaft sind, sondern an der Transformation eines ganzen Wissenschaftssystems
        h\xE4ngen. Daraus ergeben sich zwei wichtige Perspektiven f\xFCr die Zukunft:
        Die Zusammenarbeit und Kooperation unter verschiedenen Interessengruppen werden
        noch wichtiger werden als bisher. Und gleichzeitig werden der Aufbau und die
        Finanzierung offener Infrastrukturen dar\xFCber entscheiden, ob diese Bem\xFChungen
        auch dauerhaft der Kommerzialisierung von Wissen etwas entgegenhalten k\xF6nnen.
        </span><span style=\\\"font-weight: 400\\\">Denn offene, selbst getragene
        Infrastrukturen k\xF6nnen auch f\xFCr andere Probleme, insbesondere das oft
        kritisierte Datentracking von Verlagen, eine geeignete Antwort bieten, insofern
        sich die Wissenschaft das Publizieren der eigenen Forschung wieder st\xE4rker
        in die eigenen H\xE4nde holen k\xF6nnte.</span></p>\\n<p>&nbsp;</p>\\n<h3><i><span
        style=\\\"font-weight: 400\\\">Weitere Berichte zur Podiumsdiskussion:</span></i></h3>\\n<ul>\\n<li
        style=\\\"font-weight: 400\\\"><i><span style=\\\"font-weight: 400\\\">Vernetzungs-
        und Kompetenzstelle Open Access Brandenburg (VuK): <a href=\\\"https://open-access-brandenburg.de/oa-takeaways-open-access-berlin-brandenburg/\\\">Takeaways
        zur Podiumsdiskussion</a></span></i></li>\\n</ul>\\n\",\"doi\":\"https://doi.org/10.59350/yy5kk-3tz25\",\"reference\":[],\"summary\":\"Am
        24. Oktober 2023 luden das Institut f\xFCr Bibliotheks- und Informationswissenschaft
        (IBI) der Humboldt-Universit\xE4t zu Berlin (HU), der Kooperative Bibliotheksverbund
        Berlin-Brandenburg (KOBV), das Open-Access-B\xFCro Berlin (OABB) und die Vernetzungs-
        und Kompetenzstelle Open Access Brandenburg (VuK) dazu ein, im Zuse Institut
        Berlin (ZIB) den Status von Open Access in der Region Berlin-Brandenburg zu
        diskutieren.\",\"tags\":[\"Allgemein\"],\"title\":\"20 Jahre Berliner Erkl\xE4rung
        f\xFCr Open Access: Wie ist die Lage in Berlin und Brandenburg?\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Sven
        Lieber, PhD\"}],\"blog_id\":\"6wzp770\",\"blog_name\":\"FAIR Data Digest\",\"blog_slug\":\"fairdata\",\"content_html\":\"<p>Hello
        everyone,</p><p>welcome to today\u2019s Halloween edition about <em>data anti-patterns</em>
        that hopefully will scare you so much you don\u2019t do them :-) Things you
        better avoid when aiming for FAIR data.</p><p>Anti patterns? In short they
        are the <strong>opposite of best practices</strong>. Common or obvious solutions
        that are not only <em>ineffective </em>but also <em>risky</em>. Similar to
        biases, it is good to know about them so you can try avoiding them. I mainly
        know the term from software engineering, but apparently it is at least also
        known for project management and business processes (<a href=\\\"https://www.wikidata.org/wiki/Q76438\\\">Q76438</a>,
        <a href=\\\"https://en.wikipedia.org/wiki/Anti-pattern\\\">Wikipedia</a>).
        </p><p>In the following I will list some anti patterns I have seen \u201Cin
        the wild\u201D that directly relate to the FAIR principles, their evil twins
        if you like \U0001F9DB </p><div class=\\\"captioned-image-container\\\"><figure><a
        class=\\\"image-link is-viewable-img image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg\\\"
        width=\\\"1280\\\" height=\\\"960\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1456w\\\" sizes=\\\"100vw\\\" fetchpriority=\\\"high\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">daryl_mitchell
        from Saskatoon, Saskatchewan, Canada, CC BY-SA 2.0, via Wikimedia Commons</figcaption></figure></div><div><hr></div><h2>Findability:
        the needle in the haystack</h2><p>Findability is probably <em>the </em>most
        important principle to get started: if you don\u2019t know that something
        exists or you know it but you don\u2019t know where it is, then you simply
        cannot use it.</p><p>Providing unique and <strong>persistent identifiers</strong>
        for things is the common solution to deal with findability. As easy as it
        sounds, there are many possible pitfalls, some obvious and some more tricky.
        </p><ul><li><p>\U0001F631 reusing identifiers </p><p>\u27A1\uFE0F Recycling
        is good, but reusing identifiers is bad: identifiers should be <strong>unique</strong>!</p></li><li><p>\U0001F631
        implementation details as part of the identifier </p><p>\u27A1\uFE0F If you
        change your software or provider the identifier will change, hence \u201Cidentifiers\u201D
        such as <code>http://my-url.com/items?query=abc</code> are bad.</p></li><li><p>\U0001F631
        thinking that persistent identifiers are only a technical problem for the
        ICT department </p><p>\u27A1\uFE0F Providing and maintaining the technical
        infrastructure is important, but knowing the data, use cases, stakeholders
        and community best practices is just as important</p></li></ul><p>From a very
        practical perspective, searching nowadays often happen via Google. Did you
        ever try to google a persistent identifier? Often you won\u2019t find much.
        The following paper investigates how FAIR persistent identifiers actually
        are and which role validity can play (DOI: <a href=\\\"https://doi.org/10.3233/DS-190024\\\">10.3233/DS-190024</a>).</p><p>By
        the way, there are different ways to implement persistent identifiers. You
        can check out the Persistent Identifier Guide of the Dutch Digital Heritage
        Network to know which system works best for you. Just answer 25 questions.</p><p
        class=\\\"button-wrapper\\\" data-attrs=\\\"{&quot;url&quot;:&quot;https://www.pidwijzer.nl/en/&quot;,&quot;text&quot;:&quot;Read
        the Persistent Identifier Guide&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\\\"
        data-component-name=\\\"ButtonCreateButton\\\"><a class=\\\"button primary\\\"
        href=\\\"https://www.pidwijzer.nl/en/\\\"><span>Read the Persistent Identifier
        Guide</span></a></p><div><hr></div><h2>Accessibility: what\u2019s in the box?!</h2><p>To
        stay in the Halloween theme, imagine you\u2019ve <em>found </em>a secret door,
        but you don\u2019t know how to open it. You would be more than happy getting
        some <em>information </em>about how to <em>access </em>the room behind it:
        that would be metadata about accessibility.</p><ul><li><p>\U0001F631 not providing
        any metadata</p><p>\u27A1\uFE0F <em>\u201C<s>Pics</s> metadata or it didn\u2019t
        happen\u201D</em>. Help others to find out more about something (something
        that possibly is no longer available). I have heard that the phone number
        of a research projects\u2019 PI can be the most valuable piece of metadata
        :-) (DOI: <a href=\\\"https://doi.org/10.5281/zenodo.8344854\\\">10.5281/zenodo.8344854</a>,
        <a href=\\\"https://www.youtube.com/watch?v=3J-jAFldeac\\\">YouTube</a>)</p></li><li><p>\U0001F631
        not using an open standard for metadata</p><p>\u27A1\uFE0F After all the effort
        of making data available, please go the extra mile of easy accessible metadata,
        your users will thank you!</p></li></ul><div><hr></div><h2>Interoperability:
        it just works</h2><p>FAIR's tongue-twister that even grammar checkers have
        trouble with. Exchanging information between computer systems without re-inventing
        the wheel each time. It\u2019s all about standards (again).</p><ul><li><p>\U0001F631
        not using a standard</p><p>\u27A1\uFE0F Standards are important, not just
        to charge your devices via USB, also for your data! Think of the amazing feeling
        when you can simply double click on a file and it just opens without problems.</p></li><li><p>\U0001F631
        using a standard in the wrong way</p><p>\u27A1\uFE0F Read the docs to use
        the standard (vocabulary) you are using to describe your data in the correct
        way. Imagine you annotate your website wrong and therefore it will rank worse
        on Google and other search engines.</p></li><li><p>\U0001F631 using a proprietary
        format</p><p>\u27A1\uFE0F You have done an amazing job to create some valuable
        data, don\u2019t make its value and success dependent on an untrustworthy
        commercial product that may cease to exist or that only a fraction of your
        users are able to use.</p></li></ul><div><hr></div><h2>Reusability:</h2><p>You
        found something online, for free \u2026 how valuable do you think is it for
        your use case and to which extend are you allowed to use it? Likewise, if
        you create something you probably want to be credited for, so you should indicate
        how someone else can reuse what you created. These questions concern the reusability
        (of data). There are a few things to avoid.</p><ul><li><p>\U0001F631 no license</p><p>\u27A1\uFE0F
        No one likes to be sued! Indicate a license to give security to your users
        on the one hand, and tell them how you would like them to use it on the other
        hand. </p></li><li><p>\U0001F631 don\u2019t mention any sources or provenance</p><p>\u27A1\uFE0F
        Everything has its origin, knowing it is important to assess its value for
        reuse. Based on which criteria did you curate the data? Which software did
        you use? Which data sources?</p></li><li><p>\U0001F631 too complicated</p><p>\u27A1\uFE0F
        make it easy for anyone to reuse your data/software. Once again, this can
        be achieved by following standards. For example, when providing code try to
        follow common coding and installation guidelines.</p></li></ul><p></p><p>These
        were just a few anti patterns standing opposite to some of the FAIR principles.
        Feel free to google to find more specific data anti patterns. Happy Halloween
        \U0001F383</p><div><hr></div><p>That\u2019s it for this week of the FAIR Data
        Digest. If you found the content interesting, please share and subscribe.
        See you <em>in two weeks</em>!</p><p>Sven</p><div class=\\\"subscription-widget-wrap\\\"
        data-attrs=\\\"{&quot;url&quot;:&quot;https://fairdata.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\\\"
        data-component-name=\\\"SubscribeWidgetToDOM\\\"><div class=\\\"subscription-widget
        show-subscribe\\\"><div class=\\\"preamble\\\"><p class=\\\"cta-caption\\\">Thanks
        for reading FAIR Data Digest! Subscribe for free to receive new posts and
        support my work.</p></div><form class=\\\"subscription-widget-subscribe\\\"><input
        type=\\\"email\\\" class=\\\"email-input\\\" name=\\\"email\\\" placeholder=\\\"Type
        your email\u2026\\\" tabindex=\\\"-1\\\"><input type=\\\"submit\\\" class=\\\"button
        primary\\\" value=\\\"Subscribe\\\"><div class=\\\"fake-input-wrapper\\\"><div
        class=\\\"fake-input\\\"></div><div class=\\\"fake-button\\\"></div></div></form></div></div>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/849qs-eer85\",\"guid\":\"138315194\",\"id\":\"52f18b9e-5c7b-4558-9112-8606acedc146\",\"image\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg\",\"language\":\"en\",\"published_at\":1698742842,\"reference\":[],\"relationships\":[],\"summary\":\"the
        Halloween edition on scary data anti-patterns\",\"tags\":[\"Best Practices\",\"Findability\",\"Reusability\",\"Accessibility\",\"Anti
        Patterns\"],\"title\":\"FAIR Data Digest #20\",\"updated_at\":1698742842,\"url\":\"https://fairdata.substack.com/p/fair-data-digest-20\"},\"highlight\":{\"authors\":[{\"name\":\"Sven
        Lieber, PhD\"}],\"content_html\":\"<p>Hello everyone,</p><p>welcome to today\u2019s
        Halloween edition about <em>data anti-patterns</em> that hopefully will scare
        you so much you don\u2019t do them :-) Things you better avoid when aiming
        for FAIR data.</p><p>Anti patterns? In short they are the <strong>opposite
        of best practices</strong>. Common or obvious solutions that are not only
        <em>ineffective </em>but also <em>risky</em>. Similar to biases, it is good
        to know about them so you can try avoiding them. I mainly know the term from
        software engineering, but apparently it is at least also known for project
        management and business processes (<a href=\\\"https://www.wikidata.org/wiki/Q76438\\\">Q76438</a>,
        <a href=\\\"https://en.wikipedia.org/wiki/Anti-pattern\\\">Wikipedia</a>).
        </p><p>In the following I will list some anti patterns I have seen \u201Cin
        the wild\u201D that directly relate to the FAIR principles, their evil twins
        if you like \U0001F9DB </p><div class=\\\"captioned-image-container\\\"><figure><a
        class=\\\"image-link is-viewable-img image2\\\" target=\\\"_blank\\\" href=\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg\\\"
        data-component-name=\\\"Image2ToDOM\\\"><div class=\\\"image2-inset\\\"><picture><source
        type=\\\"image/webp\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1456w\\\" sizes=\\\"100vw\\\"><img src=\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg\\\"
        width=\\\"1280\\\" height=\\\"960\\\" data-attrs=\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}\\\"
        class=\\\"sizing-normal\\\" alt=\\\"\\\" srcset=\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76805904-80e4-4327-a1d9-207dc9916382_1280x960.jpeg
        1456w\\\" sizes=\\\"100vw\\\" fetchpriority=\\\"high\\\"></picture><div class=\\\"image-link-expand\\\"><svg
        xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"16\\\" height=\\\"16\\\"
        viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" stroke=\\\"#FFFFFF\\\" stroke-width=\\\"2\\\"
        stroke-linecap=\\\"round\\\" stroke-linejoin=\\\"round\\\" class=\\\"lucide
        lucide-maximize2\\\"><polyline points=\\\"15 3 21 3 21 9\\\"></polyline><polyline
        points=\\\"9 21 3 21 3 15\\\"></polyline><line x1=\\\"21\\\" x2=\\\"14\\\"
        y1=\\\"3\\\" y2=\\\"10\\\"></line><line x1=\\\"3\\\" x2=\\\"10\\\" y1=\\\"21\\\"
        y2=\\\"14\\\"></line></svg></div></div></a><figcaption class=\\\"image-caption\\\">daryl_mitchell
        from Saskatoon, Saskatchewan, Canada, CC BY-SA 2.0, via Wikimedia Commons</figcaption></figure></div><div><hr></div><h2>Findability:
        the needle in the haystack</h2><p>Findability is probably <em>the </em>most
        important principle to get started: if you don\u2019t know that something
        exists or you know it but you don\u2019t know where it is, then you simply
        cannot use it.</p><p>Providing unique and <strong>persistent identifiers</strong>
        for things is the common solution to deal with findability. As easy as it
        sounds, there are many possible pitfalls, some obvious and some more tricky.
        </p><ul><li><p>\U0001F631 reusing identifiers </p><p>\u27A1\uFE0F Recycling
        is good, but reusing identifiers is bad: identifiers should be <strong>unique</strong>!</p></li><li><p>\U0001F631
        implementation details as part of the identifier </p><p>\u27A1\uFE0F If you
        change your software or provider the identifier will change, hence \u201Cidentifiers\u201D
        such as <code>http://my-url.com/items?query=abc</code> are bad.</p></li><li><p>\U0001F631
        thinking that persistent identifiers are only a technical problem for the
        ICT department </p><p>\u27A1\uFE0F Providing and maintaining the technical
        infrastructure is important, but knowing the data, use cases, stakeholders
        and community best practices is just as important</p></li></ul><p>From a very
        practical perspective, searching nowadays often happen via Google. Did you
        ever try to google a persistent identifier? Often you won\u2019t find much.
        The following paper investigates how FAIR persistent identifiers actually
        are and which role validity can play (DOI: <a href=\\\"https://doi.org/10.3233/DS-190024\\\">10.3233/DS-190024</a>).</p><p>By
        the way, there are different ways to implement persistent identifiers. You
        can check out the Persistent Identifier Guide of the Dutch Digital Heritage
        Network to know which system works best for you. Just answer 25 questions.</p><p
        class=\\\"button-wrapper\\\" data-attrs=\\\"{&quot;url&quot;:&quot;https://www.pidwijzer.nl/en/&quot;,&quot;text&quot;:&quot;Read
        the Persistent Identifier Guide&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\\\"
        data-component-name=\\\"ButtonCreateButton\\\"><a class=\\\"button primary\\\"
        href=\\\"https://www.pidwijzer.nl/en/\\\"><span>Read the Persistent Identifier
        Guide</span></a></p><div><hr></div><h2>Accessibility: what\u2019s in the box?!</h2><p>To
        stay in the Halloween theme, imagine you\u2019ve <em>found </em>a secret door,
        but you don\u2019t know how to open it. You would be more than happy getting
        some <em>information </em>about how to <em>access </em>the room behind it:
        that would be metadata about accessibility.</p><ul><li><p>\U0001F631 not providing
        any metadata</p><p>\u27A1\uFE0F <em>\u201C<s>Pics</s> metadata or it didn\u2019t
        happen\u201D</em>. Help others to find out more about something (something
        that possibly is no longer available). I have heard that the phone number
        of a research projects\u2019 PI can be the most valuable piece of metadata
        :-) (DOI: <a href=\\\"https://doi.org/10.5281/zenodo.8344854\\\">10.5281/zenodo.8344854</a>,
        <a href=\\\"https://www.youtube.com/watch?v=3J-jAFldeac\\\">YouTube</a>)</p></li><li><p>\U0001F631
        not using an open standard for metadata</p><p>\u27A1\uFE0F After all the effort
        of making data available, please go the extra mile of easy accessible metadata,
        your users will thank you!</p></li></ul><div><hr></div><h2>Interoperability:
        it just works</h2><p>FAIR's tongue-twister that even grammar checkers have
        trouble with. Exchanging information between computer systems without re-inventing
        the wheel each time. It\u2019s all about standards (again).</p><ul><li><p>\U0001F631
        not using a standard</p><p>\u27A1\uFE0F Standards are important, not just
        to charge your devices via USB, also for your data! Think of the amazing feeling
        when you can simply double click on a file and it just opens without problems.</p></li><li><p>\U0001F631
        using a standard in the wrong way</p><p>\u27A1\uFE0F Read the docs to use
        the standard (vocabulary) you are using to describe your data in the correct
        way. Imagine you annotate your website wrong and therefore it will rank worse
        on Google and other search engines.</p></li><li><p>\U0001F631 using a proprietary
        format</p><p>\u27A1\uFE0F You have done an amazing job to create some valuable
        data, don\u2019t make its value and success dependent on an untrustworthy
        commercial product that may cease to exist or that only a fraction of your
        users are able to use.</p></li></ul><div><hr></div><h2>Reusability:</h2><p>You
        found something online, for free \u2026 how valuable do you think is it for
        your use case and to which extend are you allowed to use it? Likewise, if
        you create something you probably want to be credited for, so you should indicate
        how someone else can reuse what you created. These questions concern the reusability
        (of data). There are a few things to avoid.</p><ul><li><p>\U0001F631 no license</p><p>\u27A1\uFE0F
        No one likes to be sued! Indicate a license to give security to your users
        on the one hand, and tell them how you would like them to use it on the other
        hand. </p></li><li><p>\U0001F631 don\u2019t mention any sources or provenance</p><p>\u27A1\uFE0F
        Everything has its origin, knowing it is important to assess its value for
        reuse. Based on which criteria did you curate the data? Which software did
        you use? Which data sources?</p></li><li><p>\U0001F631 too complicated</p><p>\u27A1\uFE0F
        make it easy for anyone to reuse your data/software. Once again, this can
        be achieved by following standards. For example, when providing code try to
        follow common coding and installation guidelines.</p></li></ul><p></p><p>These
        were just a few anti patterns standing opposite to some of the FAIR principles.
        Feel free to google to find more specific data anti patterns. Happy Halloween
        \U0001F383</p><div><hr></div><p>That\u2019s it for this week of the FAIR Data
        Digest. If you found the content interesting, please share and subscribe.
        See you <em>in two weeks</em>!</p><p>Sven</p><div class=\\\"subscription-widget-wrap\\\"
        data-attrs=\\\"{&quot;url&quot;:&quot;https://fairdata.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\\\"
        data-component-name=\\\"SubscribeWidgetToDOM\\\"><div class=\\\"subscription-widget
        show-subscribe\\\"><div class=\\\"preamble\\\"><p class=\\\"cta-caption\\\">Thanks
        for reading FAIR Data Digest! Subscribe for free to receive new posts and
        support my work.</p></div><form class=\\\"subscription-widget-subscribe\\\"><input
        type=\\\"email\\\" class=\\\"email-input\\\" name=\\\"email\\\" placeholder=\\\"Type
        your email\u2026\\\" tabindex=\\\"-1\\\"><input type=\\\"submit\\\" class=\\\"button
        primary\\\" value=\\\"Subscribe\\\"><div class=\\\"fake-input-wrapper\\\"><div
        class=\\\"fake-input\\\"></div><div class=\\\"fake-button\\\"></div></div></form></div></div>\",\"doi\":\"https://doi.org/10.59350/849qs-eer85\",\"reference\":[],\"summary\":\"the
        Halloween edition on scary data anti-patterns\",\"tags\":[\"Best Practices\",\"Findability\",\"Reusability\",\"Accessibility\",\"Anti
        Patterns\"],\"title\":\"FAIR Data Digest #20\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Samuel
        Moore\"}],\"blog_id\":\"gr1by89\",\"blog_name\":\"Samuel Moore\",\"blog_slug\":\"samuelmoore\",\"content_html\":\"\\n<p>Today&#8217;s
        <em>Scholarly Kitchen </em>blog post is an attempt by David Crotty &#8212;
        the blog&#8217;s editor &#8212; to quantify the increasing consolidation of
        the academic publishing industry. Crotty concludes:</p>\\n\\n\\n\\n<blockquote
        class=\\\"wp-block-quote\\\">\\n<p>Overall, the market has significantly consolidated
        since 2000 \u2014 when the top 5 publishers held 39% of the market of articles
        to 2022 where they control 61% of it. Looking at larger sets of publishers
        makes the consolidation even more extreme, as the top 10 largest publishers
        went from 47% of the market in 2000 to 75% in 2023, and the top 20 largest
        publishers from 54% to controlling 83% of the corpus.</p>\\n<cite>https://scholarlykitchen.sspnet.org/2023/10/30/quantifying-consolidation-in-the-scholarly-journals-market/</cite></blockquote>\\n\\n\\n\\n<p>It&#8217;s
        helpful to have more data on the increasing power that a small number of academic
        publishers hold. Crotty charts this consolidation from the year 2000 onwards,
        from the concentration brought about by the effects of the Big Deal to the
        present day where 5 publishers now control 61% of the article output, brought
        about by the dominant business models for open access based on greater volume
        and technological scale. The author&#8217;s finger is pointed at Coalition
        S for instigating a &#8216;rapid state of change&#8217; that allows author-pays
        open access to flourish.</p>\\n\\n\\n\\n<p>I&#8217;m no fan of open access
        policies, Plan S especially, and I&#8217;m sure that policy interventions
        play a part in the consolidation at play. There are of course many ways of
        achieving open access without recourse to author fees, transformative agreements,
        or technologies that remove human expertise in place of automation and scale.
        But while there is nothing natural or necessary about the relationship between
        open access and consolidation, there is a much stronger connection between
        commercialisation and consolidation. The recent history of academic publishing
        has been of marketisation and, hence, consolidation.  </p>\\n\\n\\n\\n<p>I
        always bristle when I read that open access is to blame for the problems with
        the publishing market, not simply because open access does not have to be
        a market-based activity (and is better when it isn&#8217;t) but more because
        the explanation is so shallow. It is a position that usually takes as its
        starting point that the natural and proper way for academic publishing to
        be organised is as a commercial activity and any intervention that works against
        this is to blame for the deleterious effects of commercialisation. Publishing
        is and always is a business (possibly a reflection of the constituents that
        the <em>Scholarly Kitchen</em> represents), despite the fact that it is exactly
        the commercial nature of publishing that is the problem. </p>\\n\\n\\n\\n<p>Yet
        open access is good precisely because it allows us to reorient academic publishing
        away from commercial practice and to experiment with forms of publishing that
        are less reliant on competition, profiteering and the extraction of free and
        under-remunerated labour. Policymakers are starting to wake up to this fact,
        as in part illustrated by the turn towards no-fee and non-commercial forms
        of open access, and I am cautiously optimistic about this turn. The danger
        is that policymakers mandate no-fee open access in accordance with the requirements
        of commercial publishing and the need to devalue skilled labour in the pursuit
        of revenue. </p>\\n\\n\\n\\n<p>So it&#8217;s easy to criticise open access
        policies for their harmful impact, but this has to be done from a position
        that understands how the same issues of profiteering and extraction were a
        consequence of the subscription market too. They are the logical extension
        of publishing as a market-based activity, not of wanting to make the literature
        freely available to all. Open access policies do not address the problem of
        marketisation largely because they are not designed to do so. Profit-seeking
        actors create business models to maximise their revenues as a result of such
        policies. The whole system rests on this logic of extraction and that&#8217;s
        what needs to be opposed.</p>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p></p>\\n\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/z8017-00204\",\"guid\":\"https://www.samuelmoore.org/?p=1437\",\"id\":\"95fef4ed-6274-47ed-a682-494ef1cb85ef\",\"image\":\"https://www.samuelmoore.org/wp-content/uploads/2023/10/R229.aa364024.jpg\",\"language\":\"en\",\"published_at\":1698678686,\"reference\":[],\"relationships\":[],\"summary\":\"Today\u2019s
        <em>Scholarly Kitchen </em>blog post is an attempt by David Crotty \u2014
        the blog\u2019s editor \u2014 to quantify the increasing consolidation of
        the academic publishing industry.\",\"tags\":[\"Open Access\"],\"title\":\"What&#8217;s
        so bad about consolidation in academic publishing?\",\"updated_at\":1698678694,\"url\":\"https://www.samuelmoore.org/2023/10/30/whats-so-bad-about-consolidation-in-academic-publishing\"},\"highlight\":{\"authors\":[{\"name\":\"Samuel
        Moore\"}],\"content_html\":\"\\n<p>Today&#8217;s <em>Scholarly Kitchen </em>blog
        post is an attempt by David Crotty &#8212; the blog&#8217;s editor &#8212;
        to quantify the increasing consolidation of the academic publishing industry.
        Crotty concludes:</p>\\n\\n\\n\\n<blockquote class=\\\"wp-block-quote\\\">\\n<p>Overall,
        the market has significantly consolidated since 2000 \u2014 when the top 5
        publishers held 39% of the market of articles to 2022 where they control 61%
        of it. Looking at larger sets of publishers makes the consolidation even more
        extreme, as the top 10 largest publishers went from 47% of the market in 2000
        to 75% in 2023, and the top 20 largest publishers from 54% to controlling
        83% of the corpus.</p>\\n<cite>https://scholarlykitchen.sspnet.org/2023/10/30/quantifying-consolidation-in-the-scholarly-journals-market/</cite></blockquote>\\n\\n\\n\\n<p>It&#8217;s
        helpful to have more data on the increasing power that a small number of academic
        publishers hold. Crotty charts this consolidation from the year 2000 onwards,
        from the concentration brought about by the effects of the Big Deal to the
        present day where 5 publishers now control 61% of the article output, brought
        about by the dominant business models for open access based on greater volume
        and technological scale. The author&#8217;s finger is pointed at Coalition
        S for instigating a &#8216;rapid state of change&#8217; that allows author-pays
        open access to flourish.</p>\\n\\n\\n\\n<p>I&#8217;m no fan of open access
        policies, Plan S especially, and I&#8217;m sure that policy interventions
        play a part in the consolidation at play. There are of course many ways of
        achieving open access without recourse to author fees, transformative agreements,
        or technologies that remove human expertise in place of automation and scale.
        But while there is nothing natural or necessary about the relationship between
        open access and consolidation, there is a much stronger connection between
        commercialisation and consolidation. The recent history of academic publishing
        has been of marketisation and, hence, consolidation.  </p>\\n\\n\\n\\n<p>I
        always bristle when I read that open access is to blame for the problems with
        the publishing market, not simply because open access does not have to be
        a market-based activity (and is better when it isn&#8217;t) but more because
        the explanation is so shallow. It is a position that usually takes as its
        starting point that the natural and proper way for academic publishing to
        be organised is as a commercial activity and any intervention that works against
        this is to blame for the deleterious effects of commercialisation. Publishing
        is and always is a business (possibly a reflection of the constituents that
        the <em>Scholarly Kitchen</em> represents), despite the fact that it is exactly
        the commercial nature of publishing that is the problem. </p>\\n\\n\\n\\n<p>Yet
        open access is good precisely because it allows us to reorient academic publishing
        away from commercial practice and to experiment with forms of publishing that
        are less reliant on competition, profiteering and the extraction of free and
        under-remunerated labour. Policymakers are starting to wake up to this fact,
        as in part illustrated by the turn towards no-fee and non-commercial forms
        of open access, and I am cautiously optimistic about this turn. The danger
        is that policymakers mandate no-fee open access in accordance with the requirements
        of commercial publishing and the need to devalue skilled labour in the pursuit
        of revenue. </p>\\n\\n\\n\\n<p>So it&#8217;s easy to criticise open access
        policies for their harmful impact, but this has to be done from a position
        that understands how the same issues of profiteering and extraction were a
        consequence of the subscription market too. They are the logical extension
        of publishing as a market-based activity, not of wanting to make the literature
        freely available to all. Open access policies do not address the problem of
        marketisation largely because they are not designed to do so. Profit-seeking
        actors create business models to maximise their revenues as a result of such
        policies. The whole system rests on this logic of extraction and that&#8217;s
        what needs to be opposed.</p>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<p></p>\\n\",\"doi\":\"https://doi.org/10.59350/z8017-00204\",\"reference\":[],\"summary\":\"Today\u2019s
        <em>Scholarly Kitchen </em>blog post is an attempt by David Crotty \u2014
        the blog\u2019s editor \u2014 to quantify the increasing consolidation of
        the academic publishing industry.\",\"tags\":[\"Open Access\"],\"title\":\"What&#8217;s
        so bad about consolidation in academic publishing?\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Martin
        Fenner\",\"url\":\"https://orcid.org/0000-0003-1419-2405\"}],\"blog_id\":\"f0m0e38\",\"blog_name\":\"Front
        Matter\",\"blog_slug\":\"front_matter\",\"content_html\":\"<p>Today I am happy
        to announce an important milestone for the <a href=\\\"https://rogue-scholar.org\\\"
        rel=\\\"noreferrer\\\">Rogue Scholar</a> science blog archive. Starting November
        1st, all blog posts from participating blogs will automatically be archived
        by the <a href=\\\"https://archive.org/\\\" rel=\\\"noreferrer\\\">Internet
        Archive</a>.</p><p>Front Matter has signed a contract with Internet Archive
        to use their <a href=\\\"https://archive-it.org\\\" rel=\\\"noreferrer\\\">Archive-It</a>
        service, and archiving will start in November. The archiving test runs to
        estimate the data volume that needs to be archived have been completed, and
        proper archiving will start this week. The archived blog posts will be available
        via <a href=\\\"https://archive-it.org/home/rogue-scholar\\\" rel=\\\"noreferrer\\\">this
        page</a>, and it will take 1-2 weeks until all blog posts from participating
        blogs are archived.</p><p>Some participating blogs are part of a larger website
        and the archiving will be restricted to the blog posts (and all associated
        resources such as images and other attached files). Blog posts will automatically
        be archived every six months. The only limitation is video which will not
        routinely be included because of large file sizes, which was an issue with
        one participating blog. The archive size is currently limited to one GB per
        participating blog, more than enough for the currently included blogs (only
        one participating blog exceeds that data limit but is included anyway).</p><p>The
        archived blog posts will be integrated into the Rogue Scholar service so that
        DOIs are automatically updated when a blog is no longer accessible via the
        public web. The DOIs will then resolve to the blog post archived by the Internet
        Archive. So far this is the case for only one blog included in Rogue Scholar
        (<a href=\\\"https://rogue-scholar.org/blogs/thor\\\" rel=\\\"noreferrer\\\">Project
        THOR</a>), but science blogs that are no longer maintained and disappearing
        from the public web are a well-known phenomenon. Rogue Scholar is coordinating
        these archiving activities with Crossref and their work on <a href=\\\"https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois/\\\"
        rel=\\\"noreferrer\\\">digital preservation</a>.</p><p>Long-term archiving
        of science blog posts is not a good fit for a startup such as Front Matter,
        and the collaboration with the Internet Archive provides the necessary long-term
        perspective. This does not preclude other archiving activities, and I have
        started conversations with the <a href=\\\"https://zenodo.org\\\" rel=\\\"noreferrer\\\">Zenodo</a>
        open repository about archiving blog posts in ePub format. This will happen
        later this year once <a href=\\\"https://doi.org/10.53731/qq4a5-6zc45\\\"
        rel=\\\"noreferrer\\\">work on the Rogue Scholar API</a> is completed. Please
        reach out to Rogue Scholar if you want to see science blog posts archived
        in other formats and/or other archiving services.</p><h2 id=\\\"references\\\">References</h2><p>Fenner,
        M. (2023). <em>Use cases for science blogs: Grant-funded projects</em>. <a
        href=\\\"https://doi.org/10.53731/mh9a1-dw902\\\">https://doi.org/10.53731/mh9a1-dw902</a></p><p>Eve,
        M. (2023). <em>A Request for Comment: Automatic Digital Preservation and Self-Healing
        DOIs</em> [Website]. Crossref. Retrieved October 30, 2023, from <a href=\\\"https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois/\\\">https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois/</a></p><p>Fenner,
        M. (2023). <em>The Rogue Scholar API now automatically indexes blog posts</em>.
        <a href=\\\"https://doi.org/10.53731/qq4a5-6zc45\\\">https://doi.org/10.53731/qq4a5-6zc45</a></p>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.53731/hhtx0-wb293\",\"guid\":\"653fb3b12e27ba00014b6bc0\",\"id\":\"0f088281-880f-4509-b5e3-d0a929518311\",\"image\":\"https://blog.front-matter.io/content/images/2023/10/Internet_Archive_02.jpeg\",\"language\":\"en\",\"published_at\":1698677547,\"reference\":[{\"doi\":\"https://doi.org/10.53731/mh9a1-dw902\",\"key\":\"ref1\"},{\"key\":\"ref2\",\"url\":\"https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois\"},{\"doi\":\"https://doi.org/10.53731/qq4a5-6zc45\",\"key\":\"ref3\"}],\"relationships\":[],\"summary\":\"Today
        I am happy to announce an important milestone for the Rogue Scholar science
        blog archive. Starting November 1st, all blog posts from participating blogs
        will automatically be archived by the Internet Archive. Front Matter has signed
        a contract with Internet Archive to use their Archive-It service, and archiving
        will start in November.\",\"tags\":[\"News\"],\"title\":\"Starting November,
        all Rogue Scholar blog posts will be archived by the Internet Archive\",\"updated_at\":1698677547,\"url\":\"https://blog.front-matter.io/posts/rogue-scholar-blog-posts-archived-by-internet-archive\"},\"highlight\":{\"authors\":[{\"name\":\"Martin
        Fenner\",\"url\":\"https://orcid.org/0000-0003-1419-2405\"}],\"content_html\":\"<p>Today
        I am happy to announce an important milestone for the <a href=\\\"https://rogue-scholar.org\\\"
        rel=\\\"noreferrer\\\">Rogue Scholar</a> science blog archive. Starting November
        1st, all blog posts from participating blogs will automatically be archived
        by the <a href=\\\"https://archive.org/\\\" rel=\\\"noreferrer\\\">Internet
        Archive</a>.</p><p>Front Matter has signed a contract with Internet Archive
        to use their <a href=\\\"https://archive-it.org\\\" rel=\\\"noreferrer\\\">Archive-It</a>
        service, and archiving will start in November. The archiving test runs to
        estimate the data volume that needs to be archived have been completed, and
        proper archiving will start this week. The archived blog posts will be available
        via <a href=\\\"https://archive-it.org/home/rogue-scholar\\\" rel=\\\"noreferrer\\\">this
        page</a>, and it will take 1-2 weeks until all blog posts from participating
        blogs are archived.</p><p>Some participating blogs are part of a larger website
        and the archiving will be restricted to the blog posts (and all associated
        resources such as images and other attached files). Blog posts will automatically
        be archived every six months. The only limitation is video which will not
        routinely be included because of large file sizes, which was an issue with
        one participating blog. The archive size is currently limited to one GB per
        participating blog, more than enough for the currently included blogs (only
        one participating blog exceeds that data limit but is included anyway).</p><p>The
        archived blog posts will be integrated into the Rogue Scholar service so that
        DOIs are automatically updated when a blog is no longer accessible via the
        public web. The DOIs will then resolve to the blog post archived by the Internet
        Archive. So far this is the case for only one blog included in Rogue Scholar
        (<a href=\\\"https://rogue-scholar.org/blogs/thor\\\" rel=\\\"noreferrer\\\">Project
        THOR</a>), but science blogs that are no longer maintained and disappearing
        from the public web are a well-known phenomenon. Rogue Scholar is coordinating
        these archiving activities with Crossref and their work on <a href=\\\"https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois/\\\"
        rel=\\\"noreferrer\\\">digital preservation</a>.</p><p>Long-term archiving
        of science blog posts is not a good fit for a startup such as Front Matter,
        and the collaboration with the Internet Archive provides the necessary long-term
        perspective. This does not preclude other archiving activities, and I have
        started conversations with the <a href=\\\"https://zenodo.org\\\" rel=\\\"noreferrer\\\">Zenodo</a>
        open repository about archiving blog posts in ePub format. This will happen
        later this year once <a href=\\\"https://doi.org/10.53731/qq4a5-6zc45\\\"
        rel=\\\"noreferrer\\\">work on the Rogue Scholar API</a> is completed. Please
        reach out to Rogue Scholar if you want to see science blog posts archived
        in other formats and/or other archiving services.</p><h2 id=\\\"references\\\">References</h2><p>Fenner,
        M. (2023). <em>Use cases for science blogs: Grant-funded projects</em>. <a
        href=\\\"https://doi.org/10.53731/mh9a1-dw902\\\">https://doi.org/10.53731/mh9a1-dw902</a></p><p>Eve,
        M. (2023). <em>A Request for Comment: Automatic Digital Preservation and Self-Healing
        DOIs</em> [Website]. Crossref. Retrieved October 30, 2023, from <a href=\\\"https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois/\\\">https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois/</a></p><p>Fenner,
        M. (2023). <em>The Rogue Scholar API now automatically indexes blog posts</em>.
        <a href=\\\"https://doi.org/10.53731/qq4a5-6zc45\\\">https://doi.org/10.53731/qq4a5-6zc45</a></p>\",\"doi\":\"https://doi.org/10.53731/hhtx0-wb293\",\"reference\":[{\"doi\":\"https://doi.org/10.53731/mh9a1-dw902\",\"key\":\"ref1\"},{\"key\":\"ref2\",\"url\":\"https://www.crossref.org/blog/a-request-for-comment-automatic-digital-preservation-and-self-healing-dois\"},{\"doi\":\"https://doi.org/10.53731/qq4a5-6zc45\",\"key\":\"ref3\"}],\"summary\":\"Today
        I am happy to announce an important milestone for the Rogue Scholar science
        blog archive. Starting November 1st, all blog posts from participating blogs
        will automatically be archived by the Internet Archive. Front Matter has signed
        a contract with Internet Archive to use their Archive-It service, and archiving
        will start in November.\",\"tags\":[\"News\"],\"title\":\"Starting November,
        all Rogue Scholar blog posts will be archived by the Internet Archive\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}},{\"document\":{\"authors\":[{\"name\":\"Chris
        von Csefalvay\"}],\"blog_id\":\"m89n867\",\"blog_name\":\"Chris von Csefalvay\",\"blog_slug\":\"chrisvoncsefalvay\",\"content_html\":\"<p>In
        the first four entries (<a href=\\\"../llms-language\\\">1</a> <a href=\\\"../lyre-of-hephaestus\\\">2</a>
        <a href=\\\"../moral-maps\\\">3</a> <a href=\\\"../prompt-engineering\\\">4</a>)
        of this sequence, I have focused primarily on what LLMs aren\u2019t, can\u2019t,
        won\u2019t, wouldn\u2019t and shouldn\u2019t. It\u2019s probably time to conclude
        this series by that much awaited moment in all stories, where the darkest
        night finally turns into a glorious dawn, where we finally arrive at the promised
        land, where we finally get to talk about what LLMs <em>could</em> be.</p>\\n<p>What
        I see as the most successful potential model of using LLMs is as a pack of
        semi-autonomous decision-makers with their own role, competing in a connectionistic,
        hierarchical model where each layer feeds into the next that performs some
        adjudicative or aggregative function. Structured this way, LLMs could become
        teams of rivals, analogous to the teams of rivals in the real world that we
        rely on to answer hard questions. For what that would look like, we might
        need to look at a story of my own. This story begins on the morning of 01
        December 1948 on Somerton Beach, near Adelaide, Australia.</p>\\n<section
        id=\\\"tamam-shud\\\" class=\\\"level1 page-columns page-full\\\">\\n<h1>Tamam
        shud</h1>\\n<p>On that fateful morning, the body of a middle aged man was
        found, slumped against the seawall at Somerton Beach, dead of causes that
        were far from clear. Neither was his identity, as he did not carry identification
        (quite unusual at the time, so close after wartime, when identification was
        still mandatory), nor could his identity be ascertained by other means. It
        was, in fact, clear that he went some lengths to conceal his identity, having
        even removed the manufacturers\u2019 tags from his clothes. The autopsy performed
        by the police pathologist only compounded the mystery by identifying multiple
        potential causes, each of which would have been sufficient to bring about
        the unknown man\u2019s demise. The mystery kept compounding when a scrap from
        Omar Khayyam\u2019s <em>Rubaiyat</em> was found in his pocket. It read <em>tamam
        shud</em>, which roughly translates to \u201Cit is finished\u201D. Investigators
        traced this copy of the <em>Rubaiyat</em> to a copy belonging to a young lady
        whose identity wouldn\u2019t be revealed until after her death in the 1990s,
        and whose involvement remains unclear. What appeared to be an acrostic cypher
        was also found in his notes. And so began a three-quarter-century mystery
        that has captivated many, including the author of these lines.</p>\\n\\n<div
        class=\\\"no-row-height column-margin column-container\\\"><div class=\\\"\\\">\\n<div
        class=\\\"quarto-figure quarto-figure-center\\\">\\n<figure class=\\\"figure\\\">\\n<p><img
        src=\\\"https://chrisvoncsefalvay.com/posts/team-of-rivals/SomertonMan2.jpg\\\"
        class=\\\"img-fluid figure-img\\\"></p>\\n<figcaption class=\\\"figure-caption\\\">Police
        photo of the Somerton Man, taken by South Australian police and disseminated
        for identification purposes.</figcaption>\\n</figure>\\n</div>\\n</div></div><p>To
        this day, the exact identity of the Somerton Man, who he really was and why
        he had to die, remains unknown. The DNA Doe Project, who have a pretty good
        track record at this sort of stuff, have identified him as Carl Webb, an Australian
        instrument maker and electrical engineer. This raises more questions than
        it answers. And while it is undoubtedly one of the most intriguing of human
        stories, it is also an interesting lesson in the way we arrive at knowledge
        and understanding.</p>\\n<div class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div
        class=\\\"cell-output-display\\\">\\n<div id=\\\"fig-coronial-flow\\\" class=\\\"quarto-figure
        quarto-figure-center anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre
        class=\\\"mermaid mermaid-js\\\" data-label=\\\"fig-coronial-flow\\\">flowchart
        TD\\n    A[\\\"Witnesses\\\"] --&gt; B[\\\"Police\\\"]\\n    C[\\\"Involved
        parties\\\"] --&gt; B[\\\"Police\\\"]\\n    Bs[\\\"Bystanders\\\"] --&gt;
        B\\n    E(\\\"Field investigation\\\") ---&gt; B\\n    F(\\\"Material evidence\\\")
        --&gt; E\\n    B ---&gt; Cor[\\\"Coronial inquest\\\"]\\n    P[\\\"Pathologist\\\"]
        --&gt; Cor\\n    X[\\\"Expert witnesses\\\"] --&gt; Cor\\n    X -.-&gt; B\\n
        \   E -.-&gt; Bs\\n</pre>\\n</div>\\n<figcaption class=\\\"figure-caption\\\">Figure&nbsp;1:
        Information flow of the coronial inquest into the death of the Somerton Man,
        01 December 1948.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n\\n<div
        class=\\\"no-row-height column-margin column-container\\\"><div class=\\\"\\\">\\n<p>In
        Commonwealth jurisdictions, coroners are judicial officers tasked with determining
        who the decedent is, the place and time of their death and how the decedent
        \u201Ccame by his death\u201D, a somewhat archaic way of describing the narrow
        question of causation.</p>\\n</div></div><p>Human processes intended to find
        the truth are complex, as Figure&nbsp;1 shows. In general, we rely on three
        key features:</p>\\n<ol type=\\\"1\\\">\\n<li>Adversariality: especially in
        adversarial legal systems, we use parties with opposing motivations and interests
        to act as controls on the other side. The idea is that no party has as much
        of an incentive to poke holes in the other side\u2019s case as someone whose
        interests are incompatible with the other side\u2019s. This is why we have
        the adversarial system in the first place, as opposed to a single dispassionate
        judge who is supposed to be the sole arbiter of truth. The adversarial system
        is a way of using the self-interest of the parties to arrive at the truth.
        It essentially outsources the job of finding the truth to the parties, who
        are supposed to be motivated to control the other side because it is in their
        interest to do so.</li>\\n</ol>\\n\\n<div class=\\\"no-row-height column-margin
        column-container\\\"><div class=\\\"\\\">\\n<p>Coronial hearings are inquisitorial,
        not adversarial, but there\u2019s an element of adversariality in the way
        the coroner is supposed to investigate the causal hypotheses put to him.</p>\\n</div></div><ol
        start=\\\"2\\\" type=\\\"1\\\">\\n<li>Hierarchicality: we use layers of hierarchically
        arranged players to arrive at the truth. Quite typically, these are hierarchies
        of adjudication, where each \u2018layer\u2019 looks at the previous layer\u2019s
        products and works to determine which of potentially multiple competing hypotheses
        are going to prevail. This may be the product of an adversarial layer, or
        it may simply be review, as would be the case where one layer looks at what
        a previous layer\u2019s findings were and decides whether they were correct
        or not, against some superordinate norm or evidential rules.</li>\\n</ol>\\n\\n<div
        class=\\\"no-row-height column-margin column-container\\\"><div class=\\\"\\\">\\n<p>If
        you\u2019ve ever watched Law &amp; Order, you know what I\u2019m talking about
        here. Police gather evidence, DAs build a case and plead it, and the judge
        and jury decide.</p>\\n</div></div><ol start=\\\"3\\\" type=\\\"1\\\">\\n<li>Roles:
        we use players who are specialists of roles, not general \u2018truth-finding
        agents\u2019. The purview of the police pathologist is different from that
        of a witness. Authority is limited by their role. In short, everyone wields
        \u2013 to borrow Stephen Jay Gould\u2019s term \u2013 an epistemic \u2018magisterium\u2019,
        a domain over which their authority is valid.</li>\\n</ol>\\n\\n<div class=\\\"no-row-height
        column-margin column-container\\\"><div class=\\\"\\\">\\n<p>The case of the
        Somerton Man is particularly illustrative of this: there were different experts
        for toxicology, a pathologist, even a witness on tram times.</p>\\n</div></div><p>These
        three features are what make human processes of truth-finding work. Truth-finding
        is the work of a team of rivals, with often orthogonally opposed interests
        but ultimately in the same enterprise of discerning truth. And this is what
        LLMs could be: teams of rivals, each with their own magisterium, each with
        their own role, each with their own motivations and interests, but ultimately
        working towards the same goal.</p>\\n</section>\\n<section id=\\\"dueling-llms\\\"
        class=\\\"level1 page-columns page-full\\\">\\n<h1>Dueling LLMs</h1>\\n<p>Let\u2019s
        assume that we\u2019re faced with something simpler than a 75-year-old Cold
        War mystery. Say, we\u2019re interested in something this simple: we want
        to make a cheesecake that is tasty, low-calorie, easy to make in no more than
        an hour and does not require any special ingredients. We could use a single
        LLM to do this:</p>\\n\\n<div class=\\\"no-row-height column-margin column-container\\\"><div
        class=\\\"\\\">\\n<p>\u30B9\u30D5\u30EC\u30C1\u30FC\u30BA\u30B1\u30FC\u30AD
        (souffl\xE9 cheesecake)</p>\\n<ul>\\n<li>1tbsp unsalted butter</li>\\n<li>6
        large eggs - separate out the yolks</li>\\n<li>10oz cream cheese</li>\\n<li>4tbsp
        unsalted butter</li>\\n<li>1 cup heavy cream</li>\\n<li>4tbsp sugar</li>\\n<li>1
        cup cake flour</li>\\n<li>2tbsp lemon juice</li>\\n<li>2tbsp jam of your choice</li>\\n</ul>\\n<p>Grease
        cake pan and line with parchment paper. Preheat oven to 350\xBAF. Mix cream
        cheese, butter and sugar in a bowl submerged in a saucepan of warm(ish) water.
        Add the egg yolks and using a strainer, sift the cake flour. Filter and add
        lemon juice and some lemon zest, if you have any hanging around. Whisk up
        the egg whites and gently mix it into the main mixture. This should make it
        somewhat nicely frothy. Pour into cake pan. Place cake pan onto baking sheet.
        Bake for 60 minutes or so. When done, turn off the heat, open the door and
        brush with your jam diluted at around 1:1 ratio with warm water. Apricot jam
        works best, but I won\u2019t judge you for your choice.</p>\\n</div></div><div
        class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div class=\\\"cell-output-display\\\">\\n<div
        id=\\\"fig-simple-cheesecake\\\" class=\\\"quarto-figure quarto-figure-center
        anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre class=\\\"mermaid
        mermaid-js\\\" data-label=\\\"fig-simple-cheesecake\\\">flowchart TD\\n    A[\\\"What's
        a good cheesecake recipe...\\\"] --&gt; B[\\\"Here's a recipe...\\\"]\\n</pre>\\n</div>\\n<figcaption
        class=\\\"figure-caption\\\">Figure&nbsp;2: How not to ask for a good cheesecake
        recipe.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>Here\u2019s
        a better approach:</p>\\n<div class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div
        class=\\\"cell-output-display\\\">\\n<div id=\\\"fig-better-cheesecake\\\"
        class=\\\"quarto-figure quarto-figure-center anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre
        class=\\\"mermaid mermaid-js\\\" data-label=\\\"fig-better-cheesecake\\\">flowchart
        TD\\n    A[\\\"What's a good cheesecake recipe...\\\"]\\n    A --&gt; G1\\n
        \   A --&gt; G2\\n    A --&gt; GN\\n    G1[\\\"Generator 1\\\"] --&gt; r1(\\\"Recipe
        1\\\")\\n    G2[\\\"Generator 2\\\"] --&gt; r2(\\\"Recipe 2\\\")\\n    GN[\\\"Generator
        n\\\"] --&gt; rn(\\\"Recipe n\\\")\\n    r1 --&gt; Dt[\\\"Tastiness\\\\ndiscriminator\\\"]\\n
        \   r2 --&gt; Dt\\n    rn --&gt; Dt\\n    r1 --&gt; Tt[\\\"Time\\\\ndiscriminator\\\"]\\n
        \   r2 --&gt; Tt\\n    rn --&gt; Tt\\n    r1 --&gt; Id[\\\"Ingredients\\\\ndiscriminator\\\"]\\n
        \   r2 --&gt; Id\\n    rn --&gt; Id\\n    Dt --&gt; jagg[\\\"Judgment aggregator\\\"]\\n
        \   Tt --&gt; jagg\\n    Id --&gt; jagg\\n    jagg --&gt; rs(\\\"Selected
        recipe\\\")\\n    jagg --&gt; rdc(\\\"Decisional context\\\")\\n    rs --&gt;
        G1p[\\\"Generator 1\\\"]\\n    rs --&gt; G2p[\\\"Generator 2\\\"]\\n    rs
        --&gt; GNp[\\\"Generator n\\\"]\\n    rdc --&gt; G1p[\\\"Generator 1\\\"]\\n
        \   rdc --&gt; G2p[\\\"Generator 2\\\"]\\n    rdc --&gt; GNp[\\\"Generator
        n\\\"]\\n</pre>\\n</div>\\n<figcaption class=\\\"figure-caption\\\">Figure&nbsp;3:
        A better way to ask for a cheesecake recipe. We ask multiple generators to
        generate recipes, then ask narrowly defined LLMs (discriminators) to determine
        how far they comply with the constraints. Take the result as well as the decisional
        context (why the \u2018winning\u2019 recipes \u2018won\u2019) and feed them
        to a new set of generators, asking for new recipes. Repeat until adequate
        cheesecake is produced.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>Note
        the \u2018rivalry\u2019 between various generators, each of which is trying
        to produce a recipe that satisfies the criteria. If this reminds you of a
        little something you might have seen before called adversarial generative
        networks, you are not wrong. In both cases, we are using discriminators to
        determine how good a particular output is, and tweak parameters to start selecting
        for outputs that have higher acceptance, i.e.&nbsp;outputs the discriminator
        believes is more likely to meet the criteria we posed. Where this model transcends
        the mere generative adversarial model is in two ways: the use of roles and
        the use of propagating the decisional context.</p>\\n\\n<div class=\\\"no-row-height
        column-margin column-container\\\"><div class=\\\"\\\">\\n<p>The <em>decisional
        context</em> refers to the \u2018why\u2019 of the decision. In the case of
        the cheesecake, it would be the reasons why the selected recipes were selected.
        This is important because it allows us to propagate the decisional context
        to the generators, so that they can learn from it and produce better outputs.
        This is a form of meta-learning, where the generators learn from the way their
        past performance was evaluated, and use it to produce better outputs. Unlike
        reinforcement learning or adversarial learning, it leverages the fact that
        LLMs can take in a broader ambit of information: not just the relatively binary
        outcome of what prevailed but also the <em>why</em>.</p>\\n</div></div><ol
        type=\\\"1\\\">\\n<li><p>We leverage roles of discriminators, and these can
        be nested and hierarchically structured to an arbitrary degree. Recall that
        the discriminator in a GAN could essentially go one way: accept or reject,
        with a given match likelihood. Not only can we have specialised discriminators
        here, we can also build networks of discriminators. For instance, we may \u2018explode\u2019
        the notion of ingredient \u201Cavailability\u201D into subcomponents: what
        fruits are in season for the cheesecake, what is ubiquitous at one\u2019s
        individual location and so on. This again brings us to the \u2018strength
        of the pack\u2019: the connectionist idea that we can create hierarchically
        layered simple units that can produce complex outputs.</p></li>\\n<li><p>We
        leverage the decisional context because we have something that can generate
        in response to a <em>why</em>. This is a key difference between LLMs and other
        machine learning models: they can take in a broader ambit of information,
        and they can generate in response to that. A typical backpropagative model
        still \u2018learns\u2019 the way all ML models do: by trial and error. LLMs
        can learn from feedback. They can learn not just from the <em>fact</em> of
        their success or failure but also from the <em>cause</em> of that outcome,
        as explained to it by another agent \u2013\u2013 which can of course be an
        LLM.</p></li>\\n</ol>\\n<p>In short, what we want is a team of rival LLMs,
        loudly arguing their case, and another team of LLMs assessing their relative
        claims. These, in turn, may be given incentives to compete. The end result
        is a cognitive net structure that uses LLMs not as ends in themselves but
        as primitives, whether it be as simple discriminators or as more complex agents.
        Figure&nbsp;3 does not merely describe a more complex structure than Figure&nbsp;2
        \u2013 it does, but that\u2019s really not the point. The point is that different
        roles and functions create emergence faster than the best single LLM. We see
        this in the difference between simple multilayer perceptrons, which are essentially
        identical layers of neurons with activation functions thrown on top of each
        other, versus modern deep learning, which relies on different layers (pooling,
        convolution and so on). This is, incidentally, a very well learned lesson
        from neuroscience: the complexity emerges not from heaping neurons on top
        of each other, but from doing so with quite significantly different structures
        and functions.</p>\\n\\n<div class=\\\"no-row-height column-margin column-container\\\"><div
        class=\\\"\\\">\\n<p>I\u2019ve skimmed this point on <a href=\\\"../prompt-engineering\\\">my
        post about prompt engineering</a>, but it bears repeating: the best single
        LLM is not as good as a team of rival LLMs.</p>\\n</div></div></section>\\n<section
        id=\\\"into-the-wild\\\" class=\\\"level1 page-columns page-full\\\">\\n<h1>Into
        the wild</h1>\\n<p>Something that more complex architectures are capable of
        that a single question-answering LLM won\u2019t is to be able to deal with
        the real world. The real world is messy, and it exists beyond the confines
        of code. I trust that we\u2019ve all overcome our instinctual fear that AI
        models interacting with the physical world seem to engender these days. I
        think we can safely conclude that Skynet won\u2019t emerge from letting an
        LLM agent use Google. If we grant that, then we can move towards the next
        step: the tool-using LLM.</p>\\n<p>At some point a little under 4 million
        years ago, in what is today Ethiopia, a particularly clever ancestor of ours,
        the hominin <em>Australopithecus afarensis</em>, used a kind of a primitive
        middle ground between a knife and an axe to dismember his prey. This might
        have been the first evidence we have for tool use. We consider tool use to
        be one of those watershed moments (I wrote about <a href=\\\"../llms-language\\\">the
        other one here</a>) where we got something that wasn\u2019t just doing what
        it has been except better, but a qualitatively different creature. Tool use
        is how we really interact with our environment, and how we can probe that
        environment and learn from what it does in return. Giving LLMs the tools to
        be \u2018out in the world\u2019 is to give them the kind of agency that is
        required to learn and operate in an autonomous way that goes beyond RLHF (reinforcement
        learning by human feedback), the current paradigm of training LLMs. At some
        point, we will need to let them loose in the wild, and let them learn from
        their own experience. We need to let them <em>have</em> experience, and that\u2019s
        arguably more important than what an agentic LLM can do for us. In the end,
        we\u2019ve all learned more through the mistakes (often hilarious ones) we\u2019ve
        stumbled into in the real world than we ever did in the classroom. And so
        will LLMs.</p>\\n\\n<div class=\\\"no-row-height column-margin column-container\\\"><div
        class=\\\"\\\">\\n<div class=\\\"quarto-figure quarto-figure-center\\\">\\n<figure
        class=\\\"figure\\\">\\n<p><img src=\\\"https://chrisvoncsefalvay.com/posts/team-of-rivals/aaz4729-f1.jpeg\\\"
        class=\\\"img-fluid figure-img\\\"></p>\\n<figcaption class=\\\"figure-caption\\\">Endocasts
        of the skull of <em>Australopithecus afarensis</em> from the <a href=\\\"https://www.science.org/doi/10.1126/sciadv.aaz4729\\\">paper
        by Gunz <em>et al.</em> (2020)</a>. A-G show specimens from Dikika, Ethiopia,
        where evidence for tool use was found.</figcaption>\\n</figure>\\n</div>\\n</div></div><p>Tool
        use is not just about learning, though. It\u2019s also about the utility of
        these models to us. The ubiquitous \u201Cknowledge cutoff\u201D message encountered
        with LLMs is an indication of what happens if LLMs are stagnant: they become
        reflections of the past, echoes of what they were taught rather than actual
        interacting entities. If we want them to live in the present, we need to teach
        them to interact with the present.</p>\\n</section>\\n<section id=\\\"team-building\\\"
        class=\\\"level1 page-columns page-full\\\">\\n<h1>Team building</h1>\\n<p>The
        main task, then, for AI developers is going to be one of building these teams
        of rivals. This process, if other connectionist systems (looking at you, neural
        networks) is anything to go by, will require a lot less human effort than
        we\u2019d think. There\u2019s no reason why LLMs themselves can\u2019t take
        some of the role in constructing these teams the way the structure of deep
        learning effectively builds its own filters and feature extractors from its
        own \u2018experience\u2019. The equivalent of structuring deep learning models\u2019
        architectures by determining layer types and their connections is borne out
        in how we build these virtual teams of rival LLMs.</p>\\n<div class=\\\"cell\\\"
        data-layout-align=\\\"center\\\">\\n<div class=\\\"cell-output-display\\\">\\n<div
        id=\\\"fig-lang-model-architectures\\\" class=\\\"quarto-figure quarto-figure-center
        anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre class=\\\"mermaid
        mermaid-js\\\" data-label=\\\"fig-lang-model-architectures\\\">flowchart TD\\n
        \   Input --&gt; L1\\n\\n    subgraph L1[\\\"Layer 1\\\"]\\n        L11[\\\"Convolution
        + ReLu\\\"] --&gt; P11[\\\"Pooling\\\"]\\n    end\\n\\n    L1 --&gt; L2\\n\\n
        \   subgraph L2[\\\"Layer 2\\\"]\\n        L21[\\\"Convolution + ReLu\\\"]
        --&gt; P21[\\\"Pooling\\\"]\\n    end\\n\\n    L2 -.-&gt; LN\\n\\n    subgraph
        LN[\\\"Layer N\\\"]\\n        LN1[\\\"Convolution + ReLu\\\"] --&gt; PN1[\\\"Pooling\\\"]\\n
        \   end\\n\\n    LN --&gt; F\\n\\n    F[\\\"Flatten\\\"] --&gt; FCN[\\\"FCN\\\"]\\n
        \   FCN --&gt; Softmax\\n    Softmax --&gt; Output \\n\\n\\n    I2[\\\"Input\\\"]\\n\\n
        \   subgraph LL1[\\\"Layer 1\\\"]\\n        LLL1[\\\"Generator\\\"] --&gt;
        LLL2[\\\"Discriminator 1\\\"]\\n        LLL1 --&gt; LLL22[\\\"Discriminator
        2\\\"]\\n        LLL1 --&gt; LLL2N[\\\"Discriminator n\\\"]\\n    end\\n\\n
        \   I2 --&gt; LL1\\n\\n    subgraph LL2[\\\"Layer 2\\\"]\\n        LL2G[\\\"Generator\\\"]
        --&gt; LL2D[\\\"Discriminator\\\"]\\n        LL2G --&gt; LLL2D2[\\\"Discriminator
        2\\\"]\\n        LL2G --&gt; LLL2DN[\\\"Discriminator n\\\"]\\n    end\\n\\n
        \   LL1 --&gt;|Decision + context| LL2\\n\\n    subgraph LLN[\\\"Layer n\\\"]\\n
        \       LLNG[\\\"Generator\\\"] --&gt; LLND1[\\\"Discriminator 1\\\"]\\n        LLNG
        --&gt; LLND2[\\\"Discriminator 2\\\"]\\n        LLNG --&gt; LLNDN[\\\"Discriminator
        N\\\"]\\n    end\\n\\n    LL2 -.-&gt;|Decision + context| LLN\\n\\n    LLN
        --&gt; P[\\\"Parser\\\"]\\n\\n    P --&gt; Q[\\\"Output\\\"]\\n</pre>\\n</div>\\n<figcaption
        class=\\\"figure-caption\\\">Figure&nbsp;4: Hierarchical model architectures.
        A simple convolutional neural network (left) and a hierarchical generative
        multi-tiered language model (right). Note the similarities in the way that
        increasing complexity creates the emergence of a more powerful analytical
        tool.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>Of course,
        the structure laid out in Figure&nbsp;4 is the simplest possible such construct,
        consisting essentially of a single type of layer for the most part. We know
        from deep learning that that can get us a pretty long way if the first and
        last layers are right, but in addition to different kinds of discriminators
        (the way we had different discriminators for time, ingredients and so on in
        our example in Figure&nbsp;3), we can interject three other kinds of players:</p>\\n<ol
        type=\\\"1\\\">\\n<li>Supervisors: One unhelpful tendency of LLMs is that
        they are still somewhat autoregressive, meaning that every token generated
        conditions the next token\u2019s likelihood, and so on. The consequence is
        that errors accumulate. Supervisors can stem this tendency by interposing
        themselves between stages of generative inputs and curbing this accumulative
        error.</li>\\n<li>Aggregators: These are the players that take the outputs
        of the discriminators and aggregate them into a single output. This is the
        equivalent of the \u2018judgment aggregator\u2019 in Figure&nbsp;3, which
        ultimately chooses the correct recipe.</li>\\n<li>Encapsulators: These players
        take the output from a fairly complex system and give us the simple(ish) output
        that we typically desire from most such systems. The system\u2019s own inherent
        complexity notwithstanding, we often want simple answers even to complex questions,
        no matter with how much complexity those answers were arrived at. Encapsulators
        are the players that give us that simple answer.</li>\\n</ol>\\n<p>The possibility
        exists, of course, for other new kinds of players. Just as we have seen the
        emergence of new architectures in deep learning, we\u2019ll likely see new
        forms of utilisation that are at this point still unexpected.</p>\\n\\n<div
        class=\\\"no-row-height column-margin column-container\\\"><div class=\\\"\\\">\\n<p>As
        well as novel, unexpected uses of existing models. The archetypal example
        here is, of course, the transformer, which was initially designed for machine
        translation. Safe to say that it far exceeded the confines of its expected
        niche.</p>\\n</div><div id=\\\"ref-yao2022react\\\" class=\\\"csl-entry\\\">\\nYao,
        Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
        and Yuan Cao. 2022. <span>\u2018React: Synergizing Reasoning and Acting in
        Language Models\u2019</span>. <em>arXiv Preprint arXiv:2210.03629</em>.\\n</div></div><p>All
        of this is augmented by an arsenal of tools. A tool, in this context, is quite
        simply a way an LLM can interact with the world outside of itself. If linking
        together LLMs creates the rudiments of reasoning, this creates the basics
        of action. Together, reasoning and acting (which gave the popular ReAct paradigm
        <span class=\\\"citation\\\" data-cites=\\\"yao2022react\\\">(see Yao et al.
        2022)</span> its name, but is much more ubiquitous than that, of course) make
        up the components of agency that elevate LLMs beyond simple question answering
        and move it towards a kind of semi-autonomous reasoned action. Such tools
        may be retrievers, interactors or indeed anything that can be articulated
        in code.</p>\\n<div class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div
        class=\\\"cell-output-display\\\">\\n<div id=\\\"fig-cheesecake-agent\\\"
        class=\\\"quarto-figure quarto-figure-center anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre
        class=\\\"mermaid mermaid-js\\\" data-label=\\\"fig-cheesecake-agent\\\">flowchart
        TD\\n    Q[\\\"What's a good cheesecake recipe?\\\"] -.-&gt; dots[\\\"...\\\"]
        \\n    dots --&gt; jagg[\\\"Judgment aggregator\\\"]\\n    jagg --&gt; FA[\\\"Final
        recipe\\\"]\\n    FA --&gt; SC[\\\"Shopping cart\\\"]\\n    SC --&gt;|missing
        items| jagg\\n    jagg -.-&gt; dots\\n</pre>\\n</div>\\n<figcaption class=\\\"figure-caption\\\">Figure&nbsp;5:
        Interacting with the world, and the world interacting with the model. What
        if a particular ingredient is not available?</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>As
        Figure&nbsp;5 illustrates, this is not a simple one-directional approach.
        If an LLM \u2018lives in the world\u2019, it does not merely put out information,
        but also ingests information from it. For instance, if the \u2018winning\u2019
        recipe calls for, say, grated lemon peel, but none can be had for love or
        money, the LLM needs to be able to adapt to that. This is where the \u2018judgment
        aggregator\u2019 comes in: it can take in the information that a particular
        ingredient is not available, and feed that back into the system and adapt.
        LLMs can learn from feedback, and respond to the substance of feedback, rather
        than merely its fact(ual existence). This sets them apart from other ML models.
        And to me, this is what truly charts the course of their future.</p>\\n</section>\\n<section
        id=\\\"conclusion\\\" class=\\\"level1\\\">\\n<h1>Conclusion</h1>\\n<p>This
        is the fifth and final post in a five-part sequence on what LLMs aren\u2019t,
        and what they are, and what they could become, as the state of the art stands
        in late 2023. Beyond the anxiety and the abundant misunderstandings that have
        dominated discourse, there is something old and something new at play at the
        same time. That\u2019s the kind of fault line where interesting things happen.</p>\\n<p>I
        am putting the finishing touches on this post at SFO, a few miles from one
        of the most unstable fault lines in the Earth\u2019s tectonic structure. The
        geographical determinist in me likes to think that being so near a fault line,
        so near a part of the Earth that is constantly reshaping itself \u2013 and
        will likely reshape anything taller than three stories in a pretty permanent
        and non-beneficial way \u2013 has played a role in Silicon Valley becoming
        what it is (or was?). Fault lines are where interesting stuff happens. The
        fault line between \u2018traditional\u2019 ML models and LLMs, which are undeniably
        somewhat novel if for nothing else their ability to interact with us in our
        most cherished human form of interaction \u2013 language \u2013, is where
        interesting stuff is bound to happen. Like earthquakes, any pretense at being
        able to predict all but the most immediate future is folly and delusion, so
        I shall be careful not to do so. But I do think that the future of LLMs is
        not in the direction of the \u2018big\u2019 LLMs that are currently in vogue,
        but in the direction of smaller, more specialised LLMs that can interact with
        us, and with each other, in the role-defined and goal-directed agentic manner
        in which we as humans have interacted for our history.</p>\\n<p>And there,
        as players on teams of rivals, is where LLMs will truly unfold their potential,
        tackling and embracing complexity to a degree that we have not seen before.</p>\\n<p>And
        that\u2019s where the real fun begins.</p>\\n\\n\\n\\n</section>\\n\\n<div
        id=\\\"quarto-appendix\\\" class=\\\"default\\\"><section class=\\\"quarto-appendix-contents\\\"><h2
        class=\\\"anchored quarto-appendix-heading\\\">Citation</h2><div><div class=\\\"quarto-appendix-secondary-label\\\">BibTeX
        citation:</div><pre class=\\\"sourceCode code-with-copy quarto-appendix-bibtex\\\"><code
        class=\\\"sourceCode bibtex\\\">@online{csefalvay2023,\\n  author = {Chris
        von Csefalvay},\\n  title = {Teams of {Rivals}},\\n  date = {2023-10-30},\\n
        \ url = {https://chrisvoncsefalvay.com/posts/team-of-rivals},\\n  langid =
        {en-GB}\\n}\\n</code></pre><div class=\\\"quarto-appendix-secondary-label\\\">For
        attribution, please cite this work as:</div><div id=\\\"ref-csefalvay2023\\\"
        class=\\\"csl-entry quarto-appendix-citeas\\\">\\nChris von Csefalvay. 2023.
        <span>\u201CTeams of Rivals.\u201D</span> October 30,\\n2023. <a href=\\\"https://chrisvoncsefalvay.com/posts/team-of-rivals\\\">https://chrisvoncsefalvay.com/posts/team-of-rivals</a>.\\n</div></div></section></div>\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/he7k6-tx158\",\"guid\":\"https://chrisvoncsefalvay.com/posts/team-of-rivals/index.html\",\"id\":\"d25ddd3b-695e-46d2-af42-559c729c0688\",\"image\":\"https://chrisvoncsefalvay.com/posts/team-of-rivals/aaz4729-f1.jpeg\",\"language\":\"en\",\"published_at\":1698624000,\"reference\":[],\"relationships\":[],\"summary\":\"In
        the first four entries (1 2 3 4) of this sequence, I have focused primarily
        on what LLMs aren\u2019t, can\u2019t, won\u2019t, wouldn\u2019t and shouldn\u2019t.
        It\u2019s probably time to conclude this series by that much awaited moment
        in all stories, where the darkest night finally turns into a glorious dawn,
        where we finally arrive at the promised land, where we finally get to talk
        about what LLMs <em>could</em> be.  What I see as the most successful potential
        model of\",\"tags\":[\"AI\",\"LLMs\",\"Agents\"],\"title\":\"Teams of Rivals\",\"updated_at\":1698624000,\"url\":\"https://chrisvoncsefalvay.com/posts/team-of-rivals/index.html\"},\"highlight\":{\"authors\":[{\"name\":\"Chris
        von Csefalvay\"}],\"content_html\":\"<p>In the first four entries (<a href=\\\"../llms-language\\\">1</a>
        <a href=\\\"../lyre-of-hephaestus\\\">2</a> <a href=\\\"../moral-maps\\\">3</a>
        <a href=\\\"../prompt-engineering\\\">4</a>) of this sequence, I have focused
        primarily on what LLMs aren\u2019t, can\u2019t, won\u2019t, wouldn\u2019t
        and shouldn\u2019t. It\u2019s probably time to conclude this series by that
        much awaited moment in all stories, where the darkest night finally turns
        into a glorious dawn, where we finally arrive at the promised land, where
        we finally get to talk about what LLMs <em>could</em> be.</p>\\n<p>What I
        see as the most successful potential model of using LLMs is as a pack of semi-autonomous
        decision-makers with their own role, competing in a connectionistic, hierarchical
        model where each layer feeds into the next that performs some adjudicative
        or aggregative function. Structured this way, LLMs could become teams of rivals,
        analogous to the teams of rivals in the real world that we rely on to answer
        hard questions. For what that would look like, we might need to look at a
        story of my own. This story begins on the morning of 01 December 1948 on Somerton
        Beach, near Adelaide, Australia.</p>\\n<section id=\\\"tamam-shud\\\" class=\\\"level1
        page-columns page-full\\\">\\n<h1>Tamam shud</h1>\\n<p>On that fateful morning,
        the body of a middle aged man was found, slumped against the seawall at Somerton
        Beach, dead of causes that were far from clear. Neither was his identity,
        as he did not carry identification (quite unusual at the time, so close after
        wartime, when identification was still mandatory), nor could his identity
        be ascertained by other means. It was, in fact, clear that he went some lengths
        to conceal his identity, having even removed the manufacturers\u2019 tags
        from his clothes. The autopsy performed by the police pathologist only compounded
        the mystery by identifying multiple potential causes, each of which would
        have been sufficient to bring about the unknown man\u2019s demise. The mystery
        kept compounding when a scrap from Omar Khayyam\u2019s <em>Rubaiyat</em> was
        found in his pocket. It read <em>tamam shud</em>, which roughly translates
        to \u201Cit is finished\u201D. Investigators traced this copy of the <em>Rubaiyat</em>
        to a copy belonging to a young lady whose identity wouldn\u2019t be revealed
        until after her death in the 1990s, and whose involvement remains unclear.
        What appeared to be an acrostic cypher was also found in his notes. And so
        began a three-quarter-century mystery that has captivated many, including
        the author of these lines.</p>\\n\\n<div class=\\\"no-row-height column-margin
        column-container\\\"><div class=\\\"\\\">\\n<div class=\\\"quarto-figure quarto-figure-center\\\">\\n<figure
        class=\\\"figure\\\">\\n<p><img src=\\\"https://chrisvoncsefalvay.com/posts/team-of-rivals/SomertonMan2.jpg\\\"
        class=\\\"img-fluid figure-img\\\"></p>\\n<figcaption class=\\\"figure-caption\\\">Police
        photo of the Somerton Man, taken by South Australian police and disseminated
        for identification purposes.</figcaption>\\n</figure>\\n</div>\\n</div></div><p>To
        this day, the exact identity of the Somerton Man, who he really was and why
        he had to die, remains unknown. The DNA Doe Project, who have a pretty good
        track record at this sort of stuff, have identified him as Carl Webb, an Australian
        instrument maker and electrical engineer. This raises more questions than
        it answers. And while it is undoubtedly one of the most intriguing of human
        stories, it is also an interesting lesson in the way we arrive at knowledge
        and understanding.</p>\\n<div class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div
        class=\\\"cell-output-display\\\">\\n<div id=\\\"fig-coronial-flow\\\" class=\\\"quarto-figure
        quarto-figure-center anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre
        class=\\\"mermaid mermaid-js\\\" data-label=\\\"fig-coronial-flow\\\">flowchart
        TD\\n    A[\\\"Witnesses\\\"] --&gt; B[\\\"Police\\\"]\\n    C[\\\"Involved
        parties\\\"] --&gt; B[\\\"Police\\\"]\\n    Bs[\\\"Bystanders\\\"] --&gt;
        B\\n    E(\\\"Field investigation\\\") ---&gt; B\\n    F(\\\"Material evidence\\\")
        --&gt; E\\n    B ---&gt; Cor[\\\"Coronial inquest\\\"]\\n    P[\\\"Pathologist\\\"]
        --&gt; Cor\\n    X[\\\"Expert witnesses\\\"] --&gt; Cor\\n    X -.-&gt; B\\n
        \   E -.-&gt; Bs\\n</pre>\\n</div>\\n<figcaption class=\\\"figure-caption\\\">Figure&nbsp;1:
        Information flow of the coronial inquest into the death of the Somerton Man,
        01 December 1948.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n\\n<div
        class=\\\"no-row-height column-margin column-container\\\"><div class=\\\"\\\">\\n<p>In
        Commonwealth jurisdictions, coroners are judicial officers tasked with determining
        who the decedent is, the place and time of their death and how the decedent
        \u201Ccame by his death\u201D, a somewhat archaic way of describing the narrow
        question of causation.</p>\\n</div></div><p>Human processes intended to find
        the truth are complex, as Figure&nbsp;1 shows. In general, we rely on three
        key features:</p>\\n<ol type=\\\"1\\\">\\n<li>Adversariality: especially in
        adversarial legal systems, we use parties with opposing motivations and interests
        to act as controls on the other side. The idea is that no party has as much
        of an incentive to poke holes in the other side\u2019s case as someone whose
        interests are incompatible with the other side\u2019s. This is why we have
        the adversarial system in the first place, as opposed to a single dispassionate
        judge who is supposed to be the sole arbiter of truth. The adversarial system
        is a way of using the self-interest of the parties to arrive at the truth.
        It essentially outsources the job of finding the truth to the parties, who
        are supposed to be motivated to control the other side because it is in their
        interest to do so.</li>\\n</ol>\\n\\n<div class=\\\"no-row-height column-margin
        column-container\\\"><div class=\\\"\\\">\\n<p>Coronial hearings are inquisitorial,
        not adversarial, but there\u2019s an element of adversariality in the way
        the coroner is supposed to investigate the causal hypotheses put to him.</p>\\n</div></div><ol
        start=\\\"2\\\" type=\\\"1\\\">\\n<li>Hierarchicality: we use layers of hierarchically
        arranged players to arrive at the truth. Quite typically, these are hierarchies
        of adjudication, where each \u2018layer\u2019 looks at the previous layer\u2019s
        products and works to determine which of potentially multiple competing hypotheses
        are going to prevail. This may be the product of an adversarial layer, or
        it may simply be review, as would be the case where one layer looks at what
        a previous layer\u2019s findings were and decides whether they were correct
        or not, against some superordinate norm or evidential rules.</li>\\n</ol>\\n\\n<div
        class=\\\"no-row-height column-margin column-container\\\"><div class=\\\"\\\">\\n<p>If
        you\u2019ve ever watched Law &amp; Order, you know what I\u2019m talking about
        here. Police gather evidence, DAs build a case and plead it, and the judge
        and jury decide.</p>\\n</div></div><ol start=\\\"3\\\" type=\\\"1\\\">\\n<li>Roles:
        we use players who are specialists of roles, not general \u2018truth-finding
        agents\u2019. The purview of the police pathologist is different from that
        of a witness. Authority is limited by their role. In short, everyone wields
        \u2013 to borrow Stephen Jay Gould\u2019s term \u2013 an epistemic \u2018magisterium\u2019,
        a domain over which their authority is valid.</li>\\n</ol>\\n\\n<div class=\\\"no-row-height
        column-margin column-container\\\"><div class=\\\"\\\">\\n<p>The case of the
        Somerton Man is particularly illustrative of this: there were different experts
        for toxicology, a pathologist, even a witness on tram times.</p>\\n</div></div><p>These
        three features are what make human processes of truth-finding work. Truth-finding
        is the work of a team of rivals, with often orthogonally opposed interests
        but ultimately in the same enterprise of discerning truth. And this is what
        LLMs could be: teams of rivals, each with their own magisterium, each with
        their own role, each with their own motivations and interests, but ultimately
        working towards the same goal.</p>\\n</section>\\n<section id=\\\"dueling-llms\\\"
        class=\\\"level1 page-columns page-full\\\">\\n<h1>Dueling LLMs</h1>\\n<p>Let\u2019s
        assume that we\u2019re faced with something simpler than a 75-year-old Cold
        War mystery. Say, we\u2019re interested in something this simple: we want
        to make a cheesecake that is tasty, low-calorie, easy to make in no more than
        an hour and does not require any special ingredients. We could use a single
        LLM to do this:</p>\\n\\n<div class=\\\"no-row-height column-margin column-container\\\"><div
        class=\\\"\\\">\\n<p>\u30B9\u30D5\u30EC\u30C1\u30FC\u30BA\u30B1\u30FC\u30AD
        (souffl\xE9 cheesecake)</p>\\n<ul>\\n<li>1tbsp unsalted butter</li>\\n<li>6
        large eggs - separate out the yolks</li>\\n<li>10oz cream cheese</li>\\n<li>4tbsp
        unsalted butter</li>\\n<li>1 cup heavy cream</li>\\n<li>4tbsp sugar</li>\\n<li>1
        cup cake flour</li>\\n<li>2tbsp lemon juice</li>\\n<li>2tbsp jam of your choice</li>\\n</ul>\\n<p>Grease
        cake pan and line with parchment paper. Preheat oven to 350\xBAF. Mix cream
        cheese, butter and sugar in a bowl submerged in a saucepan of warm(ish) water.
        Add the egg yolks and using a strainer, sift the cake flour. Filter and add
        lemon juice and some lemon zest, if you have any hanging around. Whisk up
        the egg whites and gently mix it into the main mixture. This should make it
        somewhat nicely frothy. Pour into cake pan. Place cake pan onto baking sheet.
        Bake for 60 minutes or so. When done, turn off the heat, open the door and
        brush with your jam diluted at around 1:1 ratio with warm water. Apricot jam
        works best, but I won\u2019t judge you for your choice.</p>\\n</div></div><div
        class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div class=\\\"cell-output-display\\\">\\n<div
        id=\\\"fig-simple-cheesecake\\\" class=\\\"quarto-figure quarto-figure-center
        anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre class=\\\"mermaid
        mermaid-js\\\" data-label=\\\"fig-simple-cheesecake\\\">flowchart TD\\n    A[\\\"What's
        a good cheesecake recipe...\\\"] --&gt; B[\\\"Here's a recipe...\\\"]\\n</pre>\\n</div>\\n<figcaption
        class=\\\"figure-caption\\\">Figure&nbsp;2: How not to ask for a good cheesecake
        recipe.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>Here\u2019s
        a better approach:</p>\\n<div class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div
        class=\\\"cell-output-display\\\">\\n<div id=\\\"fig-better-cheesecake\\\"
        class=\\\"quarto-figure quarto-figure-center anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre
        class=\\\"mermaid mermaid-js\\\" data-label=\\\"fig-better-cheesecake\\\">flowchart
        TD\\n    A[\\\"What's a good cheesecake recipe...\\\"]\\n    A --&gt; G1\\n
        \   A --&gt; G2\\n    A --&gt; GN\\n    G1[\\\"Generator 1\\\"] --&gt; r1(\\\"Recipe
        1\\\")\\n    G2[\\\"Generator 2\\\"] --&gt; r2(\\\"Recipe 2\\\")\\n    GN[\\\"Generator
        n\\\"] --&gt; rn(\\\"Recipe n\\\")\\n    r1 --&gt; Dt[\\\"Tastiness\\\\ndiscriminator\\\"]\\n
        \   r2 --&gt; Dt\\n    rn --&gt; Dt\\n    r1 --&gt; Tt[\\\"Time\\\\ndiscriminator\\\"]\\n
        \   r2 --&gt; Tt\\n    rn --&gt; Tt\\n    r1 --&gt; Id[\\\"Ingredients\\\\ndiscriminator\\\"]\\n
        \   r2 --&gt; Id\\n    rn --&gt; Id\\n    Dt --&gt; jagg[\\\"Judgment aggregator\\\"]\\n
        \   Tt --&gt; jagg\\n    Id --&gt; jagg\\n    jagg --&gt; rs(\\\"Selected
        recipe\\\")\\n    jagg --&gt; rdc(\\\"Decisional context\\\")\\n    rs --&gt;
        G1p[\\\"Generator 1\\\"]\\n    rs --&gt; G2p[\\\"Generator 2\\\"]\\n    rs
        --&gt; GNp[\\\"Generator n\\\"]\\n    rdc --&gt; G1p[\\\"Generator 1\\\"]\\n
        \   rdc --&gt; G2p[\\\"Generator 2\\\"]\\n    rdc --&gt; GNp[\\\"Generator
        n\\\"]\\n</pre>\\n</div>\\n<figcaption class=\\\"figure-caption\\\">Figure&nbsp;3:
        A better way to ask for a cheesecake recipe. We ask multiple generators to
        generate recipes, then ask narrowly defined LLMs (discriminators) to determine
        how far they comply with the constraints. Take the result as well as the decisional
        context (why the \u2018winning\u2019 recipes \u2018won\u2019) and feed them
        to a new set of generators, asking for new recipes. Repeat until adequate
        cheesecake is produced.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>Note
        the \u2018rivalry\u2019 between various generators, each of which is trying
        to produce a recipe that satisfies the criteria. If this reminds you of a
        little something you might have seen before called adversarial generative
        networks, you are not wrong. In both cases, we are using discriminators to
        determine how good a particular output is, and tweak parameters to start selecting
        for outputs that have higher acceptance, i.e.&nbsp;outputs the discriminator
        believes is more likely to meet the criteria we posed. Where this model transcends
        the mere generative adversarial model is in two ways: the use of roles and
        the use of propagating the decisional context.</p>\\n\\n<div class=\\\"no-row-height
        column-margin column-container\\\"><div class=\\\"\\\">\\n<p>The <em>decisional
        context</em> refers to the \u2018why\u2019 of the decision. In the case of
        the cheesecake, it would be the reasons why the selected recipes were selected.
        This is important because it allows us to propagate the decisional context
        to the generators, so that they can learn from it and produce better outputs.
        This is a form of meta-learning, where the generators learn from the way their
        past performance was evaluated, and use it to produce better outputs. Unlike
        reinforcement learning or adversarial learning, it leverages the fact that
        LLMs can take in a broader ambit of information: not just the relatively binary
        outcome of what prevailed but also the <em>why</em>.</p>\\n</div></div><ol
        type=\\\"1\\\">\\n<li><p>We leverage roles of discriminators, and these can
        be nested and hierarchically structured to an arbitrary degree. Recall that
        the discriminator in a GAN could essentially go one way: accept or reject,
        with a given match likelihood. Not only can we have specialised discriminators
        here, we can also build networks of discriminators. For instance, we may \u2018explode\u2019
        the notion of ingredient \u201Cavailability\u201D into subcomponents: what
        fruits are in season for the cheesecake, what is ubiquitous at one\u2019s
        individual location and so on. This again brings us to the \u2018strength
        of the pack\u2019: the connectionist idea that we can create hierarchically
        layered simple units that can produce complex outputs.</p></li>\\n<li><p>We
        leverage the decisional context because we have something that can generate
        in response to a <em>why</em>. This is a key difference between LLMs and other
        machine learning models: they can take in a broader ambit of information,
        and they can generate in response to that. A typical backpropagative model
        still \u2018learns\u2019 the way all ML models do: by trial and error. LLMs
        can learn from feedback. They can learn not just from the <em>fact</em> of
        their success or failure but also from the <em>cause</em> of that outcome,
        as explained to it by another agent \u2013\u2013 which can of course be an
        LLM.</p></li>\\n</ol>\\n<p>In short, what we want is a team of rival LLMs,
        loudly arguing their case, and another team of LLMs assessing their relative
        claims. These, in turn, may be given incentives to compete. The end result
        is a cognitive net structure that uses LLMs not as ends in themselves but
        as primitives, whether it be as simple discriminators or as more complex agents.
        Figure&nbsp;3 does not merely describe a more complex structure than Figure&nbsp;2
        \u2013 it does, but that\u2019s really not the point. The point is that different
        roles and functions create emergence faster than the best single LLM. We see
        this in the difference between simple multilayer perceptrons, which are essentially
        identical layers of neurons with activation functions thrown on top of each
        other, versus modern deep learning, which relies on different layers (pooling,
        convolution and so on). This is, incidentally, a very well learned lesson
        from neuroscience: the complexity emerges not from heaping neurons on top
        of each other, but from doing so with quite significantly different structures
        and functions.</p>\\n\\n<div class=\\\"no-row-height column-margin column-container\\\"><div
        class=\\\"\\\">\\n<p>I\u2019ve skimmed this point on <a href=\\\"../prompt-engineering\\\">my
        post about prompt engineering</a>, but it bears repeating: the best single
        LLM is not as good as a team of rival LLMs.</p>\\n</div></div></section>\\n<section
        id=\\\"into-the-wild\\\" class=\\\"level1 page-columns page-full\\\">\\n<h1>Into
        the wild</h1>\\n<p>Something that more complex architectures are capable of
        that a single question-answering LLM won\u2019t is to be able to deal with
        the real world. The real world is messy, and it exists beyond the confines
        of code. I trust that we\u2019ve all overcome our instinctual fear that AI
        models interacting with the physical world seem to engender these days. I
        think we can safely conclude that Skynet won\u2019t emerge from letting an
        LLM agent use Google. If we grant that, then we can move towards the next
        step: the tool-using LLM.</p>\\n<p>At some point a little under 4 million
        years ago, in what is today Ethiopia, a particularly clever ancestor of ours,
        the hominin <em>Australopithecus afarensis</em>, used a kind of a primitive
        middle ground between a knife and an axe to dismember his prey. This might
        have been the first evidence we have for tool use. We consider tool use to
        be one of those watershed moments (I wrote about <a href=\\\"../llms-language\\\">the
        other one here</a>) where we got something that wasn\u2019t just doing what
        it has been except better, but a qualitatively different creature. Tool use
        is how we really interact with our environment, and how we can probe that
        environment and learn from what it does in return. Giving LLMs the tools to
        be \u2018out in the world\u2019 is to give them the kind of agency that is
        required to learn and operate in an autonomous way that goes beyond RLHF (reinforcement
        learning by human feedback), the current paradigm of training LLMs. At some
        point, we will need to let them loose in the wild, and let them learn from
        their own experience. We need to let them <em>have</em> experience, and that\u2019s
        arguably more important than what an agentic LLM can do for us. In the end,
        we\u2019ve all learned more through the mistakes (often hilarious ones) we\u2019ve
        stumbled into in the real world than we ever did in the classroom. And so
        will LLMs.</p>\\n\\n<div class=\\\"no-row-height column-margin column-container\\\"><div
        class=\\\"\\\">\\n<div class=\\\"quarto-figure quarto-figure-center\\\">\\n<figure
        class=\\\"figure\\\">\\n<p><img src=\\\"https://chrisvoncsefalvay.com/posts/team-of-rivals/aaz4729-f1.jpeg\\\"
        class=\\\"img-fluid figure-img\\\"></p>\\n<figcaption class=\\\"figure-caption\\\">Endocasts
        of the skull of <em>Australopithecus afarensis</em> from the <a href=\\\"https://www.science.org/doi/10.1126/sciadv.aaz4729\\\">paper
        by Gunz <em>et al.</em> (2020)</a>. A-G show specimens from Dikika, Ethiopia,
        where evidence for tool use was found.</figcaption>\\n</figure>\\n</div>\\n</div></div><p>Tool
        use is not just about learning, though. It\u2019s also about the utility of
        these models to us. The ubiquitous \u201Cknowledge cutoff\u201D message encountered
        with LLMs is an indication of what happens if LLMs are stagnant: they become
        reflections of the past, echoes of what they were taught rather than actual
        interacting entities. If we want them to live in the present, we need to teach
        them to interact with the present.</p>\\n</section>\\n<section id=\\\"team-building\\\"
        class=\\\"level1 page-columns page-full\\\">\\n<h1>Team building</h1>\\n<p>The
        main task, then, for AI developers is going to be one of building these teams
        of rivals. This process, if other connectionist systems (looking at you, neural
        networks) is anything to go by, will require a lot less human effort than
        we\u2019d think. There\u2019s no reason why LLMs themselves can\u2019t take
        some of the role in constructing these teams the way the structure of deep
        learning effectively builds its own filters and feature extractors from its
        own \u2018experience\u2019. The equivalent of structuring deep learning models\u2019
        architectures by determining layer types and their connections is borne out
        in how we build these virtual teams of rival LLMs.</p>\\n<div class=\\\"cell\\\"
        data-layout-align=\\\"center\\\">\\n<div class=\\\"cell-output-display\\\">\\n<div
        id=\\\"fig-lang-model-architectures\\\" class=\\\"quarto-figure quarto-figure-center
        anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre class=\\\"mermaid
        mermaid-js\\\" data-label=\\\"fig-lang-model-architectures\\\">flowchart TD\\n
        \   Input --&gt; L1\\n\\n    subgraph L1[\\\"Layer 1\\\"]\\n        L11[\\\"Convolution
        + ReLu\\\"] --&gt; P11[\\\"Pooling\\\"]\\n    end\\n\\n    L1 --&gt; L2\\n\\n
        \   subgraph L2[\\\"Layer 2\\\"]\\n        L21[\\\"Convolution + ReLu\\\"]
        --&gt; P21[\\\"Pooling\\\"]\\n    end\\n\\n    L2 -.-&gt; LN\\n\\n    subgraph
        LN[\\\"Layer N\\\"]\\n        LN1[\\\"Convolution + ReLu\\\"] --&gt; PN1[\\\"Pooling\\\"]\\n
        \   end\\n\\n    LN --&gt; F\\n\\n    F[\\\"Flatten\\\"] --&gt; FCN[\\\"FCN\\\"]\\n
        \   FCN --&gt; Softmax\\n    Softmax --&gt; Output \\n\\n\\n    I2[\\\"Input\\\"]\\n\\n
        \   subgraph LL1[\\\"Layer 1\\\"]\\n        LLL1[\\\"Generator\\\"] --&gt;
        LLL2[\\\"Discriminator 1\\\"]\\n        LLL1 --&gt; LLL22[\\\"Discriminator
        2\\\"]\\n        LLL1 --&gt; LLL2N[\\\"Discriminator n\\\"]\\n    end\\n\\n
        \   I2 --&gt; LL1\\n\\n    subgraph LL2[\\\"Layer 2\\\"]\\n        LL2G[\\\"Generator\\\"]
        --&gt; LL2D[\\\"Discriminator\\\"]\\n        LL2G --&gt; LLL2D2[\\\"Discriminator
        2\\\"]\\n        LL2G --&gt; LLL2DN[\\\"Discriminator n\\\"]\\n    end\\n\\n
        \   LL1 --&gt;|Decision + context| LL2\\n\\n    subgraph LLN[\\\"Layer n\\\"]\\n
        \       LLNG[\\\"Generator\\\"] --&gt; LLND1[\\\"Discriminator 1\\\"]\\n        LLNG
        --&gt; LLND2[\\\"Discriminator 2\\\"]\\n        LLNG --&gt; LLNDN[\\\"Discriminator
        N\\\"]\\n    end\\n\\n    LL2 -.-&gt;|Decision + context| LLN\\n\\n    LLN
        --&gt; P[\\\"Parser\\\"]\\n\\n    P --&gt; Q[\\\"Output\\\"]\\n</pre>\\n</div>\\n<figcaption
        class=\\\"figure-caption\\\">Figure&nbsp;4: Hierarchical model architectures.
        A simple convolutional neural network (left) and a hierarchical generative
        multi-tiered language model (right). Note the similarities in the way that
        increasing complexity creates the emergence of a more powerful analytical
        tool.</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>Of course,
        the structure laid out in Figure&nbsp;4 is the simplest possible such construct,
        consisting essentially of a single type of layer for the most part. We know
        from deep learning that that can get us a pretty long way if the first and
        last layers are right, but in addition to different kinds of discriminators
        (the way we had different discriminators for time, ingredients and so on in
        our example in Figure&nbsp;3), we can interject three other kinds of players:</p>\\n<ol
        type=\\\"1\\\">\\n<li>Supervisors: One unhelpful tendency of LLMs is that
        they are still somewhat autoregressive, meaning that every token generated
        conditions the next token\u2019s likelihood, and so on. The consequence is
        that errors accumulate. Supervisors can stem this tendency by interposing
        themselves between stages of generative inputs and curbing this accumulative
        error.</li>\\n<li>Aggregators: These are the players that take the outputs
        of the discriminators and aggregate them into a single output. This is the
        equivalent of the \u2018judgment aggregator\u2019 in Figure&nbsp;3, which
        ultimately chooses the correct recipe.</li>\\n<li>Encapsulators: These players
        take the output from a fairly complex system and give us the simple(ish) output
        that we typically desire from most such systems. The system\u2019s own inherent
        complexity notwithstanding, we often want simple answers even to complex questions,
        no matter with how much complexity those answers were arrived at. Encapsulators
        are the players that give us that simple answer.</li>\\n</ol>\\n<p>The possibility
        exists, of course, for other new kinds of players. Just as we have seen the
        emergence of new architectures in deep learning, we\u2019ll likely see new
        forms of utilisation that are at this point still unexpected.</p>\\n\\n<div
        class=\\\"no-row-height column-margin column-container\\\"><div class=\\\"\\\">\\n<p>As
        well as novel, unexpected uses of existing models. The archetypal example
        here is, of course, the transformer, which was initially designed for machine
        translation. Safe to say that it far exceeded the confines of its expected
        niche.</p>\\n</div><div id=\\\"ref-yao2022react\\\" class=\\\"csl-entry\\\">\\nYao,
        Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
        and Yuan Cao. 2022. <span>\u2018React: Synergizing Reasoning and Acting in
        Language Models\u2019</span>. <em>arXiv Preprint arXiv:2210.03629</em>.\\n</div></div><p>All
        of this is augmented by an arsenal of tools. A tool, in this context, is quite
        simply a way an LLM can interact with the world outside of itself. If linking
        together LLMs creates the rudiments of reasoning, this creates the basics
        of action. Together, reasoning and acting (which gave the popular ReAct paradigm
        <span class=\\\"citation\\\" data-cites=\\\"yao2022react\\\">(see Yao et al.
        2022)</span> its name, but is much more ubiquitous than that, of course) make
        up the components of agency that elevate LLMs beyond simple question answering
        and move it towards a kind of semi-autonomous reasoned action. Such tools
        may be retrievers, interactors or indeed anything that can be articulated
        in code.</p>\\n<div class=\\\"cell\\\" data-layout-align=\\\"center\\\">\\n<div
        class=\\\"cell-output-display\\\">\\n<div id=\\\"fig-cheesecake-agent\\\"
        class=\\\"quarto-figure quarto-figure-center anchored\\\">\\n<figure class=\\\"figure\\\">\\n<div>\\n<pre
        class=\\\"mermaid mermaid-js\\\" data-label=\\\"fig-cheesecake-agent\\\">flowchart
        TD\\n    Q[\\\"What's a good cheesecake recipe?\\\"] -.-&gt; dots[\\\"...\\\"]
        \\n    dots --&gt; jagg[\\\"Judgment aggregator\\\"]\\n    jagg --&gt; FA[\\\"Final
        recipe\\\"]\\n    FA --&gt; SC[\\\"Shopping cart\\\"]\\n    SC --&gt;|missing
        items| jagg\\n    jagg -.-&gt; dots\\n</pre>\\n</div>\\n<figcaption class=\\\"figure-caption\\\">Figure&nbsp;5:
        Interacting with the world, and the world interacting with the model. What
        if a particular ingredient is not available?</figcaption>\\n</figure>\\n</div>\\n</div>\\n</div>\\n<p>As
        Figure&nbsp;5 illustrates, this is not a simple one-directional approach.
        If an LLM \u2018lives in the world\u2019, it does not merely put out information,
        but also ingests information from it. For instance, if the \u2018winning\u2019
        recipe calls for, say, grated lemon peel, but none can be had for love or
        money, the LLM needs to be able to adapt to that. This is where the \u2018judgment
        aggregator\u2019 comes in: it can take in the information that a particular
        ingredient is not available, and feed that back into the system and adapt.
        LLMs can learn from feedback, and respond to the substance of feedback, rather
        than merely its fact(ual existence). This sets them apart from other ML models.
        And to me, this is what truly charts the course of their future.</p>\\n</section>\\n<section
        id=\\\"conclusion\\\" class=\\\"level1\\\">\\n<h1>Conclusion</h1>\\n<p>This
        is the fifth and final post in a five-part sequence on what LLMs aren\u2019t,
        and what they are, and what they could become, as the state of the art stands
        in late 2023. Beyond the anxiety and the abundant misunderstandings that have
        dominated discourse, there is something old and something new at play at the
        same time. That\u2019s the kind of fault line where interesting things happen.</p>\\n<p>I
        am putting the finishing touches on this post at SFO, a few miles from one
        of the most unstable fault lines in the Earth\u2019s tectonic structure. The
        geographical determinist in me likes to think that being so near a fault line,
        so near a part of the Earth that is constantly reshaping itself \u2013 and
        will likely reshape anything taller than three stories in a pretty permanent
        and non-beneficial way \u2013 has played a role in Silicon Valley becoming
        what it is (or was?). Fault lines are where interesting stuff happens. The
        fault line between \u2018traditional\u2019 ML models and LLMs, which are undeniably
        somewhat novel if for nothing else their ability to interact with us in our
        most cherished human form of interaction \u2013 language \u2013, is where
        interesting stuff is bound to happen. Like earthquakes, any pretense at being
        able to predict all but the most immediate future is folly and delusion, so
        I shall be careful not to do so. But I do think that the future of LLMs is
        not in the direction of the \u2018big\u2019 LLMs that are currently in vogue,
        but in the direction of smaller, more specialised LLMs that can interact with
        us, and with each other, in the role-defined and goal-directed agentic manner
        in which we as humans have interacted for our history.</p>\\n<p>And there,
        as players on teams of rivals, is where LLMs will truly unfold their potential,
        tackling and embracing complexity to a degree that we have not seen before.</p>\\n<p>And
        that\u2019s where the real fun begins.</p>\\n\\n\\n\\n</section>\\n\\n<div
        id=\\\"quarto-appendix\\\" class=\\\"default\\\"><section class=\\\"quarto-appendix-contents\\\"><h2
        class=\\\"anchored quarto-appendix-heading\\\">Citation</h2><div><div class=\\\"quarto-appendix-secondary-label\\\">BibTeX
        citation:</div><pre class=\\\"sourceCode code-with-copy quarto-appendix-bibtex\\\"><code
        class=\\\"sourceCode bibtex\\\">@online{csefalvay2023,\\n  author = {Chris
        von Csefalvay},\\n  title = {Teams of {Rivals}},\\n  date = {2023-10-30},\\n
        \ url = {https://chrisvoncsefalvay.com/posts/team-of-rivals},\\n  langid =
        {en-GB}\\n}\\n</code></pre><div class=\\\"quarto-appendix-secondary-label\\\">For
        attribution, please cite this work as:</div><div id=\\\"ref-csefalvay2023\\\"
        class=\\\"csl-entry quarto-appendix-citeas\\\">\\nChris von Csefalvay. 2023.
        <span>\u201CTeams of Rivals.\u201D</span> October 30,\\n2023. <a href=\\\"https://chrisvoncsefalvay.com/posts/team-of-rivals\\\">https://chrisvoncsefalvay.com/posts/team-of-rivals</a>.\\n</div></div></section></div>\",\"doi\":\"https://doi.org/10.59350/he7k6-tx158\",\"reference\":[],\"summary\":\"In
        the first four entries (1 2 3 4) of this sequence, I have focused primarily
        on what LLMs aren\u2019t, can\u2019t, won\u2019t, wouldn\u2019t and shouldn\u2019t.
        It\u2019s probably time to conclude this series by that much awaited moment
        in all stories, where the darkest night finally turns into a glorious dawn,
        where we finally arrive at the promised land, where we finally get to talk
        about what LLMs <em>could</em> be.  What I see as the most successful potential
        model of\",\"tags\":[\"AI\",\"LLMs\",\"Agents\"],\"title\":\"Teams of Rivals\"},\"highlights\":[],\"text_match\":100,\"text_match_info\":{\"best_field_score\":\"0\",\"best_field_weight\":12,\"fields_matched\":4,\"score\":\"100\",\"tokens_matched\":0}}],\"out_of\":10329,\"page\":1,\"request_params\":{\"collection_name\":\"posts_sep_2023\",\"per_page\":10,\"q\":\"\"},\"search_cutoff\":false,\"search_time_ms\":4}"
    headers:
      Connection:
      - keep-alive
      accept-ranges:
      - none
      access-control-allow-origin:
      - '*'
      content-encoding:
      - gzip
      content-type:
      - application/json; charset=utf-8
      transfer-encoding:
      - chunked
      vary:
      - accept-encoding
    status:
      code: 200
      message: OK
version: 1
