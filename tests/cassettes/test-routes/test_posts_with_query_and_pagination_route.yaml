interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.31.0
    method: GET
    uri: https://fmxr36stzdcbiw7hp-1.a1.typesense.net/collections/posts/documents/search?q=retraction-watch&query_by=tags%2Ctitle%2Cdoi%2Cauthors.name%2Cauthors.url%2Csummary%2Ccontent_html%2Creference&filter_by=published_at%3A%3E%3D+0&sort_by=_text_match%3Adesc&per_page=10&page=2
  response:
    body:
      string: "{\"facet_counts\":[],\"found\":12,\"hits\":[{\"document\":{\"authors\":[{\"name\":\"Elias
        Koch\",\"url\":\"\"}],\"blog_id\":\"526jy42\",\"blog_name\":\"Elephant in
        the Lab\",\"blog_slug\":\"elephantinthelab\",\"content_html\":\"\\n<p></p>\\n\\n\\n\\n<h2>Introduction</h2>\\n\\n\\n\\n<div><figure><img
        loading=\\\"lazy\\\" src=\\\"https://elephantinthelab.org/wp-content/uploads/2021/04/foto2-1024x1024.jpg\\\"
        width=\\\"185\\\" height=\\\"185\\\" /><figcaption>Renke Siems</figcaption></figure></div>\\n\\n\\n\\n<p>In
        December 2018, a University of Minnesota web librarian, Cody Hanson, participated
        in a workshop hosted by the <a href=\\\"https://www.cni.org/\\\">Coalition
        for Networked Information</a>. The topic of this, and a number of other events
        to date, is the drive by major scholarly publishers to more fully integrate
        authentication systems for accessing electronic media into their platforms.
        Under various labels such as \u201CResearch Access 21 (RA21)\u201D, \u201CSeamless
        Access\u201D, or \u201CGet Full Text Research (GetFTR)\u201D, they want to
        replace the authentication options previously supported by libraries and academic
        institutions, such as IP range activation, VPN, proxy servers, or anonymous
        authentication to neutral third parties, as with the Shibboleth service, in
        favor of their own initiatives.[1] For years, librarians have countered these
        moves with their concerns that it will undermine the privacy of their users.
        Even at the event where Cody Hanson sat, the discussion raged until Todd Carpenter
        of the National Information Standards Organization (NISO) intervened, first
        correctly noting that RA21 does not require personally identifiable information
        (PII) to be sent to the publisher for authentication to occur. In fact, services
        like RA21 and Shibboleth share the same technical basis of a single sign-on
        via the Security Assertion Markup Language (SAML) \u2013 it\u2019s just that
        the technical realization is different. But then, to calm the discussion,
        Carpenter added, \u201Cthat publishers don\u2019t need PII from RA21 to be
        able to identify library users.\u201D And that statement, of course, was absolutely
        designed to allay any concerns.</p>\\n\\n\\n\\n<p>Cody Hanson, who spent a
        lot of time developing privacy-compliant access to electronic media for his
        users, began to wonder. Should this be true and can an analysis of the source
        code of publisher platform pages provide evidence of if and how publishers
        can identify library users? Cody Hanson undertook a testing exercise: he took
        the 100 most-demanded documents at his university and looked at them to see
        which platforms were represented in them. He picked one document from each
        of the fifteen platforms found, examined it with the Ghostery browser addon,
        and downloaded the document page to examine the source code. Several thousand
        lines of JavaScript later, Cody Hanson came to a simple answer: yes, it\u2019s
        true, Carpenter was right.</p>\\n\\n\\n\\n<p>Of the fifteen platforms examined,
        one was clean (InformPubsOnline); on the others, he found a total of 139 different
        third-party asset sources. AdTech\u2019s entire technical assortment was represented:
        simple trackers, audience tools like Neustar, AddThis, Adobe, and Oracle,
        and fingerprinters like Doubleclick. These finds were significant to Cody
        Hanson:</p>\\n\\n\\n\\n<p>\u201CThe reason I was interested in third-party
        assets being loaded on these sites is that any JavaScript loaded on these
        pages has access to the entire DOM, or document object model, meaning it can
        read the address and contents of the page. It also has access to every user
        action that happens on that page, and can itself load additional scripts from
        additional sources. So when, for example, a publisher puts JavaScript from
        Google on its pages, Google can record any information from the page about
        the article being sought, or search terms from a library user in the publisher
        platform. Fourteen of the fifteen publisher platforms included Google code
        on the article page.\u201C[2]\\n\\n\\n\\n</p><p>Facebook code was also represented
        in many cases, as were a number of other data collectors, which means that
        patron privacy is no longer a given. Personalized profiles of the information
        behavior of every scientist are created, and since the publishers involve
        both the large Internet corporations and the audience tools as large data
        collectors, the data does not remain with the publishers, but flows out and
        can be linked with the knowledge that already exists elsewhere about the person.
        A seamless and thus valuable and tradable online biography of every scientist
        is created; the previous special milieu of science communication has been
        incorporated into the general commercial (and governmental) surveillance of
        the digital space.</p>\\n\\n\\n\\n<p></p>\\n\\n\\n\\n<h2>Researchers at risk</h2>\\n\\n\\n\\n<figure><img
        src=\\\"https://lh3.googleusercontent.com/jTLXe--K5Pdeovdnkfyld5L7AODrPm-aJH7MESiQ25_Tn06oY7zx7LRcSZiRl-c7PnDA8_c9u4vpQT7aYKg4MHd4iQZsGhUMXDHqKlsZ9dRGgi6oM2zHHWC_3eoHEYbOOGrvCecx\\\"
        /></figure>\\n\\n\\n\\n<p><strong>Figure 1</strong>: Screenshot of <a href=\\\"https://www.youtube.com/watch?v=uAzt-iJEkvU&amp;feature=youtu.be\\\">Cody
        Hanson\u2019s talk</a>\_</p>\\n\\n\\n\\n<p>What are the consequences of this?
        First, it must be noted that this is a violation of fundamental rights: liberal
        societies have freedom of research and teaching enshrined in their constitutions,
        but a monitored freedom is not. Thus, academic freedom as we knew it has come
        to its end. But this is not merely an abstract shortcoming, because tracking
        can put scientists at concrete and grave risk. After all, in recent years
        we have seen that, as a researcher, you can get into trouble even in countries
        where you would not have suspected it in the past: in one country you don\u2019t
        make friends with research on climate change,[3] in another not with gender
        studies.[4] And in Hungary, the Central European University was chased out
        of the country.[5] Putting pressure on scientists is, of course, much easier
        when you know more about them, and since their behavioral profiles are tradable,
        they are of course open to all interested actors, as long as they want to
        spend the necessary money.</p>\\n\\n\\n\\n<p>Economic consequences are also
        foreseeable, be it either in the form of a distortion of competition to the
        detriment of publishers who act in a trustworthy manner and therefore do not
        track, or, above all, in the destruction of the value of public research investments.
        In many future fields such as AI, personalized medicine, and material sciences,
        there is strong competition between public and commercial research, whereby
        the commercial players are in part also the ones who invented and disseminated
        the tracking technologies. Now, the same power of metadata makes itself felt
        in tracking as it does in telecommunication surveillance: if you know who
        contacted each other, when, how often, and in what sequence, you don\u2019t
        have to listen to the phone calls anymore, it\u2019s all there in plain sight
        anyway. Likewise, if researchers are being tracked, you no longer need to
        break into their lab, because you know what they are\_ working on and how
        far they have gotten with it. And since it\u2019s not just one researcher
        who is being tracked and all the data can be analyzed together in business
        intelligence \u2013 who will be faster in filing a patent application? And
        will research results still benefit the public that paid for them?</p>\\n\\n\\n\\n<p>So
        is it time to update the old joke \u201CCome to the dark side, we have cookies\u201D?
        Absolutely. And when we hear that Google, Apple and others are going to put
        a stop to third-party tracking, can we sit back because it will all have been
        an ugly transitional phase? Not at all. All the interested players are either
        busy monopolizing data access for themselves, like Google, or continuing to
        find ways for themselves, like others. If you take a closer look at your favorite
        journal, chances are you\u2019ll find tools for collecting Real Time Bidding
        Data, meaning your information behavior is auctioned off in real time without
        the need to set a cookie but e.g. for the benefit of intelligence services.[6]
        Elsevier, as part of RELX, benefits from technologies that reside in the group\u2019s
        Risk Solution division and has installed ThreatMetrix on its ScienceDirect
        platform, a technology that boasts of being able to individually identify
        billions of devices and, if you recall the discussion about ebay, does not
        shy away from port scanning.[7]\\n\\n\\n\\n</p><figure><img src=\\\"https://lh6.googleusercontent.com/NWFRpQPxHxeJXmt-jKeWDG55gh7uOY3YfoqlYwgiGOn7GdJjcexcpqfN1vPinaplTYybJXANPYLW3iMCzfYL5NznhX82AdLJLbnZAVfEBytsnrhQhqjG0CiDWxVpr54_3q7r1AVW\\\"
        /></figure>\\n\\n\\n\\n<p><strong>Figure 2</strong>: Traces of <a href=\\\"https://twitter.com/WolfieChristl/status/1295655040741445632\\\">ThreatMetrix</a>
        on the ScienceDirect-platform\_</p>\\n\\n\\n\\n<p>The argument about authentication
        is also far from over[8], and publishers sometimes use sleazy methods to enforce
        their wishes. In 2020, for example, libraries were given opt-out deadlines
        for switching to seamless access in the middle of the first lockdown, and
        those who didn\u2019t keep a constant eye on their email in this situation
        woke up to new access rules.[9] Using the same instrument over the Christmas
        holidays is also very popular, for example, to enforce the installation of
        Google CASA (Campus Activated Subscriber Access), which allows information
        behavior to be synchronized with the Google account. The goal is to always
        keep a hand on the valuable first-party data, because this is the best way
        to secure direct personalized access. Borders are hardly accepted there; currently
        Elsevier and SpringerNature are considering infiltrating Trojans into university
        networks via the libraries in order to be able to collect biometric data such
        as typing speed or types of mouse movement in order to be able to identify
        users in other areas of the Internet as well.[10]\\n\\n\\n\\n</p><p>So what
        is the strategic goal of these publishers? First of all, they want to expand
        their unassailable position in information access to the entire research life
        cycle. This is achieved by contracts such as those concluded by Elsevier in
        the Netherlands, under which researchers can publish open access at no extra
        cost if the universities license Elsevier products in the area of research
        information systems in return.[11] Such contracts are disastrous, because
        it will be immediately clear to all researchers that their research will only
        be properly noted in the next evaluation if it also ends up in the appropriate
        journals. Consequently, the market for both publication and research information
        systems will quickly clear up as a result of this linkage business, and then
        the platform will have moved a step closer to its goal again: namely, that
        it is not science, universities or research funders who determine which research
        is the \u201Cright\u201D one and how much it \u201Ccounts\u201D[12] but rather
        the platform who makes the determination.</p>\\n\\n\\n\\n<p>Such deals seem
        like revenants from the nineties, when, in the <a href=\\\"https://en.wikipedia.org/wiki/Browser_wars\\\">browser
        wars</a> Microsoft knocked Netscape out of the market by tying Internet Explorer
        to Windows. Another revenant is the behavior of libraries toward OA: if libraries
        had dried up the green road of OA in the nineties by first allowing publishers
        to transform their products and, most importantly, making self archiving worthless
        by signing up for package licenses[13] (which is why the head of SpringerNature
        called package licenses \u201Cthe best invention since sliced bread\u201D[14]),
        now they are once again holding out for publishers, where publishers are having
        their Napster moment through SciHub. By partnering with publishers to commercialize
        open access through publication fees, libraries are trying to crisis-proof
        their business: as long as science governance doesn\u2019t change, scientists
        will have to publish in the appropriate journals, and libraries will prevail
        precisely in the area of accounting and managing publication costs so they
        can continue to justify their staff and budgets. As a result, publication
        fees are already 30 times the actual expenses in some cases[15] \u2013 so
        there is always enough money with the publishers to continue to outspend,
        out-buy, and out-develop big data analytics any alternatives that are developed
        from within the scientific community. And, moreover, DEAL contracts in Germany
        are already beginning to distort competition from publishers in favor of the
        big players on whom the negotiations are focused \u2013 adding even more more
        money to even fewer players.[16]\\n\\n\\n\\n</p><h2>New Business Models</h2>\\n\\n\\n\\n<div><figure><img
        src=\\\"https://lh6.googleusercontent.com/gF6hWhzNB3Se9ktkzfDeyUxTJ1vCOeLcdfhT3b1vSZmk4zOoBtUf1DUsuWW2-glKiJ4apciOnHH1zhMMSgKXtM_HFfczZtb8_vGwfRbqyltjYK-ACO-bRvgCpVX8q-bUAguQVtgk\\\"
        /><figcaption><strong>Figure 3</strong>: <a href=\\\"https://upload.wikimedia.org/wikipedia/commons/4/4b/ICE.Arrest_lg.jpg\\\">ICE-Officer
        detaining a suspect\_</a></figcaption></figure></div>\\n\\n\\n\\n<p>But for
        some players in scholarly communication, this is no longer enough. Thomson
        Reuters and RELX have discovered the global security industry as a buyer of
        their data analytics products, and have signed big data policing contracts
        with law enforcement organizations like\_ U.S. Immigration and Customs Enforcement
        (ICE) that use these tools to track immigrants.[17] Since both publishers
        are central providers of specialized legal information, it is therefore not
        at all clear at the moment whether, for example, an attorney who consults
        Westlaw or LexisNexis for guidance on immigration law issues is thereby contributing
        to his or her clients being found and deported.[18] The publishers, of course,
        dismiss it all \u2013 and at the same time continue to escalate the path taken:
        RELX was also an early investor in Palantir, and so the information provided
        by LexisNexis can also be imported and analyzed in Palantir\u2019s analytics
        tools. And since these platforms and tools are sold globally just as the data
        they feed is collected globally, illegal immigrants don\u2019t have to be
        the only ones they\u2019re used against \u2013 the respective interested parties
        can surely think of something \u2013 and library money is supporting these
        systems of surveillance.[19] A scene of revolving-door deals between (semi-)state
        and commercial actors is emerging, where security institutions can get whatever
        they want to know on the market without having to rely any longer on onerous
        rule-of-law tools like obtaining a judicial search warrant. The motto of a
        data broker applies, who, when asked who was actually exchanging data with
        whom, replied: \u201CEveryone with everyone. It\u2019s one[2]\_ big whorehouse.\u201D[20]\\n\\n\\n\\n</p><p>So
        where do we stand? We have seen in the past that, due to the special situation
        of the Cold War, the major scientific publishers grew into an early form of
        platform economy, long before the large Internet corporations adapted this
        principle and were thus able to achieve their current position.[21] With their
        work on the Trojans, the publishers are also revealing geopolitical interests,
        namely to cut off entire states from the flow of scientific information, and
        are thus part of a Cold War 4.0.[22] All of this no longer has anything to
        do with scientific progress in knowledge, and so for years we have also had
        to recognize that the publication infrastructures are serving their actual
        purpose less and less. The attention economy of the journals has been analyzed
        time and again in the past[23], but only now, through the replication crisis
        and every glance at <a href=\\\"https://retractionwatch.com/\\\">Retraction
        Watch</a>, is it becoming clear how the compulsion for storytelling, the fudging
        of data, and the fight for the smallest publishable unit is destroying and
        corrupting science from within. Leading players in science communication will
        have no problem with this, as Elsevier itself has had half a dozen journals
        in the past that claimed to be peer reviewed but were mere shells rented out
        to pharmaceutical companies to be filled.[24] And the parent company RELX
        engages in political landscaping with donations to climate change deniers
        and to actors who drummed for U.S. withdrawal from WHO.[25] All this can only
        happen almost without contradiction because infrastructure is still a largely
        blind spot in science governance. More and more voices are calling for an
        \u201Cinfrastructural turn\u201D,[26] but even in the welcome European initiatives
        the old mistakes are being made, because in the European Open Science Cloud
        Elsevier has nested[27] just as ORE is run by F1000 Research \u2013 which
        has been bought up by Taylor &amp; Francis.[28]\\n\\n\\n\\n</p><p>So parts
        of the scientific information infrastructures are increasingly moving toward
        the dark money complex, as characterized by the Koch brothers, or even by
        Bob Mercer, whose investment made Cambridge Analytica possible. If you read
        Christopher Wylie\u2019s report on this[29] and compare it with Shoshana Zuboff\u2019s
        analysis of surveillance capitalism[30], you will see that science and antiscience[31]
        are increasingly amalgamating into a confusing tangle, because in the same
        way that this system could not be operated at all without a developed scientific
        education, it is at the same time laying the axe to the foundations of truth,
        enlightenment and social liberties. In discourse, this becomes clear in the
        meteoric rise of half-truths, which are used by conspiracy theorists, politicians,
        economic players, as well as the media \u2013 and more and more scientific
        authors. J\xF6rg Peters recently developed this in his discussion of Stuart
        Ritchie\u2019s \u201CScience Fictions\u201D as a structural problem of an
        information infrastructure that rewards unreplicable research results and
        flushes them to the forefront of attention.[32] Not from the perspective of
        an economist, but of a literary scholar, Nicola Gess analyzes half-truths
        as a technique that no longer operates according to the scheme true/false,
        but according to schemes such as credible/uncredible, affective/sober, and
        connective/closed. What is central, she says, is internal coherence and no
        longer correspondence with external facts. Half-truths, therefore, could no
        longer be resolved with a fact check, but only with a fiction check, which
        highlights how the half-truth builds a narrative around a crystallization
        core of truth while pretending to report facts.[33] We find much of this in
        the corrupted information infrastructures, such as the impossibility for the
        reviewer to follow documents of a 21st century data-driven science to the
        ground of fact within a 17th century publication system with reasonable effort.
        He also has to rely on credibility and internal coherence, and as a consequence,
        subsequently exposed falsifiers like Diederik Stapel resort to the same justification
        phrases as Nicola Gess highlights on the fallen star journalist Claas Relotius.[34]\\n\\n\\n\\n</p><h2>Conclusion</h2>\\n\\n\\n\\n<p>After
        all \u2013 when we talk about power and power abuse in academia, the information
        infrastructures must not be forgotten. They are in your lab, they are in your
        life, they\u2019re corrupting the truth, they\u2019re selling your ass and
        there is little you can do about it.[35] Nevertheless, it remains the responsibility
        of each individual to reflect on this for their actions in the science system
        and not to blindly accept it, as Tal Yarkoni states in his great rant:</p>\\n\\n\\n\\n<blockquote><p>\u201CWhen
        people start routinely accepting that The System is Broken and The Incentives
        Are Fucking Us Over, bad things tend to happen. It\u2019s very hard to have
        a stable, smoothly functioning society once everyone believes (rightly or
        wrongly) that gaming the system is the only way to get by.\u201D[36]</p></blockquote>\\n\\n\\n\\n<p>Gaming
        the system means, as in any game of chance: the bank always wins. Acting responsibly,
        on the other hand, means supporting and improving existing European initiatives.
        In Germany, this is currently being done, among other things, through the
        National Research Data Infrastructure (NFDI), which is cooperative and public.
        This path must be expanded, and the mistakes from the literature supply must
        be avoided at all costs. Science will never be happy again as long as the
        big players remain in the game. The success of the NFDI is all the more crucial
        because in the data-driven sciences, data, software and text belong in a common
        context so that results can be verified and immediately developed further.
        For scientists, the current situation is like when a programmer first writes
        his code, then publishes an article about it, and the next programmer tries
        to guess the code from the article and then creates an improved version. Sounds
        highly efficient? And that\u2019s exactly why GitHub exists \u2013 and a trusted,
        powerful and non-buyable version of it would be exactly what science needs.[37]\\n\\n\\n\\n</p><p>And
        before I forget \u2013 there is one more thing you can easily do: join the
        community of scientists who don\u2019t want everything done to them and sign
        the <a href=\\\"https://stoptrackingscience.eu/\\\">Stop Tracking Science
        petition</a>![38]\\n</p>\",\"content_text\":\"content_text\",\"id\":\"782d2266-e63e-4e6b-88d9-3f379bc0e762\",\"image\":\"https://elephantinthelab.org/wp-content/uploads/2021/04/siems-1-scaled.jpg\",\"language\":\"en\",\"published_at\":1618395479,\"reference\":[],\"relationships\":[],\"summary\":\"<strong>Introduction</strong>
        Renke Siems  In December 2018, a University of Minnesota web librarian, Cody
        Hanson, participated in a workshop hosted by the Coalition for Networked Information.
        The topic of this, and a number of other events to date, is the drive by major
        scholarly publishers to more fully integrate authentication systems for accessing
        electronic media into their platforms.\",\"tags\":[\"Infrastructure\",\"Power\",\"Opinion\"],\"title\":\"When
        your journal reads you\",\"updated_at\":1618400665,\"url\":\"https://elephantinthelab.org/when-your-journal-reads-you\"},\"highlight\":{\"content_html\":{\"matched_tokens\":[\"Retraction\",\"Watch\"],\"snippet\":\"a
        href=\\\"https://retractionwatch.com/\\\"><mark>Retraction</mark> <mark>Watch</mark></a>,
        is it becoming\"}},\"highlights\":[{\"field\":\"content_html\",\"matched_tokens\":[\"Retraction\",\"Watch\"],\"snippet\":\"a
        href=\\\"https://retractionwatch.com/\\\"><mark>Retraction</mark> <mark>Watch</mark></a>,
        is it becoming\"}],\"text_match\":1157451471441100873,\"text_match_info\":{\"best_field_score\":\"2211897868288\",\"best_field_weight\":9,\"fields_matched\":1,\"score\":\"1157451471441100873\",\"tokens_matched\":2}},{\"document\":{\"authors\":[{\"name\":\"Adam
        Day\",\"url\":null}],\"blog_id\":\"ez7c883\",\"blog_name\":\"Stories by Adam
        Day on Medium\",\"blog_slug\":\"clearskiesadam\",\"content_html\":\"<p>TL;DR:
        Join me at ConTech Live to hear about a recent project with <a href=\\\"https://opencredo.com/\\\">Open
        Credo</a> to see if we could detect unusual co-authorships in a dataset created
        by Anna Abalkina. <a href=\\\"https://www.contech.live/contech2022\\\">Sign
        up\_here!</a></p><p>Papermilling has a few definitions which you see here
        and there. Sometimes it\u2019s \u201Corganised manipulation of the peer-review
        process\u201D or it might be \u201Cmanufacturing of fraudulent research papers\u201D
        and so\_on.</p><p>I described the general roles involved in papermilling in
        <a href=\\\"https://publisherad.medium.com/how-papermills-work-generally-c080bb9fff43\\\">a
        previous blog\_post</a>.</p><figure><img src=\\\"https://cdn-images-1.medium.com/max/937/1*OSsajzQlkOFyf_LkQ6lBfw.png\\\"
        /></figure><p>But there\u2019s actually a lot of nuance in how different papermills
        operate. Consider what happens when we have multiple agents, forgers and authors
        operating in a\_network.</p><p>I\u2019m going to describe here 3 quite different
        things which get called \u2018paper mills\u2019. Perhaps there are more than
        3 main classes of papermill? Add a comment if you can think of another\_one!</p><h3>1.
        Fabrication from\_scratch</h3><p>Any organisation which fabricates research
        papers from scratch for sale. People have been fabricating research for as
        long as research has been a thing, but it\u2019s the <em>organised</em> part
        that makes this new. This happens at a vastly greater scale than it used to
        and the problem continues to\_grow.</p><p>There are definitely templates and
        re-used images and text in these papers, but I think \u2018from scratch\u2019
        is a fair distinction between this and the other\_types.</p><p>All those fake
        western blots? That\u2019s this. Those cancer genetics papers from obscure
        hospital research departments? That\u2019s this too. Take a look at cases
        highlighted by Jennifer Byrne, Elisabeth Bik, Cheshire, Smut Clyde and others
        for examples.</p><p>My <a href=\\\"https://medium.com/p/719b8b3b8253\\\">Papermill
        Alarm API</a>, mentioned in <a href=\\\"https://medium.com/p/b89db7feba9d\\\">previous
        blog posts</a>, is mostly trained to highlight this kind of papermilling.</p><h3>2.
        Fabrication by plagiarism</h3><p>Fabrication from scratch requires skill.
        What if our forger doesn\u2019t know how to write a fake paper from scratch?
        This is a <em>sad\_forger:</em></p><figure><img src=\\\"https://cdn-images-1.medium.com/max/250/1*7_PceWGAeLBoczc3dYi2Jg.png\\\"
        /><figcaption>I\u2019ve made him look sad by rotating his mouth 180\xB0. Image
        manipulation is not my\_fort\xE9.</figcaption></figure><p>So what can he do?
        How can he turn that frown upside-down?</p><figure><img src=\\\"https://cdn-images-1.medium.com/max/363/1*0b31C3OWhZwrtkz9C3Cfww.png\\\"
        /></figure><p>The idea is simple. If he can\u2019t write a fake paper, <em>he
        can just copy a real one</em>! Or he can get creative and copy several bits
        of papers and paste the bits into a new manuscript. If he\u2019s worried about
        getting caught by anti-plagiarism software, he can always drop the text into
        a paraphrasing tool. This is where we get \u201CTortured Phrases\u201D (<a
        href=\\\"https://arxiv.org/abs/2107.06751\\\">see the great work of Guillaume
        Cabanac et al on this subject</a>).</p><p>The difference between organised
        plagiarism and papermilling-from-scratch is simply that, here, the forger
        starts with 1 or more existing papers and <em>converts </em>them into a \u2018new\u2019\_one.</p><h3>3.
        Authorship brokering</h3><p>This is where authorship slots on papers, fabricated
        or not, are sold. This is actually just the \u2018agent\u2019 part of any
        papermilling operation, but sometimes we see it as a standalone service.</p><p><a
        href=\\\"https://retractionwatch.com/2019/07/18/exclusive-russian-site-says-it-has-brokered-authorships-for-more-than-10000-researchers/\\\">A
        famous case of a brokerage advertising authorships for sale was brought to
        light by Retraction Watch some years ago</a>. Anna Abalkina, a researcher
        at Freie Universit\xE4t Berlin <a href=\\\"https://arxiv.org/abs/2112.13322\\\">investigated
        this matter by tracking down the papers advertised on that site</a>. It\u2019s
        a very interesting case and well-worth reading\_about.</p><p>It\u2019s fortunate
        that the papers for sale on that site were advertised so openly. Consider
        now that anyone can sell authorship on any paper privately. How can we detect
        that? I\u2019ve learned recently that it might be possible to spot these brokered
        authorships.</p><h3>Join me at ConTech\_Live!</h3><p>Earlier this year, SAGE
        Publishing worked with <a href=\\\"https://opencredo.com/\\\">Open Credo</a>,
        a software development consultancy here in London, on a project to see if
        we could detect unusual co-authorships using the dataset created by Anna Abalkina
        in her work. If you\u2019d like to know more, I suggest signing up for <a
        href=\\\"https://www.contech.live/contech2022\\\">our forthcoming ConTech
        presentation!</a></p><img src=\\\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=14993c37ebfa\\\"
        width=\\\"1\\\" height=\\\"1\\\" />\",\"content_text\":\"content_text\",\"doi\":\"https://doi.org/10.59350/dakrb-j7a75\",\"id\":\"8ec5510e-0bab-40ef-8978-9011f579aeca\",\"image\":\"https://cdn-images-1.medium.com/max/937/1*OSsajzQlkOFyf_LkQ6lBfw.png\",\"language\":\"en\",\"published_at\":1667236116,\"reference\":[],\"relationships\":[],\"summary\":\"TL;DR:
        Join me at ConTech Live to hear about a recent project with Open Credo to
        see if we could detect unusual co-authorships in a dataset created by Anna
        Abalkina. Sign up\_here! Papermilling has a few definitions which you see
        here and there.\",\"tags\":[\"Peer Review Journals\",\"Academic Publishing\",\"Research
        Integrity\"],\"title\":\"(The?) 3 kinds of papermills\",\"updated_at\":1667236116,\"url\":\"https://clearskiesadam.medium.com/3-kinds-of-papermills-14993c37ebfa\"},\"highlight\":{\"content_html\":{\"matched_tokens\":[\"Retraction\",\"Watch\"],\"snippet\":\"brought
        to light by <mark>Retraction</mark> <mark>Watch</mark> some years ago</a>\"}},\"highlights\":[{\"field\":\"content_html\",\"matched_tokens\":[\"Retraction\",\"Watch\"],\"snippet\":\"brought
        to light by <mark>Retraction</mark> <mark>Watch</mark> some years ago</a>\"}],\"text_match\":1157451471441100873,\"text_match_info\":{\"best_field_score\":\"2211897868288\",\"best_field_weight\":9,\"fields_matched\":1,\"score\":\"1157451471441100873\",\"tokens_matched\":2}}],\"out_of\":5205,\"page\":2,\"request_params\":{\"collection_name\":\"posts_sep_2023\",\"per_page\":10,\"q\":\"retraction-watch\"},\"search_cutoff\":false,\"search_time_ms\":9}"
    headers:
      Connection:
      - keep-alive
      accept-ranges:
      - none
      access-control-allow-origin:
      - '*'
      content-encoding:
      - gzip
      content-type:
      - application/json; charset=utf-8
      transfer-encoding:
      - chunked
      vary:
      - accept-encoding
    status:
      code: 200
      message: OK
version: 1
